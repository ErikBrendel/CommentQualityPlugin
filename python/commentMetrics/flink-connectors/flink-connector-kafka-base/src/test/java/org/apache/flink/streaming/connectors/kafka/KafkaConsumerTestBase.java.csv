# id;timestamp;commentText;codeText;commentWords;codeWords
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1480685315;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1484866642;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1487173364;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1488437582;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1489419493;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1493975167;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1494830990;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1495175928;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1495175928;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if(failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1495923077;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1498894422;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1498894422;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1499314317;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1501249950;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1507568316;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1509723634;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1515177485;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1519973085;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1519973085;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1523020981;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1523020981;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("Runner for CancelingOnFullInputTest")__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout), "Runner for CancelingOnFullInputTest")___		_		runnerThread.join()___		failueCause = jobError.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,runner,for,canceling,on,full,input,test,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,for,canceling,on,full,input,test,runner,thread,join,failue,cause,job,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1523020981;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1525452496;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1539704473;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1549282380;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnFullInputTest() throws Exception;1550834396;Tests that the source can be properly canceled when reading full partitions.;public void runCancelingOnFullInputTest() throws Exception {_		final String topic = "cancelingOnFullTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		_		DataGenerators.InfiniteStringsGenerator generator =_				new DataGenerators.InfiniteStringsGenerator(kafkaServer, topic)__		generator.start()___		__		final AtomicReference<Throwable> jobError = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					jobError.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = jobError.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}__		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		if (generator.isAlive()) {_			generator.shutdown()__			generator.join()__		}_		else {_			Throwable t = generator.getError()__			if (t != null) {_				t.printStackTrace()__				fail("Generator failed: " + t.getMessage())__			} else {_				fail("Generator failed with no exception")__			}_		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,canceling,on,full,input,test,throws,exception,final,string,topic,canceling,on,full,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,data,generators,infinite,strings,generator,generator,new,data,generators,infinite,strings,generator,kafka,server,topic,generator,start,final,atomic,reference,throwable,job,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,job,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,job,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,if,generator,is,alive,generator,shutdown,generator,join,else,throwable,t,generator,get,error,if,t,null,t,print,stack,trace,fail,generator,failed,t,get,message,else,fail,generator,failed,with,no,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1487173364;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(parallelism)___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,set,parallelism,parallelism,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1488437582;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(parallelism)___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,set,parallelism,parallelism,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1489419493;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(parallelism)___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,set,parallelism,parallelism,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1493975167;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1494830990;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1495175928;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1495175928;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1495923077;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1498894422;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1498894422;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1499314317;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1501249950;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1507568316;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1509723634;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1515177485;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1519973085;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1519973085;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1523020981;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1523020981;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					env.execute(consumeExtraRecordsJobName)__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		JobManagerCommunicationUtils.waitUntilJobIsRunning(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		JobManagerCommunicationUtils.cancelCurrentJob(_			flink.getLeaderGateway(timeout),_			consumeExtraRecordsJobName)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,env,execute,consume,extra,records,job,name,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,set,t,consume,thread,start,job,manager,communication,utils,wait,until,job,is,running,flink,get,leader,gateway,timeout,consume,extra,records,job,name,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,consume,extra,records,job,name,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1523020981;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID consumeJobId = jobGraph.getJobID()___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		waitUntilJobIsRunning(client)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		client.cancel(consumeJobId)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,consume,job,id,job,graph,get,job,id,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,error,set,t,consume,thread,start,wait,until,job,is,running,client,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,client,cancel,consume,job,id,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1525452496;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID consumeJobId = jobGraph.getJobID()___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		waitUntilJobIsRunning(client)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		client.cancel(consumeJobId)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,consume,job,id,job,graph,get,job,id,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,error,set,t,consume,thread,start,wait,until,job,is,running,client,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,client,cancel,consume,job,id,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1539704473;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID consumeJobId = jobGraph.getJobID()___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		waitUntilJobIsRunning(client)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		client.cancel(consumeJobId)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,consume,job,id,job,graph,get,job,id,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,error,set,t,consume,thread,start,wait,until,job,is,running,client,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,client,cancel,consume,job,id,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1549282380;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KeyedDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KeyedDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID consumeJobId = jobGraph.getJobID()___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		waitUntilJobIsRunning(client)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		client.cancel(consumeJobId)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,keyed,deserialization,schema,tuple2,integer,integer,deser,schema,new,keyed,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,consume,job,id,job,graph,get,job,id,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,error,set,t,consume,thread,start,wait,until,job,is,running,client,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,client,cancel,consume,job,id,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runStartFromLatestOffsets() throws Exception;1550834396;This test ensures that when explicitly set to start from latest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromLatestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		_		final int extraRecordsInEachPartition = 200___		_		final String topicName = writeSequence("testStartFromLatestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		_		final String consumeExtraRecordsJobName = "Consume Extra Records Job"__		final String writeExtraRecordsJobName = "Write Extra Records Job"___		_		final TypeInformation<Tuple2<Integer, Integer>> resultType =_			TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>() {})___		final KeyedSerializationSchema<Tuple2<Integer, Integer>> serSchema =_			new KeyedSerializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		final KafkaDeserializationSchema<Tuple2<Integer, Integer>> deserSchema =_			new KafkaDeserializationSchemaWrapper<>(_				new TypeInformationSerializationSchema<>(resultType, new ExecutionConfig()))___		_		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		final Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> latestReadingConsumer =_			kafkaServer.getConsumer(topicName, deserSchema, readProps)__		latestReadingConsumer.setStartFromLatest()___		env_			.addSource(latestReadingConsumer).setParallelism(parallelism)_			.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Object>() {_				@Override_				public void flatMap(Tuple2<Integer, Integer> value, Collector<Object> out) throws Exception {_					if (value.f1 - recordsInEachPartition < 0) {_						throw new RuntimeException("test failed_ consumed a record that was previously written: " + value)__					}_				}_			}).setParallelism(1)_			.addSink(new DiscardingSink<>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID consumeJobId = jobGraph.getJobID()___		final AtomicReference<Throwable> error = new AtomicReference<>()__		Thread consumeThread = new Thread(new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						error.set(t)__					}_				}_			}_		})__		consumeThread.start()___		_		waitUntilJobIsRunning(client)___		_		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()___		env2.setParallelism(parallelism)___		DataStream<Tuple2<Integer, Integer>> extraRecordsStream = env2_			.addSource(new RichParallelSourceFunction<Tuple2<Integer, Integer>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_					int count = recordsInEachPartition_ _					int partition = getRuntimeContext().getIndexOfThisSubtask()___					while (running && count < recordsInEachPartition + extraRecordsInEachPartition) {_						ctx.collect(new Tuple2<>(partition, count))__						count++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})___		kafkaServer.produceIntoKafka(extraRecordsStream, topicName, serSchema, readProps, null)___		try {_			env2.execute(writeExtraRecordsJobName)__		}_		catch (Exception e) {_			throw new RuntimeException("Writing extra records failed", e)__		}__		_		client.cancel(consumeJobId)__		consumeThread.join()___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)___		_		_		final Throwable consumerError = error.get()__		if (consumerError != null) {_			throw new Exception("Exception in the consuming thread", consumerError)__		}_	};this,test,ensures,that,when,explicitly,set,to,start,from,latest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,latest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,int,extra,records,in,each,partition,200,final,string,topic,name,write,sequence,test,start,from,latest,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,final,string,consume,extra,records,job,name,consume,extra,records,job,final,string,write,extra,records,job,name,write,extra,records,job,final,type,information,tuple2,integer,integer,result,type,type,information,of,new,type,hint,tuple2,integer,integer,final,keyed,serialization,schema,tuple2,integer,integer,ser,schema,new,keyed,serialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,kafka,deserialization,schema,tuple2,integer,integer,deser,schema,new,kafka,deserialization,schema,wrapper,new,type,information,serialization,schema,result,type,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,final,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,flink,kafka,consumer,base,tuple2,integer,integer,latest,reading,consumer,kafka,server,get,consumer,topic,name,deser,schema,read,props,latest,reading,consumer,set,start,from,latest,env,add,source,latest,reading,consumer,set,parallelism,parallelism,flat,map,new,flat,map,function,tuple2,integer,integer,object,override,public,void,flat,map,tuple2,integer,integer,value,collector,object,out,throws,exception,if,value,f1,records,in,each,partition,0,throw,new,runtime,exception,test,failed,consumed,a,record,that,was,previously,written,value,set,parallelism,1,add,sink,new,discarding,sink,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,consume,job,id,job,graph,get,job,id,final,atomic,reference,throwable,error,new,atomic,reference,thread,consume,thread,new,thread,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,error,set,t,consume,thread,start,wait,until,job,is,running,client,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,set,parallelism,parallelism,data,stream,tuple2,integer,integer,extra,records,stream,env2,add,source,new,rich,parallel,source,function,tuple2,integer,integer,private,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,count,records,in,each,partition,int,partition,get,runtime,context,get,index,of,this,subtask,while,running,count,records,in,each,partition,extra,records,in,each,partition,ctx,collect,new,tuple2,partition,count,count,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,extra,records,stream,topic,name,ser,schema,read,props,null,try,env2,execute,write,extra,records,job,name,catch,exception,e,throw,new,runtime,exception,writing,extra,records,failed,e,client,cancel,consume,job,id,consume,thread,join,kafka,offset,handler,close,delete,test,topic,topic,name,final,throwable,consumer,error,error,get,if,consumer,error,null,throw,new,exception,exception,in,the,consuming,thread,consumer,error
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1480685315;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler(standardProps)___		final Long l50 = 50L_ _		final long deadline = 30000 + System.currentTimeMillis()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.currentTimeMillis() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,standard,props,final,long,l50,50l,final,long,deadline,30000,system,current,time,millis,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,current,time,millis,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1484866642;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler(standardProps)___		final Long l50 = 50L_ _		final long deadline = 30000 + System.currentTimeMillis()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.currentTimeMillis() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,standard,props,final,long,l50,50l,final,long,deadline,30000,system,current,time,millis,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,current,time,millis,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1487173364;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30000 + System.currentTimeMillis()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.currentTimeMillis() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,30000,system,current,time,millis,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,current,time,millis,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1488437582;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1489419493;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1493975167;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1494830990;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1495175928;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1495175928;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1495923077;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1498894422;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1498894422;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1499314317;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1501249950;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1507568316;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1509723634;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1515177485;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1519973085;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1519973085;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1523020981;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1523020981;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1523020981;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1525452496;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1539704473;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1549282380;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception;1550834396;This test ensures that when the consumers retrieve some start offset from kafka (earliest, latest), that this offset_is committed to Kafka, even if some partitions are not read.__<p>Test:_- Create 3 partitions_- write 50 messages into each._- Start three consumers with auto.offset.reset='latest' and wait until they committed into Kafka._- Check if the offsets in Kafka are set to 50 for the three partitions__<p>See FLINK-3440 as well;public void runAutoOffsetRetrievalAndCommitToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testAutoOffsetRetrievalAndCommitToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), readProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()__		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,the,consumers,retrieve,some,start,offset,from,kafka,earliest,latest,that,this,offset,is,committed,to,kafka,even,if,some,partitions,are,not,read,p,test,create,3,partitions,write,50,messages,into,each,start,three,consumers,with,auto,offset,reset,latest,and,wait,until,they,committed,into,kafka,check,if,the,offsets,in,kafka,are,set,to,50,for,the,three,partitions,p,see,flink,3440,as,well;public,void,run,auto,offset,retrieval,and,commit,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,auto,offset,retrieval,and,commit,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,read,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,final,long,l50,50l,final,long,deadline,system,nano,time,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1519973085;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1523020981;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1523020981;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1523020981;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1525452496;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1539704473;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1549282380;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromTimestamp() throws Exception;1550834396;This test ensures that the consumer correctly uses user-supplied timestamp when explicitly configured to_start from timestamp.__<p>The validated Kafka data is written in 2 steps: first, an initial 50 records is written to each partition._After that, another 30 records is appended to each partition. Before each step, a timestamp is recorded._For the validation, when the read job is configured to start from the first timestamp, each partition should start_from offset 0 and read a total of 80 records. When configured to start from the second timestamp,_each partition should start from offset 50 and read on the remaining 30 appended records.;public void runStartFromTimestamp() throws Exception {_		_		final int parallelism = 4__		final int initialRecordsInEachPartition = 50__		final int appendRecordsInEachPartition = 30___		_		_		long firstTimestamp = System.currentTimeMillis()__		String topic = writeSequence("runStartFromTimestamp", initialRecordsInEachPartition, parallelism, 1)___		long secondTimestamp = 0__		while (secondTimestamp <= firstTimestamp) {_			Thread.sleep(1000)__			secondTimestamp = System.currentTimeMillis()__		}_		writeAppendSequence(topic, initialRecordsInEachPartition, appendRecordsInEachPartition, parallelism)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)___		readSequence(env, StartupMode.TIMESTAMP, null, firstTimestamp,_			readProps, parallelism, topic, initialRecordsInEachPartition + appendRecordsInEachPartition, 0)__		readSequence(env, StartupMode.TIMESTAMP, null, secondTimestamp,_			readProps, parallelism, topic, appendRecordsInEachPartition, initialRecordsInEachPartition)___		deleteTestTopic(topic)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,timestamp,when,explicitly,configured,to,start,from,timestamp,p,the,validated,kafka,data,is,written,in,2,steps,first,an,initial,50,records,is,written,to,each,partition,after,that,another,30,records,is,appended,to,each,partition,before,each,step,a,timestamp,is,recorded,for,the,validation,when,the,read,job,is,configured,to,start,from,the,first,timestamp,each,partition,should,start,from,offset,0,and,read,a,total,of,80,records,when,configured,to,start,from,the,second,timestamp,each,partition,should,start,from,offset,50,and,read,on,the,remaining,30,appended,records;public,void,run,start,from,timestamp,throws,exception,final,int,parallelism,4,final,int,initial,records,in,each,partition,50,final,int,append,records,in,each,partition,30,long,first,timestamp,system,current,time,millis,string,topic,write,sequence,run,start,from,timestamp,initial,records,in,each,partition,parallelism,1,long,second,timestamp,0,while,second,timestamp,first,timestamp,thread,sleep,1000,second,timestamp,system,current,time,millis,write,append,sequence,topic,initial,records,in,each,partition,append,records,in,each,partition,parallelism,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,sequence,env,startup,mode,timestamp,null,first,timestamp,read,props,parallelism,topic,initial,records,in,each,partition,append,records,in,each,partition,0,read,sequence,env,startup,mode,timestamp,null,second,timestamp,read,props,parallelism,topic,append,records,in,each,partition,initial,records,in,each,partition,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1495923077;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1498894422;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1498894422;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1499314317;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1501249950;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1507568316;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1509723634;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1515177485;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1519973085;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1519973085;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1523020981;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1523020981;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1523020981;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1525452496;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType =_				TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,information,of,new,type,hint,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1539704473;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType =_				TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,information,of,new,type,hint,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1549282380;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType =_				TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,information,of,new,type,hint,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1550834396;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__<p>We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__<p>This test also ensures that FLINK-3156 doesn't happen again:__<p>The following situation caused a NPE in the FlinkKafkaConsumer__<p>topic-1 <-- elements are only produced into topic1._topic-2__<p>Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times = 2, exception = kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType =_				TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long, String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition___				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,p,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,p,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,p,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,p,topic,1,elements,are,only,produced,into,topic1,topic,2,p,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,information,of,new,type,hint,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1480685315;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1484866642;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1487173364;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1488437582;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1489419493;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1493975167;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1494830990;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1495175928;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1495175928;Test Flink's Kafka integration also with very big records (30MB)_see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been "+elCnt+" elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: "+elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1495923077;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1498894422;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1498894422;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1499314317;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1501249950;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1507568316;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1509723634;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1515177485;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1519973085;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1519973085;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1523020981;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1523020981;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1523020981;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo = TypeInfoParser.parse("Tuple2<Long, byte[]>")___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,info,parser,parse,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1525452496;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo =_				TypeInformation.of(new TypeHint<Tuple2<Long, byte[]>>(){})___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,information,of,new,type,hint,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1539704473;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo =_				TypeInformation.of(new TypeHint<Tuple2<Long, byte[]>>(){})___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,information,of,new,type,hint,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1549282380;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo =_				TypeInformation.of(new TypeHint<Tuple2<Long, byte[]>>(){})___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,information,of,new,type,hint,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runBigRecordTestTopology() throws Exception;1550834396;Test Flink's Kafka integration also with very big records (30MB).__<p>see http://stackoverflow.com/questions/21020347/kafka-sending-a-15mb-message;public void runBigRecordTestTopology() throws Exception {__		final String topic = "bigRecordTestTopic"__		final int parallelism = 1_ __		createTestTopic(topic, parallelism, 1)___		final TypeInformation<Tuple2<Long, byte[]>> longBytesInfo =_				TypeInformation.of(new TypeHint<Tuple2<Long, byte[]>>(){})___		final TypeInformationSerializationSchema<Tuple2<Long, byte[]>> serSchema =_				new TypeInformationSerializationSchema<>(longBytesInfo, new ExecutionConfig())___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()__		env.enableCheckpointing(100)__		env.setParallelism(parallelism)___		_		Properties consumerProps = new Properties()__		consumerProps.putAll(standardProps)__		consumerProps.setProperty("fetch.message.max.bytes", Integer.toString(1024 * 1024 * 14))__		consumerProps.setProperty("max.partition.fetch.bytes", Integer.toString(1024 * 1024 * 14))_ _		consumerProps.setProperty("queued.max.message.chunks", "1")__		consumerProps.putAll(secureProps)___		FlinkKafkaConsumerBase<Tuple2<Long, byte[]>> source = kafkaServer.getConsumer(topic, serSchema, consumerProps)__		DataStreamSource<Tuple2<Long, byte[]>> consuming = env.addSource(source)___		consuming.addSink(new SinkFunction<Tuple2<Long, byte[]>>() {__			private int elCnt = 0___			@Override_			public void invoke(Tuple2<Long, byte[]> value) throws Exception {_				elCnt++__				if (value.f0 == -1) {_					_					if (elCnt == 11) {_						throw new SuccessException()__					} else {_						throw new RuntimeException("There have been " + elCnt + " elements")__					}_				}_				if (elCnt > 10) {_					throw new RuntimeException("More than 10 elements seen: " + elCnt)__				}_			}_		})___		_		Properties producerProps = new Properties()__		producerProps.setProperty("max.request.size", Integer.toString(1024 * 1024 * 15))__		producerProps.setProperty("retries", "3")__		producerProps.putAll(secureProps)__		producerProps.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerConnectionStrings)___		DataStream<Tuple2<Long, byte[]>> stream = env.addSource(new RichSourceFunction<Tuple2<Long, byte[]>>() {__			private boolean running___			@Override_			public void open(Configuration parameters) throws Exception {_				super.open(parameters)__				running = true__			}__			@Override_			public void run(SourceContext<Tuple2<Long, byte[]>> ctx) throws Exception {_				Random rnd = new Random()__				long cnt = 0__				int sevenMb = 1024 * 1024 * 7___				while (running) {_					byte[] wl = new byte[sevenMb + rnd.nextInt(sevenMb)]__					ctx.collect(new Tuple2<>(cnt++, wl))___					Thread.sleep(100)___					if (cnt == 10) {_						_						ctx.collect(new Tuple2<>(-1L, new byte[]{1}))__						break__					}_				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(serSchema), producerProps, null)___		tryExecute(env, "big topology test")__		deleteTestTopic(topic)__	};test,flink,s,kafka,integration,also,with,very,big,records,30mb,p,see,http,stackoverflow,com,questions,21020347,kafka,sending,a,15mb,message;public,void,run,big,record,test,topology,throws,exception,final,string,topic,big,record,test,topic,final,int,parallelism,1,create,test,topic,topic,parallelism,1,final,type,information,tuple2,long,byte,long,bytes,info,type,information,of,new,type,hint,tuple2,long,byte,final,type,information,serialization,schema,tuple2,long,byte,ser,schema,new,type,information,serialization,schema,long,bytes,info,new,execution,config,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,env,enable,checkpointing,100,env,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,set,property,fetch,message,max,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,max,partition,fetch,bytes,integer,to,string,1024,1024,14,consumer,props,set,property,queued,max,message,chunks,1,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,byte,source,kafka,server,get,consumer,topic,ser,schema,consumer,props,data,stream,source,tuple2,long,byte,consuming,env,add,source,source,consuming,add,sink,new,sink,function,tuple2,long,byte,private,int,el,cnt,0,override,public,void,invoke,tuple2,long,byte,value,throws,exception,el,cnt,if,value,f0,1,if,el,cnt,11,throw,new,success,exception,else,throw,new,runtime,exception,there,have,been,el,cnt,elements,if,el,cnt,10,throw,new,runtime,exception,more,than,10,elements,seen,el,cnt,properties,producer,props,new,properties,producer,props,set,property,max,request,size,integer,to,string,1024,1024,15,producer,props,set,property,retries,3,producer,props,put,all,secure,props,producer,props,set,property,producer,config,broker,connection,strings,data,stream,tuple2,long,byte,stream,env,add,source,new,rich,source,function,tuple2,long,byte,private,boolean,running,override,public,void,open,configuration,parameters,throws,exception,super,open,parameters,running,true,override,public,void,run,source,context,tuple2,long,byte,ctx,throws,exception,random,rnd,new,random,long,cnt,0,int,seven,mb,1024,1024,7,while,running,byte,wl,new,byte,seven,mb,rnd,next,int,seven,mb,ctx,collect,new,tuple2,cnt,wl,thread,sleep,100,if,cnt,10,ctx,collect,new,tuple2,1l,new,byte,1,break,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,producer,props,null,try,execute,env,big,topology,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1489419493;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1493975167;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1494830990;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1495175928;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1495175928;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1495923077;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1498894422;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1498894422;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1499314317;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1501249950;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1507568316;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1509723634;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1515177485;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1519973085;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1519973085;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1523020981;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1523020981;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1523020981;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1525452496;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1539704473;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1549282380;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromSpecificOffsets() throws Exception;1550834396;This test ensures that the consumer correctly uses user-supplied specific offsets when explicitly configured to_start from specific offsets. For partitions which a specific offset can not be found for, the starting position_for them should fallback to the group offsets behaviour.__<p>4 partitions will have 50 records with offsets 0 to 49. The supplied specific offsets map is:_partition 0 --> start from offset 19_partition 1 --> not set_partition 2 --> start from offset 22_partition 3 --> not set_partition 4 --> start from offset 26 (this should be ignored because the partition does not exist)__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> committed offset 31_partition 2 --> committed offset 43_partition 3 --> no commit offset__<p>When configured to start from these specific offsets, each partition should read:_partition 0 --> start from offset 19, read to offset 49 (31 records)_partition 1 --> fallback to group offsets, so start from offset 31, read to offset 49 (19 records)_partition 2 --> start from offset 22, read to offset 49 (28 records)_partition 3 --> fallback to group offsets, but since there is no group offset for this partition,_will default to "auto.offset.reset" (set to "earliest"),_so start from offset 0, read to offset 49 (50 records);public void runStartFromSpecificOffsets() throws Exception {_		_		final int parallelism = 4__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromSpecificOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")_ __		Map<KafkaTopicPartition, Long> specificStartupOffsets = new HashMap<>()__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 0), 19L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 2), 22L)__		specificStartupOffsets.put(new KafkaTopicPartition(topicName, 4), 26L)_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(31, 19))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(19, 31))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(28, 22))_	_		partitionsToValueCountAndStartOffsets.put(3, new Tuple2<>(50, 0))_	__		readSequence(env, StartupMode.SPECIFIC_OFFSETS, specificStartupOffsets, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,user,supplied,specific,offsets,when,explicitly,configured,to,start,from,specific,offsets,for,partitions,which,a,specific,offset,can,not,be,found,for,the,starting,position,for,them,should,fallback,to,the,group,offsets,behaviour,p,4,partitions,will,have,50,records,with,offsets,0,to,49,the,supplied,specific,offsets,map,is,partition,0,start,from,offset,19,partition,1,not,set,partition,2,start,from,offset,22,partition,3,not,set,partition,4,start,from,offset,26,this,should,be,ignored,because,the,partition,does,not,exist,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,committed,offset,31,partition,2,committed,offset,43,partition,3,no,commit,offset,p,when,configured,to,start,from,these,specific,offsets,each,partition,should,read,partition,0,start,from,offset,19,read,to,offset,49,31,records,partition,1,fallback,to,group,offsets,so,start,from,offset,31,read,to,offset,49,19,records,partition,2,start,from,offset,22,read,to,offset,49,28,records,partition,3,fallback,to,group,offsets,but,since,there,is,no,group,offset,for,this,partition,will,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records;public,void,run,start,from,specific,offsets,throws,exception,final,int,parallelism,4,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,specific,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,map,kafka,topic,partition,long,specific,startup,offsets,new,hash,map,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,0,19l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,2,22l,specific,startup,offsets,put,new,kafka,topic,partition,topic,name,4,26l,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,31,19,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,19,31,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,28,22,partitions,to,value,count,and,start,offsets,put,3,new,tuple2,50,0,read,sequence,env,startup,mode,specific,startup,offsets,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1519973085;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1523020981;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1523020981;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1523020981;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1525452496;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1539704473;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1549282380;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1550834396;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Long, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, startupTimestamp, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,long,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,startup,timestamp,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1487173364;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1488437582;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1489419493;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1493975167;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1494830990;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1495175928;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1495175928;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1495923077;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1498894422;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1498894422;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1499314317;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1501249950;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1507568316;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1509723634;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1515177485;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1519973085;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1519973085;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1523020981;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1523020981;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1523020981;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1525452496;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1539704473;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1549282380;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromGroupOffsets() throws Exception;1550834396;This test ensures that the consumer correctly uses group offsets in Kafka, and defaults to "auto.offset.reset"_behaviour when necessary, when explicitly configured to start from group offsets.__<p>The partitions and their committed group offsets are setup as:_partition 0 --> committed offset 23_partition 1 --> no commit offset_partition 2 --> committed offset 43__<p>When configured to start from group offsets, each partition should read:_partition 0 --> start from offset 23, read to offset 49 (27 records)_partition 1 --> default to "auto.offset.reset" (set to earliest), so start from offset 0, read to offset 49 (50 records)_partition 2 --> start from offset 43, read to offset 49 (7 records);public void runStartFromGroupOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromGroupOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "earliest")___		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		_		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		Map<Integer, Tuple2<Integer, Integer>> partitionsToValueCountAndStartOffsets = new HashMap<>()__		partitionsToValueCountAndStartOffsets.put(0, new Tuple2<>(27, 23))_ _		partitionsToValueCountAndStartOffsets.put(1, new Tuple2<>(50, 0))_ _		partitionsToValueCountAndStartOffsets.put(2, new Tuple2<>(7, 43))_	__		readSequence(env, StartupMode.GROUP_OFFSETS, null, null, readProps, topicName, partitionsToValueCountAndStartOffsets)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,the,consumer,correctly,uses,group,offsets,in,kafka,and,defaults,to,auto,offset,reset,behaviour,when,necessary,when,explicitly,configured,to,start,from,group,offsets,p,the,partitions,and,their,committed,group,offsets,are,setup,as,partition,0,committed,offset,23,partition,1,no,commit,offset,partition,2,committed,offset,43,p,when,configured,to,start,from,group,offsets,each,partition,should,read,partition,0,start,from,offset,23,read,to,offset,49,27,records,partition,1,default,to,auto,offset,reset,set,to,earliest,so,start,from,offset,0,read,to,offset,49,50,records,partition,2,start,from,offset,43,read,to,offset,49,7,records;public,void,run,start,from,group,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,group,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,earliest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,2,43,map,integer,tuple2,integer,integer,partitions,to,value,count,and,start,offsets,new,hash,map,partitions,to,value,count,and,start,offsets,put,0,new,tuple2,27,23,partitions,to,value,count,and,start,offsets,put,1,new,tuple2,50,0,partitions,to,value,count,and,start,offsets,put,2,new,tuple2,7,43,read,sequence,env,startup,mode,null,null,read,props,topic,name,partitions,to,value,count,and,start,offsets,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1480685315;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1484866642;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1487173364;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1488437582;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1489419493;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1493975167;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1494830990;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1495175928;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1495175928;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1495923077;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1498894422;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1498894422;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1499314317;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1501249950;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1507568316;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1509723634;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1515177485;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1519973085;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1519973085;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1523020981;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> @Before 	public void ensureNoJobIsLingering() throws Exception;1523020981;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void ensureNoJobIsLingering() throws Exception {_		JobManagerCommunicationUtils.waitUntilNoJobIsRunning(flink.getLeaderGateway(timeout))__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,ensure,no,job,is,lingering,throws,exception,job,manager,communication,utils,wait,until,no,job,is,running,flink,get,leader,gateway,timeout
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1480685315;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1484866642;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1487173364;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1488437582;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1489419493;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1493975167;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1494830990;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1495175928;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1495175928;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")____		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1495923077;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1498894422;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1498894422;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1499314317;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1501249950;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1507568316;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1509723634;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1515177485;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1519973085;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1519973085;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1525452496;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1539704473;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1549282380;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception;1550834396;Tests the proper consumption when having more Flink sources than Kafka partitions, which means_that some Flink sources will read no partitions.;public void runMultipleSourcesOnePartitionExactlyOnceTest() throws Exception {_		final String topic = "manyToOneTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 8___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		_		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()__		env.setBufferTimeout(0)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_			.addSource(kafkaSource)_			.map(new PartitionValidatingMapper(numPartitions, 1))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))_			.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "multi-source-one-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,more,flink,sources,than,kafka,partitions,which,means,that,some,flink,sources,will,read,no,partitions;public,void,run,multiple,sources,one,partition,exactly,once,test,throws,exception,final,string,topic,many,to,one,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,8,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,env,set,buffer,timeout,0,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,multi,source,one,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> protected void readSequence(StreamExecutionEnvironment env, Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, final int startFrom) throws Exception;1480685315;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(StreamExecutionEnvironment env, Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount, final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,stream,execution,environment,env,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(StreamExecutionEnvironment env, Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, final int startFrom) throws Exception;1484866642;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(StreamExecutionEnvironment env, Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount, final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,stream,execution,environment,env,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> @Before 	public void setClientAndEnsureNoJobIsLingering() throws Exception;1523020981;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void setClientAndEnsureNoJobIsLingering() throws Exception {_		client = flink.getClusterClient()__		waitUntilNoJobIsRunning(client)__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,set,client,and,ensure,no,job,is,lingering,throws,exception,client,flink,get,cluster,client,wait,until,no,job,is,running,client
KafkaConsumerTestBase -> @Before 	public void setClientAndEnsureNoJobIsLingering() throws Exception;1525452496;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void setClientAndEnsureNoJobIsLingering() throws Exception {_		client = flink.getClusterClient()__		waitUntilNoJobIsRunning(client)__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,set,client,and,ensure,no,job,is,lingering,throws,exception,client,flink,get,cluster,client,wait,until,no,job,is,running,client
KafkaConsumerTestBase -> @Before 	public void setClientAndEnsureNoJobIsLingering() throws Exception;1539704473;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void setClientAndEnsureNoJobIsLingering() throws Exception {_		client = flink.getClusterClient()__		waitUntilNoJobIsRunning(client)__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,set,client,and,ensure,no,job,is,lingering,throws,exception,client,flink,get,cluster,client,wait,until,no,job,is,running,client
KafkaConsumerTestBase -> @Before 	public void setClientAndEnsureNoJobIsLingering() throws Exception;1549282380;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void setClientAndEnsureNoJobIsLingering() throws Exception {_		client = flink.getClusterClient()__		waitUntilNoJobIsRunning(client)__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,set,client,and,ensure,no,job,is,lingering,throws,exception,client,flink,get,cluster,client,wait,until,no,job,is,running,client
KafkaConsumerTestBase -> @Before 	public void setClientAndEnsureNoJobIsLingering() throws Exception;1550834396;Makes sure that no job is on the JobManager any more from any previous tests that use_the same mini cluster. Otherwise, missing slots may happen.;@Before_	public void setClientAndEnsureNoJobIsLingering() throws Exception {_		client = flink.getClusterClient()__		waitUntilNoJobIsRunning(client)__	};makes,sure,that,no,job,is,on,the,job,manager,any,more,from,any,previous,tests,that,use,the,same,mini,cluster,otherwise,missing,slots,may,happen;before,public,void,set,client,and,ensure,no,job,is,lingering,throws,exception,client,flink,get,cluster,client,wait,until,no,job,is,running,client
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1480685315;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1484866642;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1487173364;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1488437582;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1489419493;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1493975167;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1494830990;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1495175928;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1495175928;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String,Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got "+streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: "+streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got "+kafkaStreams.size()+" streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while(iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read "+read+" elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1495923077;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1498894422;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1498894422;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1499314317;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1501249950;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1507568316;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1509723634;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1515177485;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1519973085;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1519973085;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1523020981;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1523020981;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1523020981;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter);1525452496;Read topic to list, only using Kafka code.;private static List<MessageAndMetadata<byte[], byte[]>> readTopicToList(String topicName, ConsumerConfig config, final int stopAfter) {_		ConsumerConnector consumerConnector = Consumer.createJavaConsumerConnector(config)__		_		_		Map<String, Integer> topicCountMap = Collections.singletonMap(topicName, 1)__		Map<String, List<KafkaStream<byte[], byte[]>>> streams = consumerConnector.createMessageStreams(topicCountMap)__		if (streams.size() != 1) {_			throw new RuntimeException("Expected only one message stream but got " + streams.size())__		}_		List<KafkaStream<byte[], byte[]>> kafkaStreams = streams.get(topicName)__		if (kafkaStreams == null) {_			throw new RuntimeException("Requested stream not available. Available streams: " + streams.toString())__		}_		if (kafkaStreams.size() != 1) {_			throw new RuntimeException("Requested 1 stream from Kafka, bot got " + kafkaStreams.size() + " streams")__		}_		LOG.info("Opening Consumer instance for topic '{}' on group '{}'", topicName, config.groupId())__		ConsumerIterator<byte[], byte[]> iteratorToRead = kafkaStreams.get(0).iterator()___		List<MessageAndMetadata<byte[], byte[]>> result = new ArrayList<>()__		int read = 0__		while (iteratorToRead.hasNext()) {_			read++__			result.add(iteratorToRead.next())__			if (read == stopAfter) {_				LOG.info("Read " + read + " elements")__				return result__			}_		}_		return result__	};read,topic,to,list,only,using,kafka,code;private,static,list,message,and,metadata,byte,byte,read,topic,to,list,string,topic,name,consumer,config,config,final,int,stop,after,consumer,connector,consumer,connector,consumer,create,java,consumer,connector,config,map,string,integer,topic,count,map,collections,singleton,map,topic,name,1,map,string,list,kafka,stream,byte,byte,streams,consumer,connector,create,message,streams,topic,count,map,if,streams,size,1,throw,new,runtime,exception,expected,only,one,message,stream,but,got,streams,size,list,kafka,stream,byte,byte,kafka,streams,streams,get,topic,name,if,kafka,streams,null,throw,new,runtime,exception,requested,stream,not,available,available,streams,streams,to,string,if,kafka,streams,size,1,throw,new,runtime,exception,requested,1,stream,from,kafka,bot,got,kafka,streams,size,streams,log,info,opening,consumer,instance,for,topic,on,group,topic,name,config,group,id,consumer,iterator,byte,byte,iterator,to,read,kafka,streams,get,0,iterator,list,message,and,metadata,byte,byte,result,new,array,list,int,read,0,while,iterator,to,read,has,next,read,result,add,iterator,to,read,next,if,read,stop,after,log,info,read,read,elements,return,result,return,result
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1480685315;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler(standardProps)___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(50))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == 150) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,standard,props,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,50,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,150,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1484866642;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler(standardProps)___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(50))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == 150) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,standard,props,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,50,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,150,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1487173364;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(50))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == 150) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,50,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,150,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1488437582;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(50))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == 150) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,50,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,150,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1489419493;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(50))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == 150) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,50,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,150,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,create,remote,environment,localhost,flink,port,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1493975167;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300__		final int recordsToConsume = 150__		final int consumePause = 50___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(consumePause))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == recordsToConsume) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,int,records,to,consume,150,final,int,consume,pause,50,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,consume,pause,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,records,to,consume,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1494830990;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300__		final int recordsToConsume = 150__		final int consumePause = 50___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(consumePause))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == recordsToConsume) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,int,records,to,consume,150,final,int,consume,pause,50,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,consume,pause,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,records,to,consume,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1495175928;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300__		final int recordsToConsume = 150__		final int consumePause = 50___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(consumePause))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == recordsToConsume) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,int,records,to,consume,150,final,int,consume,pause,50,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,consume,pause,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,records,to,consume,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1495175928;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300__		final int recordsToConsume = 150__		final int consumePause = 50___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(consumePause))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == recordsToConsume) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,int,records,to,consume,150,final,int,consume,pause,50,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,consume,pause,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,records,to,consume,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1495923077;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300__		final int recordsToConsume = 150__		final int consumePause = 50___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(consumePause))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == recordsToConsume) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,int,records,to,consume,150,final,int,consume,pause,50,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,consume,pause,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,records,to,consume,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromKafkaCommitOffsets() throws Exception;1498894422;This test first writes a total of 300 records to a test topic, reads the first 150 so that some offsets are_committed to Kafka, and then startup the consumer again to read the remaining records starting from the committed offsets._The test ensures that whatever offsets were committed to Kafka, the consumer correctly picks them up_and starts at the correct position.;public void runStartFromKafkaCommitOffsets() throws Exception {_		final int parallelism = 3__		final int recordsInEachPartition = 300__		final int recordsToConsume = 150__		final int consumePause = 50___		final String topicName = writeSequence("testStartFromKafkaCommitOffsetsTopic", recordsInEachPartition, parallelism, 1)___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		Long o1__		Long o2__		Long o3__		int attempt = 0__		_		do {_			attempt++__			LOG.info("Attempt " + attempt + " to read records and commit some offsets to Kafka")___			final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.getConfig().disableSysoutLogging()__			env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__			env.setParallelism(parallelism)__			env.enableCheckpointing(20)_ __			env_				.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))_				.map(new ThrottledMapper<String>(consumePause))_				.map(new MapFunction<String, Object>() {_					int count = 0__					@Override_					public Object map(String value) throws Exception {_						count++__						if (count == recordsToConsume) {_							throw new SuccessException()__						}_						return null__					}_				})_				.addSink(new DiscardingSink<>())___			tryExecute(env, "Read some records to commit offsets to Kafka")___			o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		} while (o1 == null && o2 == null && o3 == null && attempt < 3)___		if (o1 == null && o2 == null && o3 == null) {_			throw new RuntimeException("No offsets have been committed after 3 attempts")__		}__		LOG.info("Got final committed offsets from Kafka o1={}, o2={}, o3={}", o1, o2, o3)___		final StreamExecutionEnvironment env2 = StreamExecutionEnvironment.getExecutionEnvironment()__		env2.getConfig().disableSysoutLogging()__		env2.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env2.setParallelism(parallelism)___		_		_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		partitionsToValuesCountAndStartOffset.put(0, new Tuple2<>(_			(o1 != null) ? (int) (recordsInEachPartition - o1) : recordsInEachPartition,_			(o1 != null) ? o1.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(1, new Tuple2<>(_			(o2 != null) ? (int) (recordsInEachPartition - o2) : recordsInEachPartition,_			(o2 != null) ? o2.intValue() : 0_		))__		partitionsToValuesCountAndStartOffset.put(2, new Tuple2<>(_			(o3 != null) ? (int) (recordsInEachPartition - o3) : recordsInEachPartition,_			(o3 != null) ? o3.intValue() : 0_		))___		readSequence(env2, StartupMode.GROUP_OFFSETS, null, standardProps, topicName, partitionsToValuesCountAndStartOffset)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,first,writes,a,total,of,300,records,to,a,test,topic,reads,the,first,150,so,that,some,offsets,are,committed,to,kafka,and,then,startup,the,consumer,again,to,read,the,remaining,records,starting,from,the,committed,offsets,the,test,ensures,that,whatever,offsets,were,committed,to,kafka,the,consumer,correctly,picks,them,up,and,starts,at,the,correct,position;public,void,run,start,from,kafka,commit,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,300,final,int,records,to,consume,150,final,int,consume,pause,50,final,string,topic,name,write,sequence,test,start,from,kafka,commit,offsets,topic,records,in,each,partition,parallelism,1,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,long,o1,long,o2,long,o3,int,attempt,0,do,attempt,log,info,attempt,attempt,to,read,records,and,commit,some,offsets,to,kafka,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,20,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,map,new,throttled,mapper,string,consume,pause,map,new,map,function,string,object,int,count,0,override,public,object,map,string,value,throws,exception,count,if,count,records,to,consume,throw,new,success,exception,return,null,add,sink,new,discarding,sink,try,execute,env,read,some,records,to,commit,offsets,to,kafka,o1,kafka,offset,handler,get,committed,offset,topic,name,0,o2,kafka,offset,handler,get,committed,offset,topic,name,1,o3,kafka,offset,handler,get,committed,offset,topic,name,2,while,o1,null,o2,null,o3,null,attempt,3,if,o1,null,o2,null,o3,null,throw,new,runtime,exception,no,offsets,have,been,committed,after,3,attempts,log,info,got,final,committed,offsets,from,kafka,o1,o2,o3,o1,o2,o3,final,stream,execution,environment,env2,stream,execution,environment,get,execution,environment,env2,get,config,disable,sysout,logging,env2,get,config,set,restart,strategy,restart,strategies,no,restart,env2,set,parallelism,parallelism,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,partitions,to,values,count,and,start,offset,put,0,new,tuple2,o1,null,int,records,in,each,partition,o1,records,in,each,partition,o1,null,o1,int,value,0,partitions,to,values,count,and,start,offset,put,1,new,tuple2,o2,null,int,records,in,each,partition,o2,records,in,each,partition,o2,null,o2,int,value,0,partitions,to,values,count,and,start,offset,put,2,new,tuple2,o3,null,int,records,in,each,partition,o3,records,in,each,partition,o3,null,o3,int,value,0,read,sequence,env2,startup,mode,null,standard,props,topic,name,partitions,to,values,count,and,start,offset,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1480685315;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30000 + System.currentTimeMillis()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler(standardProps)___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.currentTimeMillis() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,30000,system,current,time,millis,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,standard,props,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,current,time,millis,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1484866642;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30000 + System.currentTimeMillis()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler(standardProps)___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.currentTimeMillis() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,30000,system,current,time,millis,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,standard,props,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,current,time,millis,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1487173364;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30000 + System.currentTimeMillis()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.currentTimeMillis() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,30000,system,current,time,millis,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,current,time,millis,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1488437582;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1489419493;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1493975167;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1494830990;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1495175928;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1495175928;Ensures that the committed offsets to Kafka are the offsets of "the next record to process";public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1495923077;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1498894422;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1498894422;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1499314317;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t.getCause() instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,get,cause,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1501249950;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1507568316;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1509723634;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1515177485;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1519973085;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1519973085;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1523020981;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1523020981;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1523020981;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1525452496;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1539704473;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1549282380;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runCommitOffsetsToKafka() throws Exception;1550834396;Ensures that the committed offsets to Kafka are the offsets of "the next record to process".;public void runCommitOffsetsToKafka() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testCommitOffsetsToKafkaTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.setParallelism(parallelism)__		env.enableCheckpointing(200)___		DataStream<String> stream = env.addSource(kafkaServer.getConsumer(topicName, new SimpleStringSchema(), standardProps))__		stream.addSink(new DiscardingSink<String>())___		final AtomicReference<Throwable> errorRef = new AtomicReference<>()__		final Thread runner = new Thread("runner") {_			@Override_			public void run() {_				try {_					env.execute()__				}_				catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) {_						errorRef.set(t)__					}_				}_			}_		}__		runner.start()___		final Long l50 = 50L_ _		final long deadline = 30_000_000_000L + System.nanoTime()___		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()___		do {_			Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__			Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__			Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)___			if (l50.equals(o1) && l50.equals(o2) && l50.equals(o3)) {_				break__			}__			Thread.sleep(100)__		}_		while (System.nanoTime() < deadline)___		_		client.cancel(Iterables.getOnlyElement(getRunningJobs(client)))__		runner.join()___		final Throwable t = errorRef.get()__		if (t != null) {_			throw new RuntimeException("Job failed with an exception", t)__		}__		_		Long o1 = kafkaOffsetHandler.getCommittedOffset(topicName, 0)__		Long o2 = kafkaOffsetHandler.getCommittedOffset(topicName, 1)__		Long o3 = kafkaOffsetHandler.getCommittedOffset(topicName, 2)__		Assert.assertEquals(Long.valueOf(50L), o1)__		Assert.assertEquals(Long.valueOf(50L), o2)__		Assert.assertEquals(Long.valueOf(50L), o3)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};ensures,that,the,committed,offsets,to,kafka,are,the,offsets,of,the,next,record,to,process;public,void,run,commit,offsets,to,kafka,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,commit,offsets,to,kafka,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,set,parallelism,parallelism,env,enable,checkpointing,200,data,stream,string,stream,env,add,source,kafka,server,get,consumer,topic,name,new,simple,string,schema,standard,props,stream,add,sink,new,discarding,sink,string,final,atomic,reference,throwable,error,ref,new,atomic,reference,final,thread,runner,new,thread,runner,override,public,void,run,try,env,execute,catch,throwable,t,if,t,instanceof,job,cancellation,exception,error,ref,set,t,runner,start,final,long,l50,50l,final,long,deadline,system,nano,time,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,do,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,if,l50,equals,o1,l50,equals,o2,l50,equals,o3,break,thread,sleep,100,while,system,nano,time,deadline,client,cancel,iterables,get,only,element,get,running,jobs,client,runner,join,final,throwable,t,error,ref,get,if,t,null,throw,new,runtime,exception,job,failed,with,an,exception,t,long,o1,kafka,offset,handler,get,committed,offset,topic,name,0,long,o2,kafka,offset,handler,get,committed,offset,topic,name,1,long,o3,kafka,offset,handler,get,committed,offset,topic,name,2,assert,assert,equals,long,value,of,50l,o1,assert,assert,equals,long,value,of,50l,o2,assert,assert,equals,long,value,of,50l,o3,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1480685315;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1484866642;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1487173364;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1488437582;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1489419493;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1493975167;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1494830990;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1495175928;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> @RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class) 	public void runSimpleConcurrentProducerConsumerTopology() throws Exception;1495175928;Ensure Kafka is working on both producer and consumer side._This executes a job that contains two Flink pipelines.__<pre>_(generator source) --> (kafka sink)-[KAFKA-TOPIC]-(kafka source) --> (validating sink)_</pre>__We need to externally retry this test. We cannot let Flink's retry mechanism do it, because the Kafka producer_does not guarantee exactly-once output. Hence a recovery would introduce duplicates that_cause the test to fail.__This test also ensures that FLINK-3156 doesn't happen again:__The following situation caused a NPE in the FlinkKafkaConsumer__topic-1 <-- elements are only produced into topic1._topic-2__Therefore, this test is consuming as well from an empty topic.;@RetryOnException(times=2, exception=kafka.common.NotLeaderForPartitionException.class)_	public void runSimpleConcurrentProducerConsumerTopology() throws Exception {_		final String topic = "concurrentProducerConsumerTopic_" + UUID.randomUUID().toString()__		final String additionalEmptyTopic = "additionalEmptyTopic_" + UUID.randomUUID().toString()___		final int parallelism = 3__		final int elementsPerPartition = 100__		final int totalElements = parallelism * elementsPerPartition___		createTestTopic(topic, parallelism, 2)__		createTestTopic(additionalEmptyTopic, parallelism, 1)_ __		final StreamExecutionEnvironment env =_				StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(500)__		env.setRestartStrategy(RestartStrategies.noRestart())_ _		env.getConfig().disableSysoutLogging()___		TypeInformation<Tuple2<Long, String>> longStringType = TypeInfoParser.parse("Tuple2<Long, String>")___		TypeInformationSerializationSchema<Tuple2<Long, String>> sourceSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		TypeInformationSerializationSchema<Tuple2<Long, String>> sinkSchema =_				new TypeInformationSerializationSchema<>(longStringType, env.getConfig())___		__		DataStream<Tuple2<Long, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple2<Long,String>>() {__			private boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Long, String>> ctx) throws InterruptedException {_				int cnt = getRuntimeContext().getIndexOfThisSubtask() * elementsPerPartition__				int limit = cnt + elementsPerPartition____				while (running && cnt < limit) {_					ctx.collect(new Tuple2<>(1000L + cnt, "kafka-" + cnt))__					cnt++__					_					_					Thread.sleep(50)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})__		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, topic, new KeyedSerializationSchemaWrapper<>(sinkSchema), producerProperties, null)___		__		List<String> topics = new ArrayList<>()__		topics.add(topic)__		topics.add(additionalEmptyTopic)___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topics, sourceSchema, props)___		DataStreamSource<Tuple2<Long, String>> consuming = env.addSource(source).setParallelism(parallelism)___		consuming.addSink(new RichSinkFunction<Tuple2<Long, String>>() {__			private int elCnt = 0__			private BitSet validator = new BitSet(totalElements)___			@Override_			public void invoke(Tuple2<Long, String> value) throws Exception {_				String[] sp = value.f1.split("-")__				int v = Integer.parseInt(sp[1])___				assertEquals(value.f0 - 1000, (long) v)___				assertFalse("Received tuple twice", validator.get(v))__				validator.set(v)__				elCnt++___				if (elCnt == totalElements) {_					_					int nc__					if ((nc = validator.nextClearBit(0)) != totalElements) {_						fail("The bitset was not set to 1 on all elements. Next clear:"_								+ nc + " Set: " + validator)__					}_					throw new SuccessException()__				}_			}__			@Override_			public void close() throws Exception {_				super.close()__			}_		}).setParallelism(1)___		try {_			tryExecutePropagateExceptions(env, "runSimpleConcurrentProducerConsumerTopology")__		}_		catch (ProgramInvocationException | JobExecutionException e) {_			_			Throwable cause = e.getCause()___			_			int depth = 0__			while (cause != null && depth++ < 20) {_				if (cause instanceof kafka.common.NotLeaderForPartitionException) {_					throw (Exception) cause__				}_				cause = cause.getCause()__			}_			throw e__		}__		deleteTestTopic(topic)__	};ensure,kafka,is,working,on,both,producer,and,consumer,side,this,executes,a,job,that,contains,two,flink,pipelines,pre,generator,source,kafka,sink,kafka,topic,kafka,source,validating,sink,pre,we,need,to,externally,retry,this,test,we,cannot,let,flink,s,retry,mechanism,do,it,because,the,kafka,producer,does,not,guarantee,exactly,once,output,hence,a,recovery,would,introduce,duplicates,that,cause,the,test,to,fail,this,test,also,ensures,that,flink,3156,doesn,t,happen,again,the,following,situation,caused,a,npe,in,the,flink,kafka,consumer,topic,1,elements,are,only,produced,into,topic1,topic,2,therefore,this,test,is,consuming,as,well,from,an,empty,topic;retry,on,exception,times,2,exception,kafka,common,not,leader,for,partition,exception,class,public,void,run,simple,concurrent,producer,consumer,topology,throws,exception,final,string,topic,uuid,random,uuid,to,string,final,string,additional,empty,topic,uuid,random,uuid,to,string,final,int,parallelism,3,final,int,elements,per,partition,100,final,int,total,elements,parallelism,elements,per,partition,create,test,topic,topic,parallelism,2,create,test,topic,additional,empty,topic,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,500,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,tuple2,long,string,long,string,type,type,info,parser,parse,tuple2,long,string,type,information,serialization,schema,tuple2,long,string,source,schema,new,type,information,serialization,schema,long,string,type,env,get,config,type,information,serialization,schema,tuple2,long,string,sink,schema,new,type,information,serialization,schema,long,string,type,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,rich,parallel,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,interrupted,exception,int,cnt,get,runtime,context,get,index,of,this,subtask,elements,per,partition,int,limit,cnt,elements,per,partition,while,running,cnt,limit,ctx,collect,new,tuple2,1000l,cnt,kafka,cnt,cnt,thread,sleep,50,override,public,void,cancel,running,false,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,sink,schema,producer,properties,null,list,string,topics,new,array,list,topics,add,topic,topics,add,additional,empty,topic,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topics,source,schema,props,data,stream,source,tuple2,long,string,consuming,env,add,source,source,set,parallelism,parallelism,consuming,add,sink,new,rich,sink,function,tuple2,long,string,private,int,el,cnt,0,private,bit,set,validator,new,bit,set,total,elements,override,public,void,invoke,tuple2,long,string,value,throws,exception,string,sp,value,f1,split,int,v,integer,parse,int,sp,1,assert,equals,value,f0,1000,long,v,assert,false,received,tuple,twice,validator,get,v,validator,set,v,el,cnt,if,el,cnt,total,elements,int,nc,if,nc,validator,next,clear,bit,0,total,elements,fail,the,bitset,was,not,set,to,1,on,all,elements,next,clear,nc,set,validator,throw,new,success,exception,override,public,void,close,throws,exception,super,close,set,parallelism,1,try,try,execute,propagate,exceptions,env,run,simple,concurrent,producer,consumer,topology,catch,program,invocation,exception,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,while,cause,null,depth,20,if,cause,instanceof,kafka,common,not,leader,for,partition,exception,throw,exception,cause,cause,cause,get,cause,throw,e,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1480685315;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1484866642;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1487173364;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1488437582;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1489419493;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1493975167;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1494830990;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1495175928;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1495175928;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1495923077;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1498894422;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1498894422;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1499314317;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1501249950;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1507568316;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1509723634;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1515177485;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1519973085;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1519973085;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1523020981;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1523020981;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__					env.setParallelism(parallelism)__					env.enableCheckpointing(100)__					env.getConfig().disableSysoutLogging()___					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)__					FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___					env.addSource(source).addSink(new DiscardingSink<String>())___					env.execute("CancelingOnEmptyInputTest")__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))___		_		runnerThread.join()___		failueCause = error.get()__		assertNotNull("program did not fail properly due to canceling", failueCause)__		assertTrue(failueCause.getMessage().contains("Job was cancelled"))___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,runnable,job,runner,new,runnable,override,public,void,run,try,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,env,execute,canceling,on,empty,input,test,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,runner,thread,join,failue,cause,error,get,assert,not,null,program,did,not,fail,properly,due,to,canceling,failue,cause,assert,true,failue,cause,get,message,contains,job,was,cancelled,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1523020981;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1525452496;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1539704473;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1549282380;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,delete,test,topic,topic
KafkaConsumerTestBase -> public void runCancelingOnEmptyInputTest() throws Exception;1550834396;Tests that the source can be properly canceled when reading empty partitions.;public void runCancelingOnEmptyInputTest() throws Exception {_		final String topic = "cancelingOnEmptyInputTopic"___		final int parallelism = 3__		createTestTopic(topic, parallelism, 1)___		final AtomicReference<Throwable> error = new AtomicReference<>()___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(parallelism)__		env.enableCheckpointing(100)__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer(topic, new SimpleStringSchema(), props)___		env.addSource(source).addSink(new DiscardingSink<String>())___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		final Runnable jobRunner = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				}_				catch (Throwable t) {_					LOG.error("Job Runner failed with exception", t)__					error.set(t)__				}_			}_		}___		Thread runnerThread = new Thread(jobRunner, "program runner thread")__		runnerThread.start()___		_		Thread.sleep(2000)___		Throwable failueCause = error.get()__		if (failueCause != null) {_			failueCause.printStackTrace()__			Assert.fail("Test failed prematurely with: " + failueCause.getMessage())__		}_		_		client.cancel(jobId)___		_		runnerThread.join()___		assertEquals(JobStatus.CANCELED, client.getJobStatus(jobId).get())___		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,empty,partitions;public,void,run,canceling,on,empty,input,test,throws,exception,final,string,topic,canceling,on,empty,input,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,final,atomic,reference,throwable,error,new,atomic,reference,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,env,enable,checkpointing,100,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,topic,new,simple,string,schema,props,env,add,source,source,add,sink,new,discarding,sink,string,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,final,runnable,job,runner,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,log,error,job,runner,failed,with,exception,t,error,set,t,thread,runner,thread,new,thread,job,runner,program,runner,thread,runner,thread,start,thread,sleep,2000,throwable,failue,cause,error,get,if,failue,cause,null,failue,cause,print,stack,trace,assert,fail,test,failed,prematurely,with,failue,cause,get,message,client,cancel,job,id,runner,thread,join,assert,equals,job,status,canceled,client,get,job,status,job,id,get,delete,test,topic,topic
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1489419493;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1493975167;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1494830990;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1495175928;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1495175928;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1495923077;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1498894422;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1498894422;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1499314317;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1501249950;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1507568316;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1509723634;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1515177485;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Map, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, specificStartupOffsets, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,map,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,specific,startup,offsets,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1487173364;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Properties cc, 								final int sourceParallelism, 								final String topicName, 								final int valuesCount, 								final int startFrom) throws Exception;1488437582;Variant of {@link KafkaConsumerTestBase#readSequence(StreamExecutionEnvironment, StartupMode, Properties, String, Map)} to_expect reading from the same start offset and the same value count for all partitions of a single Kafka topic.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Properties cc,_								final int sourceParallelism,_								final String topicName,_								final int valuesCount,_								final int startFrom) throws Exception {_		HashMap<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset = new HashMap<>()__		for (int i = 0_ i < sourceParallelism_ i++) {_			partitionsToValuesCountAndStartOffset.put(i, new Tuple2<>(valuesCount, startFrom))__		}_		readSequence(env, startupMode, cc, topicName, partitionsToValuesCountAndStartOffset)__	};variant,of,link,kafka,consumer,test,base,read,sequence,stream,execution,environment,startup,mode,properties,string,map,to,expect,reading,from,the,same,start,offset,and,the,same,value,count,for,all,partitions,of,a,single,kafka,topic;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,properties,cc,final,int,source,parallelism,final,string,topic,name,final,int,values,count,final,int,start,from,throws,exception,hash,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,new,hash,map,for,int,i,0,i,source,parallelism,i,partitions,to,values,count,and,start,offset,put,i,new,tuple2,values,count,start,from,read,sequence,env,startup,mode,cc,topic,name,partitions,to,values,count,and,start,offset
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1489419493;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1493975167;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1494830990;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1495175928;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1495175928;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1495923077;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1498894422;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1498894422;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1499314317;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1501249950;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1507568316;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1509723634;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1515177485;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case SPECIFIC_OFFSETS:_				consumer.setStartFromSpecificOffsets(specificStartupOffsets)__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,specific,offsets,specific,startup,offsets,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1480685315;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1484866642;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1487173364;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1488437582;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1489419493;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1493975167;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1494830990;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1495175928;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1495175928;Test delete behavior and metrics for producer_@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int ELEMENT_COUNT = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < ELEMENT_COUNT_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}_			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == ELEMENT_COUNT) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1495923077;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1498894422;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1498894422;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1499314317;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1501249950;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1507568316;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1509723634;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1515177485;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1519973085;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1519973085;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1523020981;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1523020981;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1523020981;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1525452496;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1539704473;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1549282380;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runAllDeletesTest() throws Exception;1550834396;Test delete behavior and metrics for producer._@throws Exception;public void runAllDeletesTest() throws Exception {_		final String topic = "alldeletestest"__		createTestTopic(topic, 1, 1)__		final int elementCount = 300___		__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		DataStream<Tuple2<byte[], PojoValue>> kvStream = env.addSource(new SourceFunction<Tuple2<byte[], PojoValue>>() {_			@Override_			public void run(SourceContext<Tuple2<byte[], PojoValue>> ctx) throws Exception {_				Random rnd = new Random(1337)__				for (long i = 0_ i < elementCount_ i++) {_					final byte[] key = new byte[200]__					rnd.nextBytes(key)__					ctx.collect(new Tuple2<>(key, (PojoValue) null))__				}_			}__			@Override_			public void cancel() {_			}_		})___		TypeInformationKeyValueSerializationSchema<byte[], PojoValue> schema = new TypeInformationKeyValueSerializationSchema<>(byte[].class, PojoValue.class, env.getConfig())___		Properties producerProperties = FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings)__		producerProperties.setProperty("retries", "3")__		producerProperties.putAll(secureProps)__		kafkaServer.produceIntoKafka(kvStream, topic, schema, producerProperties, null)___		env.execute("Write deletes to Kafka")___		__		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(1)__		env.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		DataStream<Tuple2<byte[], PojoValue>> fromKafka = env.addSource(kafkaServer.getConsumer(topic, schema, props))___		fromKafka.flatMap(new RichFlatMapFunction<Tuple2<byte[], PojoValue>, Object>() {_			long counter = 0__			@Override_			public void flatMap(Tuple2<byte[], PojoValue> value, Collector<Object> out) throws Exception {_				_				assertNull(value.f1)__				counter++__				if (counter == elementCount) {_					_					throw new SuccessException()__				}_			}_		})___		tryExecute(env, "Read deletes from Kafka")___		deleteTestTopic(topic)__	};test,delete,behavior,and,metrics,for,producer,throws,exception;public,void,run,all,deletes,test,throws,exception,final,string,topic,alldeletestest,create,test,topic,topic,1,1,final,int,element,count,300,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,data,stream,tuple2,byte,pojo,value,kv,stream,env,add,source,new,source,function,tuple2,byte,pojo,value,override,public,void,run,source,context,tuple2,byte,pojo,value,ctx,throws,exception,random,rnd,new,random,1337,for,long,i,0,i,element,count,i,final,byte,key,new,byte,200,rnd,next,bytes,key,ctx,collect,new,tuple2,key,pojo,value,null,override,public,void,cancel,type,information,key,value,serialization,schema,byte,pojo,value,schema,new,type,information,key,value,serialization,schema,byte,class,pojo,value,class,env,get,config,properties,producer,properties,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,producer,properties,set,property,retries,3,producer,properties,put,all,secure,props,kafka,server,produce,into,kafka,kv,stream,topic,schema,producer,properties,null,env,execute,write,deletes,to,kafka,env,stream,execution,environment,get,execution,environment,env,set,parallelism,1,env,get,config,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,byte,pojo,value,from,kafka,env,add,source,kafka,server,get,consumer,topic,schema,props,from,kafka,flat,map,new,rich,flat,map,function,tuple2,byte,pojo,value,object,long,counter,0,override,public,void,flat,map,tuple2,byte,pojo,value,value,collector,object,out,throws,exception,assert,null,value,f1,counter,if,counter,element,count,throw,new,success,exception,try,execute,env,read,deletes,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1480685315;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1484866642;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1487173364;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1488437582;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1489419493;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1493975167;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1494830990;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1495175928;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1495175928;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int ELEMENT_COUNT = 300__		final String topic = writeSequence("testEndOfStream", ELEMENT_COUNT, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(ELEMENT_COUNT), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer,Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + ELEMENT_COUNT + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,300,final,string,topic,write,sequence,test,end,of,stream,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1495923077;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		JobExecutionResult result = tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,job,execution,result,result,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1498894422;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1498894422;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1499314317;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1501249950;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1507568316;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1509723634;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1515177485;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1519973085;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1519973085;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1523020981;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1523020981;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1523020981;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1525452496;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1539704473;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1549282380;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> public void runEndOfStreamTest() throws Exception;1550834396;Test that ensures that DeserializationSchema.isEndOfStream() is properly evaluated.__@throws Exception;public void runEndOfStreamTest() throws Exception {__		final int elementCount = 300__		final String topic = writeSequence("testEndOfStream", elementCount, 1, 1)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, new FixedNumberDeserializationSchema(elementCount), props))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_				_			}_		})___		tryExecute(env1, "Consume " + elementCount + " elements from Kafka")___		deleteTestTopic(topic)__	};test,that,ensures,that,deserialization,schema,is,end,of,stream,is,properly,evaluated,throws,exception;public,void,run,end,of,stream,test,throws,exception,final,int,element,count,300,final,string,topic,write,sequence,test,end,of,stream,element,count,1,1,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,new,fixed,number,deserialization,schema,element,count,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,try,execute,env1,consume,element,count,elements,from,kafka,delete,test,topic,topic
KafkaConsumerTestBase -> protected void readSequence(StreamExecutionEnvironment env, Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1480685315;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(StreamExecutionEnvironment env, Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,stream,execution,environment,env,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(StreamExecutionEnvironment env, Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1484866642;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(StreamExecutionEnvironment env, Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,stream,execution,environment,env,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1487173364;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1488437582;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1489419493;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1493975167;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1494830990;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1495175928;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1495175928;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1495923077;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1498894422;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1498894422;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1499314317;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1501249950;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1507568316;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1509723634;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1515177485;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1519973085;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1519973085;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1523020981;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1523020981;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1523020981;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1525452496;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1539704473;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1549282380;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runStartFromEarliestOffsets() throws Exception;1550834396;This test ensures that when explicitly set to start from earliest record, the consumer_ignores the "auto.offset.reset" behaviour as well as any committed group offsets in Kafka.;public void runStartFromEarliestOffsets() throws Exception {_		_		final int parallelism = 3__		final int recordsInEachPartition = 50___		final String topicName = writeSequence("testStartFromEarliestOffsetsTopic", recordsInEachPartition, parallelism, 1)___		final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		env.setParallelism(parallelism)___		Properties readProps = new Properties()__		readProps.putAll(standardProps)__		readProps.setProperty("auto.offset.reset", "latest")_ __		_		KafkaTestEnvironment.KafkaOffsetHandler kafkaOffsetHandler = kafkaServer.createOffsetHandler()__		kafkaOffsetHandler.setCommittedOffset(topicName, 0, 23)__		kafkaOffsetHandler.setCommittedOffset(topicName, 1, 31)__		kafkaOffsetHandler.setCommittedOffset(topicName, 2, 43)___		readSequence(env, StartupMode.EARLIEST, null, null, readProps, parallelism, topicName, recordsInEachPartition, 0)___		kafkaOffsetHandler.close()__		deleteTestTopic(topicName)__	};this,test,ensures,that,when,explicitly,set,to,start,from,earliest,record,the,consumer,ignores,the,auto,offset,reset,behaviour,as,well,as,any,committed,group,offsets,in,kafka;public,void,run,start,from,earliest,offsets,throws,exception,final,int,parallelism,3,final,int,records,in,each,partition,50,final,string,topic,name,write,sequence,test,start,from,earliest,offsets,topic,records,in,each,partition,parallelism,1,final,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,env,set,parallelism,parallelism,properties,read,props,new,properties,read,props,put,all,standard,props,read,props,set,property,auto,offset,reset,latest,kafka,test,environment,kafka,offset,handler,kafka,offset,handler,kafka,server,create,offset,handler,kafka,offset,handler,set,committed,offset,topic,name,0,23,kafka,offset,handler,set,committed,offset,topic,name,1,31,kafka,offset,handler,set,committed,offset,topic,name,2,43,read,sequence,env,startup,mode,earliest,null,null,read,props,parallelism,topic,name,records,in,each,partition,0,kafka,offset,handler,close,delete,test,topic,topic,name
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1480685315;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(ProgramInvocationException pie) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,create,remote,environment,localhost,flink,port,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,program,invocation,exception,pie,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1484866642;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(ProgramInvocationException pie) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,create,remote,environment,localhost,flink,port,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,program,invocation,exception,pie,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1487173364;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(ProgramInvocationException pie) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,create,remote,environment,localhost,flink,port,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,program,invocation,exception,pie,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1488437582;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(ProgramInvocationException pie) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,create,remote,environment,localhost,flink,port,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,program,invocation,exception,pie,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1489419493;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(ProgramInvocationException pie) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(pie.getCause() instanceof JobExecutionException)___				JobExecutionException jee = (JobExecutionException) pie.getCause()___				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,create,remote,environment,localhost,flink,port,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,program,invocation,exception,pie,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,pie,get,cause,instanceof,job,execution,exception,job,execution,exception,jee,job,execution,exception,pie,get,cause,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1493975167;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(JobExecutionException jee) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1494830990;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(JobExecutionException jee) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1495175928;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(JobExecutionException jee) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1495175928;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch(JobExecutionException jee) {_			if(kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1495923077;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions for the requested topics [doesntexist]"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions,for,the,requested,topics,doesntexist
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1498894422;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1498894422;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1499314317;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1501249950;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1507568316;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1509723634;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1515177485;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1519973085;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1519973085;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1523020981;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1523020981;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1523020981;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1525452496;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") || kafkaServer.getVersion().equals("0.10") || kafkaServer.getVersion().equals("0.11")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1539704473;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") ||_				kafkaServer.getVersion().equals("0.10") ||_				kafkaServer.getVersion().equals("0.11") ||_				kafkaServer.getVersion().equals("2.0")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,kafka,server,get,version,equals,2,0,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1549282380;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") ||_				kafkaServer.getVersion().equals("0.10") ||_				kafkaServer.getVersion().equals("0.11") ||_				kafkaServer.getVersion().equals("2.0")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,kafka,server,get,version,equals,2,0,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runFailOnNoBrokerTest() throws Exception;1550834396;Test that ensures the KafkaConsumer is properly failing if the topic doesnt exist_and a wrong broker was specified.__@throws Exception;public void runFailOnNoBrokerTest() throws Exception {_		try {_			Properties properties = new Properties()___			StreamExecutionEnvironment see = StreamExecutionEnvironment.getExecutionEnvironment()__			see.getConfig().disableSysoutLogging()__			see.setRestartStrategy(RestartStrategies.noRestart())__			see.setParallelism(1)___			_			properties.setProperty("bootstrap.servers", "localhost:80")__			properties.setProperty("zookeeper.connect", "localhost:80")__			properties.setProperty("group.id", "test")__			properties.setProperty("request.timeout.ms", "3000")_ _			properties.setProperty("socket.timeout.ms", "3000")__			properties.setProperty("session.timeout.ms", "2000")__			properties.setProperty("fetch.max.wait.ms", "2000")__			properties.setProperty("heartbeat.interval.ms", "1000")__			properties.putAll(secureProps)__			FlinkKafkaConsumerBase<String> source = kafkaServer.getConsumer("doesntexist", new SimpleStringSchema(), properties)__			DataStream<String> stream = see.addSource(source)__			stream.print()__			see.execute("No broker test")__		} catch (JobExecutionException jee) {_			if (kafkaServer.getVersion().equals("0.9") ||_				kafkaServer.getVersion().equals("0.10") ||_				kafkaServer.getVersion().equals("0.11") ||_				kafkaServer.getVersion().equals("2.0")) {_				assertTrue(jee.getCause() instanceof TimeoutException)___				TimeoutException te = (TimeoutException) jee.getCause()___				assertEquals("Timeout expired while fetching topic metadata", te.getMessage())__			} else {_				assertTrue(jee.getCause() instanceof RuntimeException)___				RuntimeException re = (RuntimeException) jee.getCause()___				assertTrue(re.getMessage().contains("Unable to retrieve any partitions"))__			}_		}_	};test,that,ensures,the,kafka,consumer,is,properly,failing,if,the,topic,doesnt,exist,and,a,wrong,broker,was,specified,throws,exception;public,void,run,fail,on,no,broker,test,throws,exception,try,properties,properties,new,properties,stream,execution,environment,see,stream,execution,environment,get,execution,environment,see,get,config,disable,sysout,logging,see,set,restart,strategy,restart,strategies,no,restart,see,set,parallelism,1,properties,set,property,bootstrap,servers,localhost,80,properties,set,property,zookeeper,connect,localhost,80,properties,set,property,group,id,test,properties,set,property,request,timeout,ms,3000,properties,set,property,socket,timeout,ms,3000,properties,set,property,session,timeout,ms,2000,properties,set,property,fetch,max,wait,ms,2000,properties,set,property,heartbeat,interval,ms,1000,properties,put,all,secure,props,flink,kafka,consumer,base,string,source,kafka,server,get,consumer,doesntexist,new,simple,string,schema,properties,data,stream,string,stream,see,add,source,source,stream,print,see,execute,no,broker,test,catch,job,execution,exception,jee,if,kafka,server,get,version,equals,0,9,kafka,server,get,version,equals,0,10,kafka,server,get,version,equals,0,11,kafka,server,get,version,equals,2,0,assert,true,jee,get,cause,instanceof,timeout,exception,timeout,exception,te,timeout,exception,jee,get,cause,assert,equals,timeout,expired,while,fetching,topic,metadata,te,get,message,else,assert,true,jee,get,cause,instanceof,runtime,exception,runtime,exception,re,runtime,exception,jee,get,cause,assert,true,re,get,message,contains,unable,to,retrieve,any,partitions
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1480685315;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1484866642;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1487173364;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1488437582;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1489419493;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1493975167;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1494830990;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1495175928;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1495175928;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1495923077;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1498894422;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1498894422;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, numPartitions, numElementsPerPartition, false)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,false,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1499314317;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1501249950;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1507568316;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1509723634;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1515177485;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1519973085;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1519973085;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1525452496;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1539704473;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1549282380;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception;1550834396;Tests the proper consumption when having fewer Flink sources than Kafka partitions, so_one Flink source will read multiple Kafka partitions.;public void runOneSourceMultiplePartitionsExactlyOnceTest() throws Exception {_		final String topic = "oneToManyTopic"__		final int numPartitions = 5__		final int numElementsPerPartition = 1000__		final int totalElements = numPartitions * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		final int parallelism = 2___		createTestTopic(topic, numPartitions, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				numPartitions,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(numPartitions, 3))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-source-multi-partitions exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,fewer,flink,sources,than,kafka,partitions,so,one,flink,source,will,read,multiple,kafka,partitions;public,void,run,one,source,multiple,partitions,exactly,once,test,throws,exception,final,string,topic,one,to,many,topic,final,int,num,partitions,5,final,int,num,elements,per,partition,1000,final,int,total,elements,num,partitions,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,final,int,parallelism,2,create,test,topic,topic,num,partitions,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,num,partitions,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,num,partitions,3,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,source,multi,partitions,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1480685315;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}_		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1484866642;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}_		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1487173364;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}_		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1488437582;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}_		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1489419493;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}_		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1493975167;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws java.lang.Exception;1494830990;Test producing and consuming into multiple topics_@throws java.lang.Exception;public void runProduceConsumeMultipleTopics() throws java.lang.Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,java,lang,exception;public,void,run,produce,consume,multiple,topics,throws,java,lang,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1480685315;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t.getCause() instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,get,cause,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1484866642;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t.getCause() instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,get,cause,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1487173364;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t.getCause() instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,get,cause,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1488437582;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t.getCause() instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,get,cause,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1489419493;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t.getCause() instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,create,remote,environment,localhost,flink,port,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,get,cause,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1493975167;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1494830990;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1495175928;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1495175928;Test metrics reporting for consumer__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch(Throwable t) {_					LOG.warn("Got exception during execution", t)__					if(!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)____			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1495923077;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					LOG.warn("Got exception during execution", t)__					if (!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1498894422;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					LOG.warn("Got exception during execution", t)__					if (!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1498894422;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					LOG.warn("Got exception during execution", t)__					if (!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1499314317;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					LOG.warn("Got exception during execution", t)__					if (!(t instanceof JobCancellationException)) { _						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__		}__		while (jobThread.isAlive()) {_			Thread.sleep(50)__		}_		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,log,warn,got,exception,during,execution,t,if,t,instanceof,job,cancellation,exception,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,while,job,thread,is,alive,thread,sleep,50,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1501249950;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1507568316;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1509723634;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1515177485;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1519973085;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1519973085;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1523020981;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1523020981;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)__		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					_					final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__					env1.setParallelism(1)__					env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__					env1.getConfig().disableSysoutLogging()__					env1.disableOperatorChaining()_ __					Properties props = new Properties()__					props.putAll(standardProps)__					props.putAll(secureProps)___					TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__					DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__					fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_						@Override_						public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_						}_					})___					DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_						boolean running = true___						@Override_						public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_							int i = 0__							while (running) {_								ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__								Thread.sleep(1)__							}_						}__						@Override_						public void cancel() {_							running = false__						}_					})___					kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___					env1.execute("Metrics test job")__				} catch (Throwable t) {_					if (!(t instanceof JobCancellationException)) { _						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			JobManagerCommunicationUtils.cancelCurrentJob(flink.getLeaderGateway(timeout))__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,runnable,job,new,runnable,override,public,void,run,try,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,env1,execute,metrics,test,job,catch,throwable,t,if,t,instanceof,job,cancellation,exception,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,job,manager,communication,utils,cancel,current,job,flink,get,leader,gateway,timeout,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1523020981;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()__		env1.disableOperatorChaining()_ __		TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(TypeInfoParser.<Tuple2<Integer, Integer>>parse("Tuple2<Integer, Integer>"), env1.getConfig())__		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_			}_		})___		DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_			boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_				int i = 0__				while (running) {_					ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__					Thread.sleep(1)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env1.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			client.cancel(jobId)__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,info,parser,tuple2,integer,integer,parse,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env1,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,runnable,job,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,client,cancel,job,id,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1525452496;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()__		env1.disableOperatorChaining()_ __		TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){}), env1.getConfig())___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_			}_		})___		DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_			boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_				int i = 0__				while (running) {_					ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__					Thread.sleep(1)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env1.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			client.cancel(jobId)__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,information,of,new,type,hint,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env1,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,runnable,job,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,client,cancel,job,id,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1539704473;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()__		env1.disableOperatorChaining()_ __		TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){}), env1.getConfig())___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_			}_		})___		DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_			boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_				int i = 0__				while (running) {_					ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__					Thread.sleep(1)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env1.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			client.cancel(jobId)__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,information,of,new,type,hint,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env1,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,runnable,job,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,client,cancel,job,id,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1549282380;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()__		env1.disableOperatorChaining()_ __		TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){}), env1.getConfig())___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_			}_		})___		DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_			boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_				int i = 0__				while (running) {_					ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__					Thread.sleep(1)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env1.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			client.cancel(jobId)__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,information,of,new,type,hint,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env1,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,runnable,job,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,client,cancel,job,id,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runMetricsTest() throws Throwable;1550834396;Test metrics reporting for consumer.__@throws Exception;public void runMetricsTest() throws Throwable {__		_		final String topic = "metricsStream"__		createTestTopic(topic, 5, 1)___		final Tuple1<Throwable> error = new Tuple1<>(null)___		_		final StreamExecutionEnvironment env1 = StreamExecutionEnvironment.getExecutionEnvironment()__		env1.setParallelism(1)__		env1.getConfig().setRestartStrategy(RestartStrategies.noRestart())__		env1.getConfig().disableSysoutLogging()__		env1.disableOperatorChaining()_ __		TypeInformationSerializationSchema<Tuple2<Integer, Integer>> schema = new TypeInformationSerializationSchema<>(_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){}), env1.getConfig())___		DataStream<Tuple2<Integer, Integer>> fromKafka = env1.addSource(kafkaServer.getConsumer(topic, schema, standardProps))__		fromKafka.flatMap(new FlatMapFunction<Tuple2<Integer, Integer>, Void>() {_			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Void> out) throws Exception {_			}_		})___		DataStream<Tuple2<Integer, Integer>> fromGen = env1.addSource(new RichSourceFunction<Tuple2<Integer, Integer>>() {_			boolean running = true___			@Override_			public void run(SourceContext<Tuple2<Integer, Integer>> ctx) throws Exception {_				int i = 0__				while (running) {_					ctx.collect(Tuple2.of(i++, getRuntimeContext().getIndexOfThisSubtask()))__					Thread.sleep(1)__				}_			}__			@Override_			public void cancel() {_				running = false__			}_		})___		kafkaServer.produceIntoKafka(fromGen, topic, new KeyedSerializationSchemaWrapper<>(schema), standardProps, null)___		JobGraph jobGraph = StreamingJobGraphGenerator.createJobGraph(env1.getStreamGraph())__		final JobID jobId = jobGraph.getJobID()___		Runnable job = new Runnable() {_			@Override_			public void run() {_				try {_					client.setDetached(false)__					client.submitJob(jobGraph, KafkaConsumerTestBase.class.getClassLoader())__				} catch (Throwable t) {_					if (!ExceptionUtils.findThrowable(t, JobCancellationException.class).isPresent()) {_						LOG.warn("Got exception during execution", t)__						error.f0 = t__					}_				}_			}_		}__		Thread jobThread = new Thread(job)__		jobThread.start()___		try {_			_			MBeanServer mBeanServer = ManagementFactory.getPlatformMBeanServer()__			_			Set<ObjectName> offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__			while (offsetMetrics.size() < 5) { _				if (error.f0 != null) {_					_					throw error.f0__				}_				offsetMetrics = mBeanServer.queryNames(new ObjectName("*current-offsets*:*"), null)__				Thread.sleep(50)__			}_			Assert.assertEquals(5, offsetMetrics.size())__			_			_			_			while (true) {_				int numPosOffsets = 0__				_				for (ObjectName object : offsetMetrics) {_					Object offset = mBeanServer.getAttribute(object, "Value")__					if ((long) offset >= 0) {_						numPosOffsets++__					}_				}_				if (numPosOffsets == 5) {_					break__				}_				_				Thread.sleep(50)__			}__			_			Set<ObjectName> producerMetrics = mBeanServer.queryNames(new ObjectName("*KafkaProducer*:*"), null)__			Assert.assertTrue("No producer metrics found", producerMetrics.size() > 30)___			LOG.info("Found all JMX metrics. Cancelling job.")__		} finally {_			_			client.cancel(jobId)__			_			jobThread.join()__		}__		if (error.f0 != null) {_			throw error.f0__		}__		deleteTestTopic(topic)__	};test,metrics,reporting,for,consumer,throws,exception;public,void,run,metrics,test,throws,throwable,final,string,topic,metrics,stream,create,test,topic,topic,5,1,final,tuple1,throwable,error,new,tuple1,null,final,stream,execution,environment,env1,stream,execution,environment,get,execution,environment,env1,set,parallelism,1,env1,get,config,set,restart,strategy,restart,strategies,no,restart,env1,get,config,disable,sysout,logging,env1,disable,operator,chaining,type,information,serialization,schema,tuple2,integer,integer,schema,new,type,information,serialization,schema,type,information,of,new,type,hint,tuple2,integer,integer,env1,get,config,data,stream,tuple2,integer,integer,from,kafka,env1,add,source,kafka,server,get,consumer,topic,schema,standard,props,from,kafka,flat,map,new,flat,map,function,tuple2,integer,integer,void,override,public,void,flat,map,tuple2,integer,integer,value,collector,void,out,throws,exception,data,stream,tuple2,integer,integer,from,gen,env1,add,source,new,rich,source,function,tuple2,integer,integer,boolean,running,true,override,public,void,run,source,context,tuple2,integer,integer,ctx,throws,exception,int,i,0,while,running,ctx,collect,tuple2,of,i,get,runtime,context,get,index,of,this,subtask,thread,sleep,1,override,public,void,cancel,running,false,kafka,server,produce,into,kafka,from,gen,topic,new,keyed,serialization,schema,wrapper,schema,standard,props,null,job,graph,job,graph,streaming,job,graph,generator,create,job,graph,env1,get,stream,graph,final,job,id,job,id,job,graph,get,job,id,runnable,job,new,runnable,override,public,void,run,try,client,set,detached,false,client,submit,job,job,graph,kafka,consumer,test,base,class,get,class,loader,catch,throwable,t,if,exception,utils,find,throwable,t,job,cancellation,exception,class,is,present,log,warn,got,exception,during,execution,t,error,f0,t,thread,job,thread,new,thread,job,job,thread,start,try,mbean,server,m,bean,server,management,factory,get,platform,mbean,server,set,object,name,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,while,offset,metrics,size,5,if,error,f0,null,throw,error,f0,offset,metrics,m,bean,server,query,names,new,object,name,current,offsets,null,thread,sleep,50,assert,assert,equals,5,offset,metrics,size,while,true,int,num,pos,offsets,0,for,object,name,object,offset,metrics,object,offset,m,bean,server,get,attribute,object,value,if,long,offset,0,num,pos,offsets,if,num,pos,offsets,5,break,thread,sleep,50,set,object,name,producer,metrics,m,bean,server,query,names,new,object,name,kafka,producer,null,assert,assert,true,no,producer,metrics,found,producer,metrics,size,30,log,info,found,all,jmx,metrics,cancelling,job,finally,client,cancel,job,id,job,thread,join,if,error,f0,null,throw,error,f0,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1495175928;Test producing and consuming into multiple topics_@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1495175928;Test producing and consuming into multiple topics_@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int NUM_TOPICS = 5__		final int NUM_ELEMENTS = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()__		_		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < NUM_TOPICS_ topicId++) {_					for (int i = 0_ i < NUM_ELEMENTS_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(NUM_TOPICS)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < NUM_ELEMENTS) {_						break_ _					}_					if (el.getValue() > NUM_ELEMENTS) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")____		_		for (int i = 0_ i < NUM_TOPICS_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,5,final,int,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,topic,id,for,int,i,0,i,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,break,if,el,get,value,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1495923077;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1498894422;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1498894422;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1499314317;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1501249950;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1507568316;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1509723634;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1515177485;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1519973085;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1519973085;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1523020981;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1523020981;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1523020981;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1525452496;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1539704473;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1549282380;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runProduceConsumeMultipleTopics() throws Exception;1550834396;Test producing and consuming into multiple topics._@throws Exception;public void runProduceConsumeMultipleTopics() throws Exception {_		final int numTopics = 5__		final int numElements = 20___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		_		final List<String> topics = new ArrayList<>()__		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			topics.add(topic)__			_			createTestTopic(topic, i + 1 , 1)__		}__		_		env.setParallelism(1)___		_		DataStream<Tuple3<Integer, Integer, String>> stream = env.addSource(new RichParallelSourceFunction<Tuple3<Integer, Integer, String>>() {__			@Override_			public void run(SourceContext<Tuple3<Integer, Integer, String>> ctx) throws Exception {_				int partition = getRuntimeContext().getIndexOfThisSubtask()___				for (int topicId = 0_ topicId < numTopics_ topicId++) {_					for (int i = 0_ i < numElements_ i++) {_						ctx.collect(new Tuple3<>(partition, i, "topic-" + topicId))__					}_				}_			}__			@Override_			public void cancel() {_			}_		})___		Tuple2WithTopicSchema schema = new Tuple2WithTopicSchema(env.getConfig())___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		kafkaServer.produceIntoKafka(stream, "dummy", schema, props, null)___		env.execute("Write to topics")___		_		env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.getConfig().disableSysoutLogging()___		stream = env.addSource(kafkaServer.getConsumer(topics, schema, props))___		stream.flatMap(new FlatMapFunction<Tuple3<Integer, Integer, String>, Integer>() {_			Map<String, Integer> countPerTopic = new HashMap<>(numTopics)__			@Override_			public void flatMap(Tuple3<Integer, Integer, String> value, Collector<Integer> out) throws Exception {_				Integer count = countPerTopic.get(value.f2)__				if (count == null) {_					count = 1__				} else {_					count++__				}_				countPerTopic.put(value.f2, count)___				_				for (Map.Entry<String, Integer> el: countPerTopic.entrySet()) {_					if (el.getValue() < numElements) {_						break_ _					}_					if (el.getValue() > numElements) {_						throw new RuntimeException("There is a failure in the test. I've read " +_								el.getValue() + " from topic " + el.getKey())__					}_				}_				_				throw new SuccessException()__			}_		}).setParallelism(1)___		tryExecute(env, "Count elements from the topics")___		_		for (int i = 0_ i < numTopics_ i++) {_			final String topic = "topic-" + i__			deleteTestTopic(topic)__		}_	};test,producing,and,consuming,into,multiple,topics,throws,exception;public,void,run,produce,consume,multiple,topics,throws,exception,final,int,num,topics,5,final,int,num,elements,20,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,final,list,string,topics,new,array,list,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,topics,add,topic,create,test,topic,topic,i,1,1,env,set,parallelism,1,data,stream,tuple3,integer,integer,string,stream,env,add,source,new,rich,parallel,source,function,tuple3,integer,integer,string,override,public,void,run,source,context,tuple3,integer,integer,string,ctx,throws,exception,int,partition,get,runtime,context,get,index,of,this,subtask,for,int,topic,id,0,topic,id,num,topics,topic,id,for,int,i,0,i,num,elements,i,ctx,collect,new,tuple3,partition,i,topic,topic,id,override,public,void,cancel,tuple2with,topic,schema,schema,new,tuple2with,topic,schema,env,get,config,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,dummy,schema,props,null,env,execute,write,to,topics,env,stream,execution,environment,get,execution,environment,env,get,config,disable,sysout,logging,stream,env,add,source,kafka,server,get,consumer,topics,schema,props,stream,flat,map,new,flat,map,function,tuple3,integer,integer,string,integer,map,string,integer,count,per,topic,new,hash,map,num,topics,override,public,void,flat,map,tuple3,integer,integer,string,value,collector,integer,out,throws,exception,integer,count,count,per,topic,get,value,f2,if,count,null,count,1,else,count,count,per,topic,put,value,f2,count,for,map,entry,string,integer,el,count,per,topic,entry,set,if,el,get,value,num,elements,break,if,el,get,value,num,elements,throw,new,runtime,exception,there,is,a,failure,in,the,test,i,ve,read,el,get,value,from,topic,el,get,key,throw,new,success,exception,set,parallelism,1,try,execute,env,count,elements,from,the,topics,for,int,i,0,i,num,topics,i,final,string,topic,topic,i,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1480685315;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1484866642;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1487173364;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1488437582;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1489419493;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,create,remote,environment,localhost,flink,port,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1493975167;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1494830990;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1495175928;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1495175928;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1495923077;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1498894422;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1498894422;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic, parallelism, numElementsPerPartition, true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1499314317;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1501249950;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1507568316;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1509723634;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1515177485;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1519973085;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1519973085;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1523020981;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1525452496;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1539704473;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1549282380;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> public void runOneToOneExactlyOnceTest() throws Exception;1550834396;Tests the proper consumption when having a 1:1 correspondence between kafka partitions and_Flink sources.;public void runOneToOneExactlyOnceTest() throws Exception {__		final String topic = "oneToOneTopic"__		final int parallelism = 5__		final int numElementsPerPartition = 1000__		final int totalElements = parallelism * numElementsPerPartition__		final int failAfterElements = numElementsPerPartition / 3___		createTestTopic(topic, parallelism, 1)___		DataGenerators.generateRandomizedIntegerSequence(_				StreamExecutionEnvironment.getExecutionEnvironment(),_				kafkaServer,_				topic,_				parallelism,_				numElementsPerPartition,_				true)___		__		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(parallelism)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)___		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.map(new PartitionValidatingMapper(parallelism, 1))_				.map(new FailingIdentityMapper<Integer>(failAfterElements))_				.addSink(new ValidatingExactlyOnceSink(totalElements)).setParallelism(1)___		FailingIdentityMapper.failedBefore = false__		tryExecute(env, "One-to-one exactly once test")___		deleteTestTopic(topic)__	};tests,the,proper,consumption,when,having,a,1,1,correspondence,between,kafka,partitions,and,flink,sources;public,void,run,one,to,one,exactly,once,test,throws,exception,final,string,topic,one,to,one,topic,final,int,parallelism,5,final,int,num,elements,per,partition,1000,final,int,total,elements,parallelism,num,elements,per,partition,final,int,fail,after,elements,num,elements,per,partition,3,create,test,topic,topic,parallelism,1,data,generators,generate,randomized,integer,sequence,stream,execution,environment,get,execution,environment,kafka,server,topic,parallelism,num,elements,per,partition,true,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,parallelism,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,map,new,partition,validating,mapper,parallelism,1,map,new,failing,identity,mapper,integer,fail,after,elements,add,sink,new,validating,exactly,once,sink,total,elements,set,parallelism,1,failing,identity,mapper,failed,before,false,try,execute,env,one,to,one,exactly,once,test,delete,test,topic,topic
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1519973085;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1523020981;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1523020981;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1523020981;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1525452496;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType =_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){})___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,information,of,new,type,hint,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1539704473;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType =_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){})___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,information,of,new,type,hint,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1549282380;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType =_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){})___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,information,of,new,type,hint,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Map<KafkaTopicPartition, Long> specificStartupOffsets, 								final Long startupTimestamp, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1550834396;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Map<KafkaTopicPartition, Long> specificStartupOffsets,_								final Long startupTimestamp,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType =_				TypeInformation.of(new TypeHint<Tuple2<Integer, Integer>>(){})___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		setKafkaConsumerOffset(startupMode, consumer, specificStartupOffsets, startupTimestamp)___		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,map,kafka,topic,partition,long,specific,startup,offsets,final,long,startup,timestamp,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,information,of,new,type,hint,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,set,kafka,consumer,offset,startup,mode,consumer,specific,startup,offsets,startup,timestamp,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1480685315;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (ProgramInvocationException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,program,invocation,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1484866642;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (ProgramInvocationException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,program,invocation,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1487173364;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (ProgramInvocationException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,program,invocation,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1488437582;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (ProgramInvocationException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,program,invocation,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1489419493;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (ProgramInvocationException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,program,invocation,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1493975167;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1494830990;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1495175928;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1495175928;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1495923077;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1498894422;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1498894422;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1499314317;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1501249950;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1507568316;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1509723634;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1515177485;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1519973085;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> public void runFailOnDeployTest() throws Exception;1519973085;Tests that the source can be properly canceled when reading full partitions.;public void runFailOnDeployTest() throws Exception {_		final String topic = "failOnDeployTopic"___		createTestTopic(topic, 2, 1)___		DeserializationSchema<Integer> schema =_				new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(12)_ _		env.getConfig().disableSysoutLogging()___		Properties props = new Properties()__		props.putAll(standardProps)__		props.putAll(secureProps)__		FlinkKafkaConsumerBase<Integer> kafkaSource = kafkaServer.getConsumer(topic, schema, props)___		env_				.addSource(kafkaSource)_				.addSink(new DiscardingSink<Integer>())___		try {_			env.execute("test fail on deploy")__			fail("this test should fail with an exception")__		}_		catch (JobExecutionException e) {__			_			Throwable cause = e.getCause()__			int depth = 0__			boolean foundResourceException = false___			while (cause != null && depth++ < 20) {_				if (cause instanceof NoResourceAvailableException) {_					foundResourceException = true__					break__				}_				cause = cause.getCause()__			}__			assertTrue("Wrong exception", foundResourceException)__		}__		deleteTestTopic(topic)__	};tests,that,the,source,can,be,properly,canceled,when,reading,full,partitions;public,void,run,fail,on,deploy,test,throws,exception,final,string,topic,fail,on,deploy,topic,create,test,topic,topic,2,1,deserialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,12,env,get,config,disable,sysout,logging,properties,props,new,properties,props,put,all,standard,props,props,put,all,secure,props,flink,kafka,consumer,base,integer,kafka,source,kafka,server,get,consumer,topic,schema,props,env,add,source,kafka,source,add,sink,new,discarding,sink,integer,try,env,execute,test,fail,on,deploy,fail,this,test,should,fail,with,an,exception,catch,job,execution,exception,e,throwable,cause,e,get,cause,int,depth,0,boolean,found,resource,exception,false,while,cause,null,depth,20,if,cause,instanceof,no,resource,available,exception,found,resource,exception,true,break,cause,cause,get,cause,assert,true,wrong,exception,found,resource,exception,delete,test,topic,topic
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1487173364;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
KafkaConsumerTestBase -> protected void readSequence(final StreamExecutionEnvironment env, 								final StartupMode startupMode, 								final Properties cc, 								final String topicName, 								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception;1488437582;Runs a job using the provided environment to read a sequence of records from a single Kafka topic._The method allows to individually specify the expected starting offset and total read value count of each partition._The job will be considered successful only if all partition read results match the start offset and value count criteria.;protected void readSequence(final StreamExecutionEnvironment env,_								final StartupMode startupMode,_								final Properties cc,_								final String topicName,_								final Map<Integer, Tuple2<Integer, Integer>> partitionsToValuesCountAndStartOffset) throws Exception {_		final int sourceParallelism = partitionsToValuesCountAndStartOffset.keySet().size()___		int finalCountTmp = 0__		for (Map.Entry<Integer, Tuple2<Integer, Integer>> valuesCountAndStartOffset : partitionsToValuesCountAndStartOffset.entrySet()) {_			finalCountTmp += valuesCountAndStartOffset.getValue().f0__		}_		final int finalCount = finalCountTmp___		final TypeInformation<Tuple2<Integer, Integer>> intIntTupleType = TypeInfoParser.parse("Tuple2<Integer, Integer>")___		final TypeInformationSerializationSchema<Tuple2<Integer, Integer>> deser =_			new TypeInformationSerializationSchema<>(intIntTupleType, env.getConfig())___		_		cc.putAll(secureProps)__		FlinkKafkaConsumerBase<Tuple2<Integer, Integer>> consumer = kafkaServer.getConsumer(topicName, deser, cc)__		switch (startupMode) {_			case EARLIEST:_				consumer.setStartFromEarliest()__				break__			case LATEST:_				consumer.setStartFromLatest()__				break__			case GROUP_OFFSETS:_				consumer.setStartFromGroupOffsets()__				break__		}__		DataStream<Tuple2<Integer, Integer>> source = env_			.addSource(consumer).setParallelism(sourceParallelism)_			.map(new ThrottledMapper<Tuple2<Integer, Integer>>(20)).setParallelism(sourceParallelism)___		_		source.flatMap(new RichFlatMapFunction<Tuple2<Integer, Integer>, Integer>() {__			private HashMap<Integer, BitSet> partitionsToValueCheck__			private int count = 0___			@Override_			public void open(Configuration parameters) throws Exception {_				partitionsToValueCheck = new HashMap<>()__				for (Integer partition : partitionsToValuesCountAndStartOffset.keySet()) {_					partitionsToValueCheck.put(partition, new BitSet())__				}_			}__			@Override_			public void flatMap(Tuple2<Integer, Integer> value, Collector<Integer> out) throws Exception {_				int partition = value.f0__				int val = value.f1___				BitSet bitSet = partitionsToValueCheck.get(partition)__				if (bitSet == null) {_					throw new RuntimeException("Got a record from an unknown partition")__				} else {_					bitSet.set(val - partitionsToValuesCountAndStartOffset.get(partition).f1)__				}__				count++___				LOG.info("Received message {}, total {} messages", value, count)___				_				if (count == finalCount) {_					for (Map.Entry<Integer, BitSet> partitionsToValueCheck : this.partitionsToValueCheck.entrySet()) {_						BitSet check = partitionsToValueCheck.getValue()__						int expectedValueCount = partitionsToValuesCountAndStartOffset.get(partitionsToValueCheck.getKey()).f0___						if (check.cardinality() != expectedValueCount) {_							throw new RuntimeException("Expected cardinality to be " + expectedValueCount +_								", but was " + check.cardinality())__						} else if (check.nextClearBit(0) != expectedValueCount) {_							throw new RuntimeException("Expected next clear bit to be " + expectedValueCount +_								", but was " + check.cardinality())__						}_					}__					_					throw new SuccessException()__				}_			}__		}).setParallelism(1)___		tryExecute(env, "Read data from Kafka")___		LOG.info("Successfully read sequence for verification")__	};runs,a,job,using,the,provided,environment,to,read,a,sequence,of,records,from,a,single,kafka,topic,the,method,allows,to,individually,specify,the,expected,starting,offset,and,total,read,value,count,of,each,partition,the,job,will,be,considered,successful,only,if,all,partition,read,results,match,the,start,offset,and,value,count,criteria;protected,void,read,sequence,final,stream,execution,environment,env,final,startup,mode,startup,mode,final,properties,cc,final,string,topic,name,final,map,integer,tuple2,integer,integer,partitions,to,values,count,and,start,offset,throws,exception,final,int,source,parallelism,partitions,to,values,count,and,start,offset,key,set,size,int,final,count,tmp,0,for,map,entry,integer,tuple2,integer,integer,values,count,and,start,offset,partitions,to,values,count,and,start,offset,entry,set,final,count,tmp,values,count,and,start,offset,get,value,f0,final,int,final,count,final,count,tmp,final,type,information,tuple2,integer,integer,int,int,tuple,type,type,info,parser,parse,tuple2,integer,integer,final,type,information,serialization,schema,tuple2,integer,integer,deser,new,type,information,serialization,schema,int,int,tuple,type,env,get,config,cc,put,all,secure,props,flink,kafka,consumer,base,tuple2,integer,integer,consumer,kafka,server,get,consumer,topic,name,deser,cc,switch,startup,mode,case,earliest,consumer,set,start,from,earliest,break,case,latest,consumer,set,start,from,latest,break,case,consumer,set,start,from,group,offsets,break,data,stream,tuple2,integer,integer,source,env,add,source,consumer,set,parallelism,source,parallelism,map,new,throttled,mapper,tuple2,integer,integer,20,set,parallelism,source,parallelism,source,flat,map,new,rich,flat,map,function,tuple2,integer,integer,integer,private,hash,map,integer,bit,set,partitions,to,value,check,private,int,count,0,override,public,void,open,configuration,parameters,throws,exception,partitions,to,value,check,new,hash,map,for,integer,partition,partitions,to,values,count,and,start,offset,key,set,partitions,to,value,check,put,partition,new,bit,set,override,public,void,flat,map,tuple2,integer,integer,value,collector,integer,out,throws,exception,int,partition,value,f0,int,val,value,f1,bit,set,bit,set,partitions,to,value,check,get,partition,if,bit,set,null,throw,new,runtime,exception,got,a,record,from,an,unknown,partition,else,bit,set,set,val,partitions,to,values,count,and,start,offset,get,partition,f1,count,log,info,received,message,total,messages,value,count,if,count,final,count,for,map,entry,integer,bit,set,partitions,to,value,check,this,partitions,to,value,check,entry,set,bit,set,check,partitions,to,value,check,get,value,int,expected,value,count,partitions,to,values,count,and,start,offset,get,partitions,to,value,check,get,key,f0,if,check,cardinality,expected,value,count,throw,new,runtime,exception,expected,cardinality,to,be,expected,value,count,but,was,check,cardinality,else,if,check,next,clear,bit,0,expected,value,count,throw,new,runtime,exception,expected,next,clear,bit,to,be,expected,value,count,but,was,check,cardinality,throw,new,success,exception,set,parallelism,1,try,execute,env,read,data,from,kafka,log,info,successfully,read,sequence,for,verification
