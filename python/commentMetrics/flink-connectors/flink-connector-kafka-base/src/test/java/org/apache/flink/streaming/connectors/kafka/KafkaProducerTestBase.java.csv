# id;timestamp;commentText;codeText;commentWords;codeWords
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception;1526978550;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception {_		final String topic = (regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator") + sinksCount__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		for (int i = 0_ i < sinksCount_ i++) {_			createTestTopic(topic + i, 1, 1)__		}__		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		for (int i = 0_ i < sinksCount_ i++) {_			FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			}___			if (regularSink) {_				StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic + i, keyedSerializationSchema, properties, partitioner)__				inputStream.addSink(kafkaSink.getUserFunction())__			} else {_				kafkaServer.produceIntoKafka(inputStream, topic + i, keyedSerializationSchema, properties, partitioner)__			}_		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		for (int i = 0_ i < sinksCount_ i++) {_			_			assertExactlyOnceForTopic(_				properties,_				topic + i,_				partition,_				expectedElements,_				KAFKA_READ_TIMEOUT)__			deleteTestTopic(topic + i)__		}_	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,int,sinks,count,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,sinks,count,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,for,int,i,0,i,sinks,count,i,create,test,topic,topic,i,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,for,int,i,0,i,sinks,count,i,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,i,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,i,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,for,int,i,0,i,sinks,count,i,assert,exactly,once,for,topic,properties,topic,i,partition,expected,elements,delete,test,topic,topic,i
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception;1541587130;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception {_		final String topic = (regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator") + sinksCount__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		for (int i = 0_ i < sinksCount_ i++) {_			createTestTopic(topic + i, 1, 1)__		}__		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper<>(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		for (int i = 0_ i < sinksCount_ i++) {_			FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			}___			if (regularSink) {_				StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic + i, keyedSerializationSchema, properties, partitioner)__				inputStream.addSink(kafkaSink.getUserFunction())__			} else {_				kafkaServer.produceIntoKafka(inputStream, topic + i, keyedSerializationSchema, properties, partitioner)__			}_		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		for (int i = 0_ i < sinksCount_ i++) {_			_			assertExactlyOnceForTopic(_				properties,_				topic + i,_				partition,_				expectedElements,_				KAFKA_READ_TIMEOUT)__			deleteTestTopic(topic + i)__		}_	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,int,sinks,count,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,sinks,count,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,for,int,i,0,i,sinks,count,i,create,test,topic,topic,i,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,for,int,i,0,i,sinks,count,i,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,i,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,i,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,for,int,i,0,i,sinks,count,i,assert,exactly,once,for,topic,properties,topic,i,partition,expected,elements,delete,test,topic,topic,i
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception;1549282380;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink, int sinksCount) throws Exception {_		final String topic = (regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator") + sinksCount__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		for (int i = 0_ i < sinksCount_ i++) {_			createTestTopic(topic + i, 1, 1)__		}__		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper<>(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		for (int i = 0_ i < sinksCount_ i++) {_			FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			}___			if (regularSink) {_				StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic + i, keyedSerializationSchema, properties, partitioner)__				inputStream.addSink(kafkaSink.getUserFunction())__			} else {_				kafkaServer.produceIntoKafka(inputStream, topic + i, keyedSerializationSchema, properties, partitioner)__			}_		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		for (int i = 0_ i < sinksCount_ i++) {_			_			assertExactlyOnceForTopic(_				properties,_				topic + i,_				partition,_				expectedElements,_				KAFKA_READ_TIMEOUT)__			deleteTestTopic(topic + i)__		}_	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,int,sinks,count,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,sinks,count,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,for,int,i,0,i,sinks,count,i,create,test,topic,topic,i,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,for,int,i,0,i,sinks,count,i,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,i,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,i,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,for,int,i,0,i,sinks,count,i,assert,exactly,once,for,topic,properties,topic,i,partition,expected,elements,delete,test,topic,topic,i
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1499314317;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1502179982;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1502726910;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1505994399;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1507568316;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1509404700;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1509723634;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1511783194;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1521662511;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1521662511;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1526562770;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1526978550;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1541587130;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceCustomOperator() throws Exception;1549282380;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {_		testOneToOneAtLeastOnce(false)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,custom,operator,throws,exception,test,one,to,one,at,least,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1507568316;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1509404700;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1509723634;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1511783194;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1521662511;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1521662511;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1526562770;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1526978550;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true, 1)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true,1
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1541587130;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true, 1)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true,1
KafkaProducerTestBase -> @Test 	public void testExactlyOnceRegularSink() throws Exception;1549282380;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceRegularSink() throws Exception {_		testExactlyOnce(true, 1)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,regular,sink,throws,exception,test,exactly,once,true,1
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1507568316;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1509404700;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1509723634;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1511783194;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1521662511;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1521662511;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1526562770;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1526978550;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false, 1)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false,1
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1541587130;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false, 1)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false,1
KafkaProducerTestBase -> @Test 	public void testExactlyOnceCustomOperator() throws Exception;1549282380;Tests the exactly-once semantic for the simple writes into Kafka.;@Test_	public void testExactlyOnceCustomOperator() throws Exception {_		testExactlyOnce(false, 1)__	};tests,the,exactly,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,exactly,once,custom,operator,throws,exception,test,exactly,once,false,1
KafkaProducerTestBase -> private void assertAtLeastOnceForTopic( 			Properties properties, 			String topic, 			int partition, 			Set<Integer> expectedElements, 			long timeoutMillis) throws Exception;1499314317;We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error._After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.;private void assertAtLeastOnceForTopic(_			Properties properties,_			String topic,_			int partition,_			Set<Integer> expectedElements,_			long timeoutMillis) throws Exception {__		long startMillis = System.currentTimeMillis()__		Set<Integer> actualElements = new HashSet<>()___		_		while (System.currentTimeMillis() < startMillis + timeoutMillis) {_			properties.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")__			properties.put("value.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")___			_			Collection<ConsumerRecord<Integer, Integer>> records = kafkaServer.getAllRecordsFromTopic(properties, topic, partition, 100)___			for (ConsumerRecord<Integer, Integer> record : records) {_				actualElements.add(record.value())__			}__			_			if (actualElements.containsAll(expectedElements)) {_				return__			}_		}__		fail(String.format("Expected to contain all of: <%s>, but was: <%s>", expectedElements, actualElements))__	};we,manually,handle,the,timeout,instead,of,using,junit,s,timeout,to,return,failure,instead,of,timeout,error,after,timeout,we,assume,that,there,are,missing,records,and,there,is,a,bug,not,that,the,test,has,run,out,of,time;private,void,assert,at,least,once,for,topic,properties,properties,string,topic,int,partition,set,integer,expected,elements,long,timeout,millis,throws,exception,long,start,millis,system,current,time,millis,set,integer,actual,elements,new,hash,set,while,system,current,time,millis,start,millis,timeout,millis,properties,put,key,deserializer,org,apache,kafka,common,serialization,integer,deserializer,properties,put,value,deserializer,org,apache,kafka,common,serialization,integer,deserializer,collection,consumer,record,integer,integer,records,kafka,server,get,all,records,from,topic,properties,topic,partition,100,for,consumer,record,integer,integer,record,records,actual,elements,add,record,value,if,actual,elements,contains,all,expected,elements,return,fail,string,format,expected,to,contain,all,of,s,but,was,s,expected,elements,actual,elements
KafkaProducerTestBase -> private void assertAtLeastOnceForTopic( 			Properties properties, 			String topic, 			int partition, 			Set<Integer> expectedElements, 			long timeoutMillis) throws Exception;1502179982;We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error._After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.;private void assertAtLeastOnceForTopic(_			Properties properties,_			String topic,_			int partition,_			Set<Integer> expectedElements,_			long timeoutMillis) throws Exception {__		long startMillis = System.currentTimeMillis()__		Set<Integer> actualElements = new HashSet<>()___		_		while (System.currentTimeMillis() < startMillis + timeoutMillis) {_			properties.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")__			properties.put("value.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")___			_			Collection<ConsumerRecord<Integer, Integer>> records = kafkaServer.getAllRecordsFromTopic(properties, topic, partition, 100)___			for (ConsumerRecord<Integer, Integer> record : records) {_				actualElements.add(record.value())__			}__			_			if (actualElements.containsAll(expectedElements)) {_				return__			}_		}__		fail(String.format("Expected to contain all of: <%s>, but was: <%s>", expectedElements, actualElements))__	};we,manually,handle,the,timeout,instead,of,using,junit,s,timeout,to,return,failure,instead,of,timeout,error,after,timeout,we,assume,that,there,are,missing,records,and,there,is,a,bug,not,that,the,test,has,run,out,of,time;private,void,assert,at,least,once,for,topic,properties,properties,string,topic,int,partition,set,integer,expected,elements,long,timeout,millis,throws,exception,long,start,millis,system,current,time,millis,set,integer,actual,elements,new,hash,set,while,system,current,time,millis,start,millis,timeout,millis,properties,put,key,deserializer,org,apache,kafka,common,serialization,integer,deserializer,properties,put,value,deserializer,org,apache,kafka,common,serialization,integer,deserializer,collection,consumer,record,integer,integer,records,kafka,server,get,all,records,from,topic,properties,topic,partition,100,for,consumer,record,integer,integer,record,records,actual,elements,add,record,value,if,actual,elements,contains,all,expected,elements,return,fail,string,format,expected,to,contain,all,of,s,but,was,s,expected,elements,actual,elements
KafkaProducerTestBase -> private void assertAtLeastOnceForTopic( 			Properties properties, 			String topic, 			int partition, 			Set<Integer> expectedElements, 			long timeoutMillis) throws Exception;1502726910;We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error._After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.;private void assertAtLeastOnceForTopic(_			Properties properties,_			String topic,_			int partition,_			Set<Integer> expectedElements,_			long timeoutMillis) throws Exception {__		long startMillis = System.currentTimeMillis()__		Set<Integer> actualElements = new HashSet<>()___		_		while (System.currentTimeMillis() < startMillis + timeoutMillis) {_			properties.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")__			properties.put("value.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")___			_			Collection<ConsumerRecord<Integer, Integer>> records = kafkaServer.getAllRecordsFromTopic(properties, topic, partition, 100)___			for (ConsumerRecord<Integer, Integer> record : records) {_				actualElements.add(record.value())__			}__			_			if (actualElements.containsAll(expectedElements)) {_				return__			}_		}__		fail(String.format("Expected to contain all of: <%s>, but was: <%s>", expectedElements, actualElements))__	};we,manually,handle,the,timeout,instead,of,using,junit,s,timeout,to,return,failure,instead,of,timeout,error,after,timeout,we,assume,that,there,are,missing,records,and,there,is,a,bug,not,that,the,test,has,run,out,of,time;private,void,assert,at,least,once,for,topic,properties,properties,string,topic,int,partition,set,integer,expected,elements,long,timeout,millis,throws,exception,long,start,millis,system,current,time,millis,set,integer,actual,elements,new,hash,set,while,system,current,time,millis,start,millis,timeout,millis,properties,put,key,deserializer,org,apache,kafka,common,serialization,integer,deserializer,properties,put,value,deserializer,org,apache,kafka,common,serialization,integer,deserializer,collection,consumer,record,integer,integer,records,kafka,server,get,all,records,from,topic,properties,topic,partition,100,for,consumer,record,integer,integer,record,records,actual,elements,add,record,value,if,actual,elements,contains,all,expected,elements,return,fail,string,format,expected,to,contain,all,of,s,but,was,s,expected,elements,actual,elements
KafkaProducerTestBase -> private void assertAtLeastOnceForTopic( 			Properties properties, 			String topic, 			int partition, 			Set<Integer> expectedElements, 			long timeoutMillis) throws Exception;1505994399;We manually handle the timeout instead of using JUnit's timeout to return failure instead of timeout error._After timeout we assume that there are missing records and there is a bug, not that the test has run out of time.;private void assertAtLeastOnceForTopic(_			Properties properties,_			String topic,_			int partition,_			Set<Integer> expectedElements,_			long timeoutMillis) throws Exception {__		long startMillis = System.currentTimeMillis()__		Set<Integer> actualElements = new HashSet<>()___		_		while (System.currentTimeMillis() < startMillis + timeoutMillis) {_			properties.put("key.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")__			properties.put("value.deserializer", "org.apache.kafka.common.serialization.IntegerDeserializer")___			_			Collection<ConsumerRecord<Integer, Integer>> records = kafkaServer.getAllRecordsFromTopic(properties, topic, partition, 100)___			for (ConsumerRecord<Integer, Integer> record : records) {_				actualElements.add(record.value())__			}__			_			if (actualElements.containsAll(expectedElements)) {_				return__			}_		}__		fail(String.format("Expected to contain all of: <%s>, but was: <%s>", expectedElements, actualElements))__	};we,manually,handle,the,timeout,instead,of,using,junit,s,timeout,to,return,failure,instead,of,timeout,error,after,timeout,we,assume,that,there,are,missing,records,and,there,is,a,bug,not,that,the,test,has,run,out,of,time;private,void,assert,at,least,once,for,topic,properties,properties,string,topic,int,partition,set,integer,expected,elements,long,timeout,millis,throws,exception,long,start,millis,system,current,time,millis,set,integer,actual,elements,new,hash,set,while,system,current,time,millis,start,millis,timeout,millis,properties,put,key,deserializer,org,apache,kafka,common,serialization,integer,deserializer,properties,put,value,deserializer,org,apache,kafka,common,serialization,integer,deserializer,collection,consumer,record,integer,integer,records,kafka,server,get,all,records,from,topic,properties,topic,partition,100,for,consumer,record,integer,integer,record,records,actual,elements,add,record,value,if,actual,elements,contains,all,expected,elements,return,fail,string,format,expected,to,contain,all,of,s,but,was,s,expected,elements,actual,elements
KafkaProducerTestBase -> public void runCustomPartitioningTest();1480685315;<pre>_+------> (sink) --+--> [KAFKA-1] --> (source) -> (map) --+_/                  |                                       \_/                   |                                        \_(source) ----------> (sink) --+--> [KAFKA-2] --> (source) -> (map) -----+-> (sink)_\                   |                                        /_\                  |                                       /_+------> (sink) --+--> [KAFKA-3] --> (source) -> (map) --+_</pre>__The mapper validates that the values come consistently from the correct Kafka partition.__The final sink validates that there are no duplicates and that all partitions are present.;public void runCustomPartitioningTest() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String topic = "customPartitioningTestTopic"__			final int parallelism = 3__			_			createTestTopic(topic, parallelism, 1)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.createRemoteEnvironment("localhost", flinkPort)__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_					new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_					new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			_			_			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})_			.setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)__			_			_			kafkaServer.produceIntoKafka(stream, topic,_					new KeyedSerializationSchemaWrapper<>(serSchema),_					props,_					new CustomPartitioner(parallelism)).setParallelism(parallelism)___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topic, deserSchema, consumerProps)__			_			env.addSource(source).setParallelism(parallelism)__					_					.map(new RichMapFunction<Tuple2<Long, String>, Integer>() {_						_						private int ourPartition = -1__						@Override_						public Integer map(Tuple2<Long, String> value) {_							int partition = value.f0.intValue() % parallelism__							if (ourPartition != -1) {_								assertEquals("inconsistent partitioning", ourPartition, partition)__							} else {_								ourPartition = partition__							}_							return partition__						}_					}).setParallelism(parallelism)_					_					.addSink(new SinkFunction<Integer>() {_						_						private int[] valuesPerPartition = new int[parallelism]__						_						@Override_						public void invoke(Integer value) throws Exception {_							valuesPerPartition[value]++__							_							boolean missing = false__							for (int i : valuesPerPartition) {_								if (i < 100) {_									missing = true__									break__								}_							}_							if (!missing) {_								throw new SuccessException()__							}_						}_					}).setParallelism(1)__			_			tryExecute(env, "custom partitioning test")___			deleteTestTopic(topic)__			_			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};pre,sink,kafka,1,source,map,source,sink,kafka,2,source,map,sink,sink,kafka,3,source,map,pre,the,mapper,validates,that,the,values,come,consistently,from,the,correct,kafka,partition,the,final,sink,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;public,void,run,custom,partitioning,test,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,topic,custom,partitioning,test,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,create,remote,environment,localhost,flink,port,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,props,new,custom,partitioner,parallelism,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topic,deser,schema,consumer,props,env,add,source,source,set,parallelism,parallelism,map,new,rich,map,function,tuple2,long,string,integer,private,int,our,partition,1,override,public,integer,map,tuple2,long,string,value,int,partition,value,f0,int,value,parallelism,if,our,partition,1,assert,equals,inconsistent,partitioning,our,partition,partition,else,our,partition,partition,return,partition,set,parallelism,parallelism,add,sink,new,sink,function,integer,private,int,values,per,partition,new,int,parallelism,override,public,void,invoke,integer,value,throws,exception,values,per,partition,value,boolean,missing,false,for,int,i,values,per,partition,if,i,100,missing,true,break,if,missing,throw,new,success,exception,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> public void runCustomPartitioningTest();1493975167;<pre>_+------> (sink) --+--> [KAFKA-1] --> (source) -> (map) --+_/                  |                                       \_/                   |                                        \_(source) ----------> (sink) --+--> [KAFKA-2] --> (source) -> (map) -----+-> (sink)_\                   |                                        /_\                  |                                       /_+------> (sink) --+--> [KAFKA-3] --> (source) -> (map) --+_</pre>__The mapper validates that the values come consistently from the correct Kafka partition.__The final sink validates that there are no duplicates and that all partitions are present.;public void runCustomPartitioningTest() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String topic = "customPartitioningTestTopic"__			final int parallelism = 3__			_			createTestTopic(topic, parallelism, 1)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_					new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_					new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			_			_			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})_			.setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)__			_			_			kafkaServer.produceIntoKafka(stream, topic,_					new KeyedSerializationSchemaWrapper<>(serSchema),_					props,_					new CustomPartitioner(parallelism)).setParallelism(parallelism)___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topic, deserSchema, consumerProps)__			_			env.addSource(source).setParallelism(parallelism)__					_					.map(new RichMapFunction<Tuple2<Long, String>, Integer>() {_						_						private int ourPartition = -1__						@Override_						public Integer map(Tuple2<Long, String> value) {_							int partition = value.f0.intValue() % parallelism__							if (ourPartition != -1) {_								assertEquals("inconsistent partitioning", ourPartition, partition)__							} else {_								ourPartition = partition__							}_							return partition__						}_					}).setParallelism(parallelism)_					_					.addSink(new SinkFunction<Integer>() {_						_						private int[] valuesPerPartition = new int[parallelism]__						_						@Override_						public void invoke(Integer value) throws Exception {_							valuesPerPartition[value]++__							_							boolean missing = false__							for (int i : valuesPerPartition) {_								if (i < 100) {_									missing = true__									break__								}_							}_							if (!missing) {_								throw new SuccessException()__							}_						}_					}).setParallelism(1)__			_			tryExecute(env, "custom partitioning test")___			deleteTestTopic(topic)__			_			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};pre,sink,kafka,1,source,map,source,sink,kafka,2,source,map,sink,sink,kafka,3,source,map,pre,the,mapper,validates,that,the,values,come,consistently,from,the,correct,kafka,partition,the,final,sink,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;public,void,run,custom,partitioning,test,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,topic,custom,partitioning,test,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,props,new,custom,partitioner,parallelism,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topic,deser,schema,consumer,props,env,add,source,source,set,parallelism,parallelism,map,new,rich,map,function,tuple2,long,string,integer,private,int,our,partition,1,override,public,integer,map,tuple2,long,string,value,int,partition,value,f0,int,value,parallelism,if,our,partition,1,assert,equals,inconsistent,partitioning,our,partition,partition,else,our,partition,partition,return,partition,set,parallelism,parallelism,add,sink,new,sink,function,integer,private,int,values,per,partition,new,int,parallelism,override,public,void,invoke,integer,value,throws,exception,values,per,partition,value,boolean,missing,false,for,int,i,values,per,partition,if,i,100,missing,true,break,if,missing,throw,new,success,exception,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> public void runCustomPartitioningTest();1495175928;<pre>_+------> (sink) --+--> [KAFKA-1] --> (source) -> (map) --+_/                  |                                       \_/                   |                                        \_(source) ----------> (sink) --+--> [KAFKA-2] --> (source) -> (map) -----+-> (sink)_\                   |                                        /_\                  |                                       /_+------> (sink) --+--> [KAFKA-3] --> (source) -> (map) --+_</pre>__The mapper validates that the values come consistently from the correct Kafka partition.__The final sink validates that there are no duplicates and that all partitions are present.;public void runCustomPartitioningTest() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String topic = "customPartitioningTestTopic"__			final int parallelism = 3__			_			createTestTopic(topic, parallelism, 1)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_					new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_					new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			_			_			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			})_			.setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)__			_			_			kafkaServer.produceIntoKafka(stream, topic,_					new KeyedSerializationSchemaWrapper<>(serSchema),_					props,_					new CustomPartitioner(parallelism)).setParallelism(parallelism)___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> source = kafkaServer.getConsumer(topic, deserSchema, consumerProps)__			_			env.addSource(source).setParallelism(parallelism)__					_					.map(new RichMapFunction<Tuple2<Long, String>, Integer>() {_						_						private int ourPartition = -1__						@Override_						public Integer map(Tuple2<Long, String> value) {_							int partition = value.f0.intValue() % parallelism__							if (ourPartition != -1) {_								assertEquals("inconsistent partitioning", ourPartition, partition)__							} else {_								ourPartition = partition__							}_							return partition__						}_					}).setParallelism(parallelism)_					_					.addSink(new SinkFunction<Integer>() {_						_						private int[] valuesPerPartition = new int[parallelism]__						_						@Override_						public void invoke(Integer value) throws Exception {_							valuesPerPartition[value]++__							_							boolean missing = false__							for (int i : valuesPerPartition) {_								if (i < 100) {_									missing = true__									break__								}_							}_							if (!missing) {_								throw new SuccessException()__							}_						}_					}).setParallelism(1)__			_			tryExecute(env, "custom partitioning test")___			deleteTestTopic(topic)__			_			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};pre,sink,kafka,1,source,map,source,sink,kafka,2,source,map,sink,sink,kafka,3,source,map,pre,the,mapper,validates,that,the,values,come,consistently,from,the,correct,kafka,partition,the,final,sink,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;public,void,run,custom,partitioning,test,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,topic,custom,partitioning,test,topic,final,int,parallelism,3,create,test,topic,topic,parallelism,1,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,topic,new,keyed,serialization,schema,wrapper,ser,schema,props,new,custom,partitioner,parallelism,set,parallelism,parallelism,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,source,kafka,server,get,consumer,topic,deser,schema,consumer,props,env,add,source,source,set,parallelism,parallelism,map,new,rich,map,function,tuple2,long,string,integer,private,int,our,partition,1,override,public,integer,map,tuple2,long,string,value,int,partition,value,f0,int,value,parallelism,if,our,partition,1,assert,equals,inconsistent,partitioning,our,partition,partition,else,our,partition,partition,return,partition,set,parallelism,parallelism,add,sink,new,sink,function,integer,private,int,values,per,partition,new,int,parallelism,override,public,void,invoke,integer,value,throws,exception,values,per,partition,value,boolean,missing,false,for,int,i,values,per,partition,if,i,100,missing,true,break,if,missing,throw,new,success,exception,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> public void runCustomPartitioningTest();1495175928;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;public void runCustomPartitioningTest() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(stream, defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;public,void,run,custom,partitioning,test,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> public void runCustomPartitioningTest();1495923077;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;public void runCustomPartitioningTest() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(stream, defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;public,void,run,custom,partitioning,test,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1499314317;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1502179982;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1502726910;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1505994399;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1507568316;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1509404700;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1509723634;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1511783194;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1521662511;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1521662511;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInfoParser.parse("Tuple2<Long, String>")___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,info,parser,parse,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1526562770;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,information,of,new,type,hint,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1526978550;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,information,of,new,type,hint,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1541587130;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,information,of,new,type,hint,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> @Test 	public void testCustomPartitioning();1549282380;This tests verifies that custom partitioning works correctly, with a default topic_and dynamic topic. The number of partitions for each topic is deliberately different.__<p>Test topology:__<pre>_+------> (sink) --+--> [DEFAULT_TOPIC-1] --> (source) -> (map) -----+_/                  |                             |          |        |_|                   |                             |          |  ------+--> (sink)_+------> (sink) --+--> [DEFAULT_TOPIC-2] --> (source) -> (map) -----+_/                  |_|                   |_(source) ----------> (sink) --+--> [DYNAMIC_TOPIC-1] --> (source) -> (map) -----+_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-2] --> (source) -> (map) -----+--> (sink)_|                   |                             |          |        |_\                  |                             |          |        |_+------> (sink) --+--> [DYNAMIC_TOPIC-3] --> (source) -> (map) -----+_</pre>__<p>Each topic has an independent mapper that validates the values come consistently from_the correct Kafka partition of the topic is is responsible of.__<p>Each topic also has a final sink that validates that there are no duplicates and that all_partitions are present.;@Test_	public void testCustomPartitioning() {_		try {_			LOG.info("Starting KafkaProducerITCase.testCustomPartitioning()")___			final String defaultTopic = "defaultTopic"__			final int defaultTopicPartitions = 2___			final String dynamicTopic = "dynamicTopic"__			final int dynamicTopicPartitions = 3___			createTestTopic(defaultTopic, defaultTopicPartitions, 1)__			createTestTopic(dynamicTopic, dynamicTopicPartitions, 1)___			Map<String, Integer> expectedTopicsToNumPartitions = new HashMap<>(2)__			expectedTopicsToNumPartitions.put(defaultTopic, defaultTopicPartitions)__			expectedTopicsToNumPartitions.put(dynamicTopic, dynamicTopicPartitions)___			TypeInformation<Tuple2<Long, String>> longStringInfo = TypeInformation.of(new TypeHint<Tuple2<Long, String>>(){})___			StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__			env.setRestartStrategy(RestartStrategies.noRestart())__			env.getConfig().disableSysoutLogging()___			TypeInformationSerializationSchema<Tuple2<Long, String>> serSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			TypeInformationSerializationSchema<Tuple2<Long, String>> deserSchema =_				new TypeInformationSerializationSchema<>(longStringInfo, env.getConfig())___			__			_			DataStream<Tuple2<Long, String>> stream = env.addSource(new SourceFunction<Tuple2<Long, String>>() {__				private boolean running = true___				@Override_				public void run(SourceContext<Tuple2<Long, String>> ctx) throws Exception {_					long cnt = 0__					while (running) {_						ctx.collect(new Tuple2<Long, String>(cnt, "kafka-" + cnt))__						cnt++__						if (cnt % 100 == 0) {_							Thread.sleep(1)__						}_					}_				}__				@Override_				public void cancel() {_					running = false__				}_			}).setParallelism(1)___			Properties props = new Properties()__			props.putAll(FlinkKafkaProducerBase.getPropertiesFromBrokerList(brokerConnectionStrings))__			props.putAll(secureProps)___			_			kafkaServer.produceIntoKafka(_					stream,_					defaultTopic,_					_					new CustomKeyedSerializationSchemaWrapper(serSchema, defaultTopic, dynamicTopic),_					props,_					new CustomPartitioner(expectedTopicsToNumPartitions))_				.setParallelism(Math.max(defaultTopicPartitions, dynamicTopicPartitions))___			__			Properties consumerProps = new Properties()__			consumerProps.putAll(standardProps)__			consumerProps.putAll(secureProps)___			FlinkKafkaConsumerBase<Tuple2<Long, String>> defaultTopicSource =_					kafkaServer.getConsumer(defaultTopic, deserSchema, consumerProps)__			FlinkKafkaConsumerBase<Tuple2<Long, String>> dynamicTopicSource =_					kafkaServer.getConsumer(dynamicTopic, deserSchema, consumerProps)___			env.addSource(defaultTopicSource).setParallelism(defaultTopicPartitions)_				.map(new PartitionValidatingMapper(defaultTopicPartitions)).setParallelism(defaultTopicPartitions)_				.addSink(new PartitionValidatingSink(defaultTopicPartitions)).setParallelism(1)___			env.addSource(dynamicTopicSource).setParallelism(dynamicTopicPartitions)_				.map(new PartitionValidatingMapper(dynamicTopicPartitions)).setParallelism(dynamicTopicPartitions)_				.addSink(new PartitionValidatingSink(dynamicTopicPartitions)).setParallelism(1)___			tryExecute(env, "custom partitioning test")___			deleteTestTopic(defaultTopic)__			deleteTestTopic(dynamicTopic)___			LOG.info("Finished KafkaProducerITCase.testCustomPartitioning()")__		}_		catch (Exception e) {_			e.printStackTrace()__			fail(e.getMessage())__		}_	};this,tests,verifies,that,custom,partitioning,works,correctly,with,a,default,topic,and,dynamic,topic,the,number,of,partitions,for,each,topic,is,deliberately,different,p,test,topology,pre,sink,1,source,map,sink,sink,2,source,map,source,sink,1,source,map,sink,2,source,map,sink,sink,3,source,map,pre,p,each,topic,has,an,independent,mapper,that,validates,the,values,come,consistently,from,the,correct,kafka,partition,of,the,topic,is,is,responsible,of,p,each,topic,also,has,a,final,sink,that,validates,that,there,are,no,duplicates,and,that,all,partitions,are,present;test,public,void,test,custom,partitioning,try,log,info,starting,kafka,producer,itcase,test,custom,partitioning,final,string,default,topic,default,topic,final,int,default,topic,partitions,2,final,string,dynamic,topic,dynamic,topic,final,int,dynamic,topic,partitions,3,create,test,topic,default,topic,default,topic,partitions,1,create,test,topic,dynamic,topic,dynamic,topic,partitions,1,map,string,integer,expected,topics,to,num,partitions,new,hash,map,2,expected,topics,to,num,partitions,put,default,topic,default,topic,partitions,expected,topics,to,num,partitions,put,dynamic,topic,dynamic,topic,partitions,type,information,tuple2,long,string,long,string,info,type,information,of,new,type,hint,tuple2,long,string,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,type,information,serialization,schema,tuple2,long,string,ser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,type,information,serialization,schema,tuple2,long,string,deser,schema,new,type,information,serialization,schema,long,string,info,env,get,config,data,stream,tuple2,long,string,stream,env,add,source,new,source,function,tuple2,long,string,private,boolean,running,true,override,public,void,run,source,context,tuple2,long,string,ctx,throws,exception,long,cnt,0,while,running,ctx,collect,new,tuple2,long,string,cnt,kafka,cnt,cnt,if,cnt,100,0,thread,sleep,1,override,public,void,cancel,running,false,set,parallelism,1,properties,props,new,properties,props,put,all,flink,kafka,producer,base,get,properties,from,broker,list,broker,connection,strings,props,put,all,secure,props,kafka,server,produce,into,kafka,stream,default,topic,new,custom,keyed,serialization,schema,wrapper,ser,schema,default,topic,dynamic,topic,props,new,custom,partitioner,expected,topics,to,num,partitions,set,parallelism,math,max,default,topic,partitions,dynamic,topic,partitions,properties,consumer,props,new,properties,consumer,props,put,all,standard,props,consumer,props,put,all,secure,props,flink,kafka,consumer,base,tuple2,long,string,default,topic,source,kafka,server,get,consumer,default,topic,deser,schema,consumer,props,flink,kafka,consumer,base,tuple2,long,string,dynamic,topic,source,kafka,server,get,consumer,dynamic,topic,deser,schema,consumer,props,env,add,source,default,topic,source,set,parallelism,default,topic,partitions,map,new,partition,validating,mapper,default,topic,partitions,set,parallelism,default,topic,partitions,add,sink,new,partition,validating,sink,default,topic,partitions,set,parallelism,1,env,add,source,dynamic,topic,source,set,parallelism,dynamic,topic,partitions,map,new,partition,validating,mapper,dynamic,topic,partitions,set,parallelism,dynamic,topic,partitions,add,sink,new,partition,validating,sink,dynamic,topic,partitions,set,parallelism,1,try,execute,env,custom,partitioning,test,delete,test,topic,default,topic,delete,test,topic,dynamic,topic,log,info,finished,kafka,producer,itcase,test,custom,partitioning,catch,exception,e,e,print,stack,trace,fail,e,get,message
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1499314317;This test sets KafkaProducer so that it will not automatically flush the data and_and fails the broker to check whether FlinkKafkaProducer flushed records manually on snapshotState.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		int leaderId = kafkaServer.getLeaderToShutDown(topic)__		BrokerRestartingMapper.resetState()___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<Integer>(leaderId, failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			assertEquals("Broker was shutdown!", ex.getCause().getMessage())__		}__		kafkaServer.restartBroker(leaderId)___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				ImmutableSet.copyOf(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot)),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,int,leader,id,kafka,server,get,leader,to,shut,down,topic,broker,restarting,mapper,reset,state,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,integer,leader,id,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,assert,equals,broker,was,shutdown,ex,get,cause,get,message,kafka,server,restart,broker,leader,id,assert,at,least,once,for,topic,properties,topic,partition,immutable,set,copy,of,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1502179982;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				ImmutableSet.copyOf(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot)),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,immutable,set,copy,of,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1502726910;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1505994399;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1507568316;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1509404700;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1509723634;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1511783194;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1521662511;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.numElementsBeforeSnapshot))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,num,elements,before,snapshot,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1521662511;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.lastSnapshotedElementBeforeShutdown))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,last,snapshoted,element,before,shutdown,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1526562770;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.lastSnapshotedElementBeforeShutdown))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,last,snapshoted,element,before,shutdown,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1526978550;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.lastSnapshotedElementBeforeShutdown))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,last,snapshoted,element,before,shutdown,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1541587130;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.lastSnapshotedElementBeforeShutdown))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,last,snapshoted,element,before,shutdown,delete,test,topic,topic
KafkaProducerTestBase -> protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception;1549282380;This test sets KafkaProducer so that it will not automatically flush the data and_simulate network failure between Flink and Kafka to check whether FlinkKafkaProducer_flushed records manually on snapshotState.__<p>Due to legacy reasons there are two different ways of instantiating a Kafka 0.10 sink. The_parameter controls which method is used.;protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "oneToOneTopicRegularSink" : "oneToOneTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.noRestart())__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)__		_		properties.setProperty("timeout.ms", "10000")__		properties.setProperty("max.block.ms", "10000")__		_		properties.setProperty("batch.size", "10240000")__		properties.setProperty("linger.ms", "10000")___		BrokerRestartingMapper.resetState(kafkaServer::blockProxyTraffic)___		_		DataStream<Integer> inputStream = env_			.fromCollection(getIntegersSequence(numElements))_			.map(new BrokerRestartingMapper<>(failAfterElements))___		StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		})___		if (regularSink) {_			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner<Integer>() {_				@Override_				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_					return partition__				}_			})__		}__		FailingIdentityMapper.failedBefore = false__		try {_			env.execute("One-to-one at least once test")__			fail("Job should fail!")__		}_		catch (JobExecutionException ex) {_			_		}__		kafkaServer.unblockProxyTraffic()___		_		assertAtLeastOnceForTopic(_				properties,_				topic,_				partition,_				Collections.unmodifiableSet(new HashSet<>(getIntegersSequence(BrokerRestartingMapper.lastSnapshotedElementBeforeShutdown))),_				KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,not,automatically,flush,the,data,and,simulate,network,failure,between,flink,and,kafka,to,check,whether,flink,kafka,producer,flushed,records,manually,on,snapshot,state,p,due,to,legacy,reasons,there,are,two,different,ways,of,instantiating,a,kafka,0,10,sink,the,parameter,controls,which,method,is,used;protected,void,test,one,to,one,at,least,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,one,to,one,topic,regular,sink,one,to,one,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,no,restart,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,properties,set,property,timeout,ms,10000,properties,set,property,max,block,ms,10000,properties,set,property,batch,size,10240000,properties,set,property,linger,ms,10000,broker,restarting,mapper,reset,state,kafka,server,block,proxy,traffic,data,stream,integer,input,stream,env,from,collection,get,integers,sequence,num,elements,map,new,broker,restarting,mapper,fail,after,elements,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,failing,identity,mapper,failed,before,false,try,env,execute,one,to,one,at,least,once,test,fail,job,should,fail,catch,job,execution,exception,ex,kafka,server,unblock,proxy,traffic,assert,at,least,once,for,topic,properties,topic,partition,collections,unmodifiable,set,new,hash,set,get,integers,sequence,broker,restarting,mapper,last,snapshoted,element,before,shutdown,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1507568316;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1509404700;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1509723634;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			30000L)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,30000l,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1511783194;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1521662511;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1521662511;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,delete,test,topic,topic
KafkaProducerTestBase -> protected void testExactlyOnce(boolean regularSink) throws Exception;1526562770;This test sets KafkaProducer so that it will  automatically flush the data and_and fails the broker to check whether flushed records since last checkpoint were not duplicated.;protected void testExactlyOnce(boolean regularSink) throws Exception {_		final String topic = regularSink ? "exactlyOnceTopicRegularSink" : "exactlyTopicCustomOperator"__		final int partition = 0__		final int numElements = 1000__		final int failAfterElements = 333___		createTestTopic(topic, 1, 1)___		TypeInformationSerializationSchema<Integer> schema = new TypeInformationSerializationSchema<>(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig())__		KeyedSerializationSchema<Integer> keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.enableCheckpointing(500)__		env.setParallelism(1)__		env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1, 0))__		env.getConfig().disableSysoutLogging()___		Properties properties = new Properties()__		properties.putAll(standardProps)__		properties.putAll(secureProps)___		_		List<Integer> expectedElements = getIntegersSequence(numElements)___		DataStream<Integer> inputStream = env_			.addSource(new IntegerSource(numElements))_			.map(new FailingIdentityMapper<Integer>(failAfterElements))___		FlinkKafkaPartitioner<Integer> partitioner = new FlinkKafkaPartitioner<Integer>() {_			@Override_			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) {_				return partition__			}_		}__		if (regularSink) {_			StreamSink<Integer> kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, partitioner)__			inputStream.addSink(kafkaSink.getUserFunction())__		}_		else {_			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, partitioner)__		}__		FailingIdentityMapper.failedBefore = false__		TestUtils.tryExecute(env, "Exactly once test")___		_		assertExactlyOnceForTopic(_			properties,_			topic,_			partition,_			expectedElements,_			KAFKA_READ_TIMEOUT)___		deleteTestTopic(topic)__	};this,test,sets,kafka,producer,so,that,it,will,automatically,flush,the,data,and,and,fails,the,broker,to,check,whether,flushed,records,since,last,checkpoint,were,not,duplicated;protected,void,test,exactly,once,boolean,regular,sink,throws,exception,final,string,topic,regular,sink,exactly,once,topic,regular,sink,exactly,topic,custom,operator,final,int,partition,0,final,int,num,elements,1000,final,int,fail,after,elements,333,create,test,topic,topic,1,1,type,information,serialization,schema,integer,schema,new,type,information,serialization,schema,basic,type,info,new,execution,config,keyed,serialization,schema,integer,keyed,serialization,schema,new,keyed,serialization,schema,wrapper,schema,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,enable,checkpointing,500,env,set,parallelism,1,env,set,restart,strategy,restart,strategies,fixed,delay,restart,1,0,env,get,config,disable,sysout,logging,properties,properties,new,properties,properties,put,all,standard,props,properties,put,all,secure,props,list,integer,expected,elements,get,integers,sequence,num,elements,data,stream,integer,input,stream,env,add,source,new,integer,source,num,elements,map,new,failing,identity,mapper,integer,fail,after,elements,flink,kafka,partitioner,integer,partitioner,new,flink,kafka,partitioner,integer,override,public,int,partition,integer,record,byte,key,byte,value,string,target,topic,int,partitions,return,partition,if,regular,sink,stream,sink,integer,kafka,sink,kafka,server,get,producer,sink,topic,keyed,serialization,schema,properties,partitioner,input,stream,add,sink,kafka,sink,get,user,function,else,kafka,server,produce,into,kafka,input,stream,topic,keyed,serialization,schema,properties,partitioner,failing,identity,mapper,failed,before,false,test,utils,try,execute,env,exactly,once,test,assert,exactly,once,for,topic,properties,topic,partition,expected,elements,delete,test,topic,topic
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1499314317;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1502179982;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1502726910;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1505994399;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1507568316;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1509404700;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1509723634;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1511783194;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1521662511;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1521662511;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1526562770;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1526978550;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1541587130;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
KafkaProducerTestBase -> @Test 	public void testOneToOneAtLeastOnceRegularSink() throws Exception;1549282380;Tests the at-least-once semantic for the simple writes into Kafka.;@Test_	public void testOneToOneAtLeastOnceRegularSink() throws Exception {_		testOneToOneAtLeastOnce(true)__	};tests,the,at,least,once,semantic,for,the,simple,writes,into,kafka;test,public,void,test,one,to,one,at,least,once,regular,sink,throws,exception,test,one,to,one,at,least,once,true
