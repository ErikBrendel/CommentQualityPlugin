# id;timestamp;commentText;codeText;commentWords;codeWords
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1480685315;This tests user defined hdfs configuration_@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer,String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1487871589;This tests user defined hdfs configuration_@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer,String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1495923089;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1515481551;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1515600854;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1519567828;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1524138809;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1529682304;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1530796781;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1540389860;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testUserDefinedConfiguration() throws Exception;1550863152;This tests user defined hdfs configuration._@throws Exception;@Test_	public void testUserDefinedConfiguration() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-with-config"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_			.broadcast()_			.filter(new OddEvenFilter())___		Configuration conf = new Configuration()__		conf.set("io.file.buffer.size", "40960")__		RollingSink<String> sink = new RollingSink<String>(outPath)_			.setFSConfig(conf)_			.setWriter(new StreamWriterWithConfigCheck<String>("io.file.buffer.size", "40960"))_			.setBucketer(new NonRollingBucketer())_			.setPartPrefix("part")_			.setPendingPrefix("")_			.setPendingSuffix("")___		source_			.map(new MapFunction<Tuple2<Integer, String>, String>() {_				private static final long serialVersionUID = 1L__				@Override_				public String map(Tuple2<Integer, String> value) throws Exception {_					return value.f1__				}_			})_			.addSink(sink)___		env.execute("RollingSink with configuration Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,user,defined,hdfs,configuration,throws,exception;test,public,void,test,user,defined,configuration,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,with,config,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,configuration,conf,new,configuration,conf,set,io,file,buffer,size,40960,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,fsconfig,conf,set,writer,new,stream,writer,with,config,check,string,io,file,buffer,size,40960,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,with,configuration,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1480685315;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)_____		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				NUM_ELEMENTS))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1487871589;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)_____		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				NUM_ELEMENTS))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1495923089;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1515481551;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1515600854;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1519567828;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1524138809;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1529682304;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1530796781;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1540389860;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testDateTimeRollingStringWriter() throws Exception;1550863152;This uses {@link org.apache.flink.streaming.connectors.fs.DateTimeBucketer} to_produce rolling files. The clock of DateTimeBucketer is set to_{@link ModifyableClock} to keep the time in lockstep with the processing of elements using_latches.;@Test_	public void testDateTimeRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/rolling-out"__		DateTimeBucketer.setClock(new ModifyableClock())__		ModifyableClock.setCurrentTime(0)___		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new WaitingTestSourceFunction(_				numElements))_				.broadcast()___		_		_		DataStream<String> mapped = source_				.flatMap(new RichFlatMapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L___					int count = 0__					@Override_					public void flatMap(Tuple2<Integer, String> value,_							Collector<String> out) throws Exception {_						out.collect(value.f1)__						count++__						if (count >= 5) {_							if (getRuntimeContext().getIndexOfThisSubtask() == 0) {_								latch1.trigger()__							} else {_								latch2.trigger()__							}_							count = 0__						}_					}__				})___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new DateTimeBucketer("ss"))_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		RemoteIterator<LocatedFileStatus> files = dfs.listFiles(new Path(outPath), true)___		_		int numFiles = 0__		while (files.hasNext()) {_			LocatedFileStatus file = files.next()__			numFiles++__			if (file.getPath().toString().contains("rolling-out/00")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 0_ i < 5_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/05")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 5_ i < 10_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/10")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 10_ i < 15_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else if (file.getPath().toString().contains("rolling-out/15")) {_				FSDataInputStream inStream = dfs.open(file.getPath())___				BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___				for (int i = 15_ i < 20_ i++) {_					String line = br.readLine()__					Assert.assertEquals("message #" + i, line)__				}__				inStream.close()__			} else {_				Assert.fail("File " + file + " does not match any expected roll pattern.")__			}_		}__		Assert.assertEquals(8, numFiles)__	};this,uses,link,org,apache,flink,streaming,connectors,fs,date,time,bucketer,to,produce,rolling,files,the,clock,of,date,time,bucketer,is,set,to,link,modifyable,clock,to,keep,the,time,in,lockstep,with,the,processing,of,elements,using,latches;test,public,void,test,date,time,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,rolling,out,date,time,bucketer,set,clock,new,modifyable,clock,modifyable,clock,set,current,time,0,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,waiting,test,source,function,num,elements,broadcast,data,stream,string,mapped,source,flat,map,new,rich,flat,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,int,count,0,override,public,void,flat,map,tuple2,integer,string,value,collector,string,out,throws,exception,out,collect,value,f1,count,if,count,5,if,get,runtime,context,get,index,of,this,subtask,0,latch1,trigger,else,latch2,trigger,count,0,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,date,time,bucketer,ss,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,remote,iterator,located,file,status,files,dfs,list,files,new,path,out,path,true,int,num,files,0,while,files,has,next,located,file,status,file,files,next,num,files,if,file,get,path,to,string,contains,rolling,out,00,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,5,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,05,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,5,i,10,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,10,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,10,i,15,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,if,file,get,path,to,string,contains,rolling,out,15,fsdata,input,stream,in,stream,dfs,open,file,get,path,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,15,i,20,i,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,else,assert,fail,file,file,does,not,match,any,expected,roll,pattern,assert,assert,equals,8,num,files
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1480685315;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer,String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1487871589;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer,String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1495923089;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1515481551;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1515600854;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1519567828;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1524138809;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1529682304;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1530796781;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1540389860;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingStringWriter() throws Exception;1550863152;This tests {@link StringWriter} with_non-rolling output.;@Test_	public void testNonRollingStringWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/string-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		RollingSink<String> sink = new RollingSink<String>(outPath)_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source_				.map(new MapFunction<Tuple2<Integer, String>, String>() {_					private static final long serialVersionUID = 1L__					@Override_					public String map(Tuple2<Integer, String> value) throws Exception {_						return value.f1__					}_				})_				.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		BufferedReader br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 0_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		br = new BufferedReader(new InputStreamReader(inStream))___		for (int i = 1_ i < numElements_ i += 2) {_			String line = br.readLine()__			Assert.assertEquals("message #" + i, line)__		}__		inStream.close()__	};this,tests,link,string,writer,with,non,rolling,output;test,public,void,test,non,rolling,string,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,string,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,rolling,sink,string,sink,new,rolling,sink,string,out,path,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,map,new,map,function,tuple2,integer,string,string,private,static,final,long,serial,version,uid,1l,override,public,string,map,tuple2,integer,string,value,throws,exception,return,value,f1,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,buffered,reader,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,0,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,br,new,buffered,reader,new,input,stream,reader,in,stream,for,int,i,1,i,num,elements,i,2,string,line,br,read,line,assert,assert,equals,message,i,line,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1480685315;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())____		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1487871589;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())____		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1495923089;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1515481551;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1515600854;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1519567828;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1524138809;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1529682304;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1530796781;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1540389860;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception;1550863152;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and with compression.;@Test_	public void testNonRollingAvroKeyValueWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS, String.valueOf(true))__		properties.put(AvroKeyValueSinkWriter.CONF_COMPRESS_CODEC, DataFileConstants.SNAPPY_CODEC)__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,with,compression;test,public,void,test,non,rolling,avro,key,value,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,properties,put,avro,key,value,sink,writer,string,value,of,true,properties,put,avro,key,value,sink,writer,data,file,constants,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1480685315;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer,String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})____		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1487871589;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer,String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})____		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1495923089;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1515481551;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1515600854;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1519567828;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1524138809;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1529682304;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1530796781;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1540389860;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception;1550863152;This tests {@link SequenceFileWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingSequenceFileWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>())_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,sequence,file,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1480685315;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())____		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1487871589;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())____		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1495923089;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1515481551;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1515600854;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1519567828;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1524138809;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1529682304;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1530796781;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1540389860;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception;1550863152;This tests {@link AvroKeyValueSinkWriter}_with non-rolling output and without compression.;@Test_	public void testNonRollingAvroKeyValueWithoutCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/avro-kv-no-comp-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		Map<String, String> properties = new HashMap<>()__		Schema keySchema = Schema.create(Type.INT)__		Schema valueSchema = Schema.create(Type.STRING)__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_KEY_SCHEMA, keySchema.toString())__		properties.put(AvroKeyValueSinkWriter.CONF_OUTPUT_VALUE_SCHEMA, valueSchema.toString())__		RollingSink<Tuple2<Integer, String>> sink = new RollingSink<Tuple2<Integer, String>>(outPath)_				.setWriter(new AvroKeyValueSinkWriter<Integer, String>(properties))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		source.addSink(sink)___		env.execute("RollingSink Avro KeyValue Writer Test")___		GenericData.setStringType(valueSchema, StringType.String)__		Schema elementSchema = AvroKeyValue.getSchema(keySchema, valueSchema)___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))__		SpecificDatumReader<GenericRecord> elementReader = new SpecificDatumReader<GenericRecord>(elementSchema)__		DataFileStream<GenericRecord> dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)__		for (int i = 0_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))__		dataFileStream = new DataFileStream<GenericRecord>(inStream, elementReader)___		for (int i = 1_ i < numElements_ i += 2) {_			AvroKeyValue<Integer, String> wrappedEntry = new AvroKeyValue<Integer, String>(dataFileStream.next())__			int key = wrappedEntry.getKey().intValue()__			Assert.assertEquals(i, key)__			String value = wrappedEntry.getValue()__			Assert.assertEquals("message #" + i, value)__		}__		dataFileStream.close()__		inStream.close()__	};this,tests,link,avro,key,value,sink,writer,with,non,rolling,output,and,without,compression;test,public,void,test,non,rolling,avro,key,value,without,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,avro,kv,no,comp,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,map,string,string,properties,new,hash,map,schema,key,schema,schema,create,type,int,schema,value,schema,schema,create,type,string,properties,put,avro,key,value,sink,writer,key,schema,to,string,properties,put,avro,key,value,sink,writer,value,schema,to,string,rolling,sink,tuple2,integer,string,sink,new,rolling,sink,tuple2,integer,string,out,path,set,writer,new,avro,key,value,sink,writer,integer,string,properties,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,source,add,sink,sink,env,execute,rolling,sink,avro,key,value,writer,test,generic,data,set,string,type,value,schema,string,type,string,schema,element,schema,avro,key,value,get,schema,key,schema,value,schema,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,specific,datum,reader,generic,record,element,reader,new,specific,datum,reader,generic,record,element,schema,data,file,stream,generic,record,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,0,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,data,file,stream,new,data,file,stream,generic,record,in,stream,element,reader,for,int,i,1,i,num,elements,i,2,avro,key,value,integer,string,wrapped,entry,new,avro,key,value,integer,string,data,file,stream,next,int,key,wrapped,entry,get,key,int,value,assert,assert,equals,i,key,string,value,wrapped,entry,get,value,assert,assert,equals,message,i,value,data,file,stream,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1480685315;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer,String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})____		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1487871589;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int NUM_ELEMENTS = 20__		final int PARALLELISM = 2__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(PARALLELISM)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(NUM_ELEMENTS))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer,String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})____		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < NUM_ELEMENTS_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,20,final,int,parallelism,2,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,parallelism,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1495923089;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1515481551;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1515600854;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1519567828;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1524138809;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1529682304;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1530796781;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1540389860;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
RollingSinkITCase -> @Test 	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception;1550863152;This tests {@link SequenceFileWriter}_with non-rolling output but with compression.;@Test_	public void testNonRollingSequenceFileWithCompressionWriter() throws Exception {_		final int numElements = 20__		final String outPath = hdfsURI + "/seq-non-rolling-out"__		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment()__		env.setParallelism(2)___		DataStream<Tuple2<Integer, String>> source = env.addSource(new TestSourceFunction(numElements))_				.broadcast()_				.filter(new OddEvenFilter())___		DataStream<Tuple2<IntWritable, Text>> mapped =  source.map(new MapFunction<Tuple2<Integer, String>, Tuple2<IntWritable, Text>>() {_			private static final long serialVersionUID = 1L___			@Override_			public Tuple2<IntWritable, Text> map(Tuple2<Integer, String> value) throws Exception {_				return Tuple2.of(new IntWritable(value.f0), new Text(value.f1))__			}_		})___		RollingSink<Tuple2<IntWritable, Text>> sink = new RollingSink<Tuple2<IntWritable, Text>>(outPath)_				.setWriter(new SequenceFileWriter<IntWritable, Text>("Default", SequenceFile.CompressionType.BLOCK))_				.setBucketer(new NonRollingBucketer())_				.setPartPrefix("part")_				.setPendingPrefix("")_				.setPendingSuffix("")___		mapped.addSink(sink)___		env.execute("RollingSink String Write Test")___		FSDataInputStream inStream = dfs.open(new Path(outPath + "/part-0-0"))___		SequenceFile.Reader reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		IntWritable intWritable = new IntWritable()__		Text txt = new Text()___		for (int i = 0_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()___		inStream = dfs.open(new Path(outPath + "/part-1-0"))___		reader = new SequenceFile.Reader(inStream,_				1000,_				0,_				100000,_				new Configuration())___		for (int i = 1_ i < numElements_ i += 2) {_			reader.next(intWritable, txt)__			Assert.assertEquals(i, intWritable.get())__			Assert.assertEquals("message #" + i, txt.toString())__		}__		reader.close()__		inStream.close()__	};this,tests,link,sequence,file,writer,with,non,rolling,output,but,with,compression;test,public,void,test,non,rolling,sequence,file,with,compression,writer,throws,exception,final,int,num,elements,20,final,string,out,path,hdfs,uri,seq,non,rolling,out,stream,execution,environment,env,stream,execution,environment,get,execution,environment,env,set,parallelism,2,data,stream,tuple2,integer,string,source,env,add,source,new,test,source,function,num,elements,broadcast,filter,new,odd,even,filter,data,stream,tuple2,int,writable,text,mapped,source,map,new,map,function,tuple2,integer,string,tuple2,int,writable,text,private,static,final,long,serial,version,uid,1l,override,public,tuple2,int,writable,text,map,tuple2,integer,string,value,throws,exception,return,tuple2,of,new,int,writable,value,f0,new,text,value,f1,rolling,sink,tuple2,int,writable,text,sink,new,rolling,sink,tuple2,int,writable,text,out,path,set,writer,new,sequence,file,writer,int,writable,text,default,sequence,file,compression,type,block,set,bucketer,new,non,rolling,bucketer,set,part,prefix,part,set,pending,prefix,set,pending,suffix,mapped,add,sink,sink,env,execute,rolling,sink,string,write,test,fsdata,input,stream,in,stream,dfs,open,new,path,out,path,part,0,0,sequence,file,reader,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,int,writable,int,writable,new,int,writable,text,txt,new,text,for,int,i,0,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close,in,stream,dfs,open,new,path,out,path,part,1,0,reader,new,sequence,file,reader,in,stream,1000,0,100000,new,configuration,for,int,i,1,i,num,elements,i,2,reader,next,int,writable,txt,assert,assert,equals,i,int,writable,get,assert,assert,equals,message,i,txt,to,string,reader,close,in,stream,close
