# id;timestamp;commentText;codeText;commentWords;codeWords
HCatInputFormatBase -> public HCatInputFormatBase<T> withFilter(String filter) throws IOException;1480685315;Specifies a SQL-like filter condition on the table's partition columns._Filter conditions on non-partition columns are invalid._A partition filter can significantly reduce the amount of data to be read.__@param filter A SQL-like filter condition on the table's partition columns._@return This InputFormat with specified partition filter._@throws java.io.IOException;public HCatInputFormatBase<T> withFilter(String filter) throws IOException {__		_		this.hCatInputFormat.setFilter(filter)___		return this__	};specifies,a,sql,like,filter,condition,on,the,table,s,partition,columns,filter,conditions,on,non,partition,columns,are,invalid,a,partition,filter,can,significantly,reduce,the,amount,of,data,to,be,read,param,filter,a,sql,like,filter,condition,on,the,table,s,partition,columns,return,this,input,format,with,specified,partition,filter,throws,java,io,ioexception;public,hcat,input,format,base,t,with,filter,string,filter,throws,ioexception,this,h,cat,input,format,set,filter,filter,return,this
HCatInputFormatBase -> public HCatInputFormatBase<T> withFilter(String filter) throws IOException;1495923070;Specifies a SQL-like filter condition on the table's partition columns._Filter conditions on non-partition columns are invalid._A partition filter can significantly reduce the amount of data to be read.__@param filter A SQL-like filter condition on the table's partition columns._@return This InputFormat with specified partition filter._@throws java.io.IOException;public HCatInputFormatBase<T> withFilter(String filter) throws IOException {__		_		this.hCatInputFormat.setFilter(filter)___		return this__	};specifies,a,sql,like,filter,condition,on,the,table,s,partition,columns,filter,conditions,on,non,partition,columns,are,invalid,a,partition,filter,can,significantly,reduce,the,amount,of,data,to,be,read,param,filter,a,sql,like,filter,condition,on,the,table,s,partition,columns,return,this,input,format,with,specified,partition,filter,throws,java,io,ioexception;public,hcat,input,format,base,t,with,filter,string,filter,throws,ioexception,this,h,cat,input,format,set,filter,filter,return,this
HCatInputFormatBase -> public HCatInputFormatBase<T> withFilter(String filter) throws IOException;1501258801;Specifies a SQL-like filter condition on the table's partition columns._Filter conditions on non-partition columns are invalid._A partition filter can significantly reduce the amount of data to be read.__@param filter A SQL-like filter condition on the table's partition columns._@return This InputFormat with specified partition filter._@throws java.io.IOException;public HCatInputFormatBase<T> withFilter(String filter) throws IOException {__		_		this.hCatInputFormat.setFilter(filter)___		return this__	};specifies,a,sql,like,filter,condition,on,the,table,s,partition,columns,filter,conditions,on,non,partition,columns,are,invalid,a,partition,filter,can,significantly,reduce,the,amount,of,data,to,be,read,param,filter,a,sql,like,filter,condition,on,the,table,s,partition,columns,return,this,input,format,with,specified,partition,filter,throws,java,io,ioexception;public,hcat,input,format,base,t,with,filter,string,filter,throws,ioexception,this,h,cat,input,format,set,filter,filter,return,this
HCatInputFormatBase -> public Configuration getConfiguration();1480685315;Returns the {@link org.apache.hadoop.conf.Configuration} of the HCatInputFormat.__@return The Configuration of the HCatInputFormat.;public Configuration getConfiguration() {_		return this.configuration__	};returns,the,link,org,apache,hadoop,conf,configuration,of,the,hcat,input,format,return,the,configuration,of,the,hcat,input,format;public,configuration,get,configuration,return,this,configuration
HCatInputFormatBase -> public Configuration getConfiguration();1495923070;Returns the {@link org.apache.hadoop.conf.Configuration} of the HCatInputFormat.__@return The Configuration of the HCatInputFormat.;public Configuration getConfiguration() {_		return this.configuration__	};returns,the,link,org,apache,hadoop,conf,configuration,of,the,hcat,input,format,return,the,configuration,of,the,hcat,input,format;public,configuration,get,configuration,return,this,configuration
HCatInputFormatBase -> public Configuration getConfiguration();1501258801;Returns the {@link org.apache.hadoop.conf.Configuration} of the HCatInputFormat.__@return The Configuration of the HCatInputFormat.;public Configuration getConfiguration() {_		return this.configuration__	};returns,the,link,org,apache,hadoop,conf,configuration,of,the,hcat,input,format,return,the,configuration,of,the,hcat,input,format;public,configuration,get,configuration,return,this,configuration
HCatInputFormatBase -> public HCatSchema getOutputSchema();1480685315;Returns the {@link org.apache.hive.hcatalog.data.schema.HCatSchema} of the {@link org.apache.hive.hcatalog.data.HCatRecord}_returned by this InputFormat.__@return The HCatSchema of the HCatRecords returned by this InputFormat.;public HCatSchema getOutputSchema() {_		return this.outputSchema__	};returns,the,link,org,apache,hive,hcatalog,data,schema,hcat,schema,of,the,link,org,apache,hive,hcatalog,data,hcat,record,returned,by,this,input,format,return,the,hcat,schema,of,the,hcat,records,returned,by,this,input,format;public,hcat,schema,get,output,schema,return,this,output,schema
HCatInputFormatBase -> public HCatSchema getOutputSchema();1495923070;Returns the {@link org.apache.hive.hcatalog.data.schema.HCatSchema} of the {@link org.apache.hive.hcatalog.data.HCatRecord}_returned by this InputFormat.__@return The HCatSchema of the HCatRecords returned by this InputFormat.;public HCatSchema getOutputSchema() {_		return this.outputSchema__	};returns,the,link,org,apache,hive,hcatalog,data,schema,hcat,schema,of,the,link,org,apache,hive,hcatalog,data,hcat,record,returned,by,this,input,format,return,the,hcat,schema,of,the,hcat,records,returned,by,this,input,format;public,hcat,schema,get,output,schema,return,this,output,schema
HCatInputFormatBase -> public HCatSchema getOutputSchema();1501258801;Returns the {@link org.apache.hive.hcatalog.data.schema.HCatSchema} of the {@link org.apache.hive.hcatalog.data.HCatRecord}_returned by this InputFormat.__@return The HCatSchema of the HCatRecords returned by this InputFormat.;public HCatSchema getOutputSchema() {_		return this.outputSchema__	};returns,the,link,org,apache,hive,hcatalog,data,schema,hcat,schema,of,the,link,org,apache,hive,hcatalog,data,hcat,record,returned,by,this,input,format,return,the,hcat,schema,of,the,hcat,records,returned,by,this,input,format;public,hcat,schema,get,output,schema,return,this,output,schema
HCatInputFormatBase -> public HCatInputFormatBase<T> asFlinkTuples() throws HCatException;1480685315;Specifies that the InputFormat returns Flink tuples instead of_{@link org.apache.hive.hcatalog.data.HCatRecord}.__Note: Flink tuples might only support a limited number of fields (depending on the API).__@return This InputFormat._@throws org.apache.hive.hcatalog.common.HCatException;public HCatInputFormatBase<T> asFlinkTuples() throws HCatException {__		_		int numFields = outputSchema.getFields().size()__		if(numFields > this.getMaxFlinkTupleSize()) {_			throw new IllegalArgumentException("Only up to "+this.getMaxFlinkTupleSize()+_					" fields can be returned as Flink tuples.")__		}__		TypeInformation[] fieldTypes = new TypeInformation[numFields]__		fieldNames = new String[numFields]__		for (String fieldName : outputSchema.getFieldNames()) {_			HCatFieldSchema field = outputSchema.get(fieldName)___			int fieldPos = outputSchema.getPosition(fieldName)__			TypeInformation fieldType = getFieldType(field)___			fieldTypes[fieldPos] = fieldType__			fieldNames[fieldPos] = fieldName___		}_		this.resultType = new TupleTypeInfo(fieldTypes)___		return this__	};specifies,that,the,input,format,returns,flink,tuples,instead,of,link,org,apache,hive,hcatalog,data,hcat,record,note,flink,tuples,might,only,support,a,limited,number,of,fields,depending,on,the,api,return,this,input,format,throws,org,apache,hive,hcatalog,common,hcat,exception;public,hcat,input,format,base,t,as,flink,tuples,throws,hcat,exception,int,num,fields,output,schema,get,fields,size,if,num,fields,this,get,max,flink,tuple,size,throw,new,illegal,argument,exception,only,up,to,this,get,max,flink,tuple,size,fields,can,be,returned,as,flink,tuples,type,information,field,types,new,type,information,num,fields,field,names,new,string,num,fields,for,string,field,name,output,schema,get,field,names,hcat,field,schema,field,output,schema,get,field,name,int,field,pos,output,schema,get,position,field,name,type,information,field,type,get,field,type,field,field,types,field,pos,field,type,field,names,field,pos,field,name,this,result,type,new,tuple,type,info,field,types,return,this
HCatInputFormatBase -> public HCatInputFormatBase<T> asFlinkTuples() throws HCatException;1495923070;Specifies that the InputFormat returns Flink tuples instead of_{@link org.apache.hive.hcatalog.data.HCatRecord}.__<p>Note: Flink tuples might only support a limited number of fields (depending on the API).__@return This InputFormat._@throws org.apache.hive.hcatalog.common.HCatException;public HCatInputFormatBase<T> asFlinkTuples() throws HCatException {__		_		int numFields = outputSchema.getFields().size()__		if (numFields > this.getMaxFlinkTupleSize()) {_			throw new IllegalArgumentException("Only up to " + this.getMaxFlinkTupleSize() +_					" fields can be returned as Flink tuples.")__		}__		TypeInformation[] fieldTypes = new TypeInformation[numFields]__		fieldNames = new String[numFields]__		for (String fieldName : outputSchema.getFieldNames()) {_			HCatFieldSchema field = outputSchema.get(fieldName)___			int fieldPos = outputSchema.getPosition(fieldName)__			TypeInformation fieldType = getFieldType(field)___			fieldTypes[fieldPos] = fieldType__			fieldNames[fieldPos] = fieldName___		}_		this.resultType = new TupleTypeInfo(fieldTypes)___		return this__	};specifies,that,the,input,format,returns,flink,tuples,instead,of,link,org,apache,hive,hcatalog,data,hcat,record,p,note,flink,tuples,might,only,support,a,limited,number,of,fields,depending,on,the,api,return,this,input,format,throws,org,apache,hive,hcatalog,common,hcat,exception;public,hcat,input,format,base,t,as,flink,tuples,throws,hcat,exception,int,num,fields,output,schema,get,fields,size,if,num,fields,this,get,max,flink,tuple,size,throw,new,illegal,argument,exception,only,up,to,this,get,max,flink,tuple,size,fields,can,be,returned,as,flink,tuples,type,information,field,types,new,type,information,num,fields,field,names,new,string,num,fields,for,string,field,name,output,schema,get,field,names,hcat,field,schema,field,output,schema,get,field,name,int,field,pos,output,schema,get,position,field,name,type,information,field,type,get,field,type,field,field,types,field,pos,field,type,field,names,field,pos,field,name,this,result,type,new,tuple,type,info,field,types,return,this
HCatInputFormatBase -> public HCatInputFormatBase<T> asFlinkTuples() throws HCatException;1501258801;Specifies that the InputFormat returns Flink tuples instead of_{@link org.apache.hive.hcatalog.data.HCatRecord}.__<p>Note: Flink tuples might only support a limited number of fields (depending on the API).__@return This InputFormat._@throws org.apache.hive.hcatalog.common.HCatException;public HCatInputFormatBase<T> asFlinkTuples() throws HCatException {__		_		int numFields = outputSchema.getFields().size()__		if (numFields > this.getMaxFlinkTupleSize()) {_			throw new IllegalArgumentException("Only up to " + this.getMaxFlinkTupleSize() +_					" fields can be returned as Flink tuples.")__		}__		TypeInformation[] fieldTypes = new TypeInformation[numFields]__		fieldNames = new String[numFields]__		for (String fieldName : outputSchema.getFieldNames()) {_			HCatFieldSchema field = outputSchema.get(fieldName)___			int fieldPos = outputSchema.getPosition(fieldName)__			TypeInformation fieldType = getFieldType(field)___			fieldTypes[fieldPos] = fieldType__			fieldNames[fieldPos] = fieldName___		}_		this.resultType = new TupleTypeInfo(fieldTypes)___		return this__	};specifies,that,the,input,format,returns,flink,tuples,instead,of,link,org,apache,hive,hcatalog,data,hcat,record,p,note,flink,tuples,might,only,support,a,limited,number,of,fields,depending,on,the,api,return,this,input,format,throws,org,apache,hive,hcatalog,common,hcat,exception;public,hcat,input,format,base,t,as,flink,tuples,throws,hcat,exception,int,num,fields,output,schema,get,fields,size,if,num,fields,this,get,max,flink,tuple,size,throw,new,illegal,argument,exception,only,up,to,this,get,max,flink,tuple,size,fields,can,be,returned,as,flink,tuples,type,information,field,types,new,type,information,num,fields,field,names,new,string,num,fields,for,string,field,name,output,schema,get,field,names,hcat,field,schema,field,output,schema,get,field,name,int,field,pos,output,schema,get,position,field,name,type,information,field,type,get,field,type,field,field,types,field,pos,field,type,field,names,field,pos,field,name,this,result,type,new,tuple,type,info,field,types,return,this
HCatInputFormatBase -> public HCatInputFormatBase(String database, String table, Configuration config) throws IOException;1480685315;Creates a HCatInputFormat for the given database, table, and_{@link org.apache.hadoop.conf.Configuration}._By default, the InputFormat returns {@link org.apache.hive.hcatalog.data.HCatRecord}._The return type of the InputFormat can be changed to Flink-native tuples by calling_{@link HCatInputFormatBase#asFlinkTuples()}.__@param database The name of the database to read from._@param table The name of the table to read._@param config The Configuration for the InputFormat._@throws java.io.IOException;public HCatInputFormatBase(String database, String table, Configuration config) throws IOException {_		super()__		this.configuration = config__		HadoopUtils.mergeHadoopConf(this.configuration)___		this.hCatInputFormat = org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(this.configuration, database, table)__		this.outputSchema = org.apache.hive.hcatalog.mapreduce.HCatInputFormat.getTableSchema(this.configuration)___		_		configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema))__		_		this.resultType = new WritableTypeInfo(DefaultHCatRecord.class)__	};creates,a,hcat,input,format,for,the,given,database,table,and,link,org,apache,hadoop,conf,configuration,by,default,the,input,format,returns,link,org,apache,hive,hcatalog,data,hcat,record,the,return,type,of,the,input,format,can,be,changed,to,flink,native,tuples,by,calling,link,hcat,input,format,base,as,flink,tuples,param,database,the,name,of,the,database,to,read,from,param,table,the,name,of,the,table,to,read,param,config,the,configuration,for,the,input,format,throws,java,io,ioexception;public,hcat,input,format,base,string,database,string,table,configuration,config,throws,ioexception,super,this,configuration,config,hadoop,utils,merge,hadoop,conf,this,configuration,this,h,cat,input,format,org,apache,hive,hcatalog,mapreduce,hcat,input,format,set,input,this,configuration,database,table,this,output,schema,org,apache,hive,hcatalog,mapreduce,hcat,input,format,get,table,schema,this,configuration,configuration,set,mapreduce,lib,hcat,output,schema,hcat,util,serialize,output,schema,this,result,type,new,writable,type,info,default,hcat,record,class
HCatInputFormatBase -> public HCatInputFormatBase(String database, String table, Configuration config) throws IOException;1495923070;Creates a HCatInputFormat for the given database, table, and_{@link org.apache.hadoop.conf.Configuration}._By default, the InputFormat returns {@link org.apache.hive.hcatalog.data.HCatRecord}._The return type of the InputFormat can be changed to Flink-native tuples by calling_{@link HCatInputFormatBase#asFlinkTuples()}.__@param database The name of the database to read from._@param table The name of the table to read._@param config The Configuration for the InputFormat._@throws java.io.IOException;public HCatInputFormatBase(String database, String table, Configuration config) throws IOException {_		super()__		this.configuration = config__		HadoopUtils.mergeHadoopConf(this.configuration)___		this.hCatInputFormat = org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(this.configuration, database, table)__		this.outputSchema = org.apache.hive.hcatalog.mapreduce.HCatInputFormat.getTableSchema(this.configuration)___		_		configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema))__		_		this.resultType = new WritableTypeInfo(DefaultHCatRecord.class)__	};creates,a,hcat,input,format,for,the,given,database,table,and,link,org,apache,hadoop,conf,configuration,by,default,the,input,format,returns,link,org,apache,hive,hcatalog,data,hcat,record,the,return,type,of,the,input,format,can,be,changed,to,flink,native,tuples,by,calling,link,hcat,input,format,base,as,flink,tuples,param,database,the,name,of,the,database,to,read,from,param,table,the,name,of,the,table,to,read,param,config,the,configuration,for,the,input,format,throws,java,io,ioexception;public,hcat,input,format,base,string,database,string,table,configuration,config,throws,ioexception,super,this,configuration,config,hadoop,utils,merge,hadoop,conf,this,configuration,this,h,cat,input,format,org,apache,hive,hcatalog,mapreduce,hcat,input,format,set,input,this,configuration,database,table,this,output,schema,org,apache,hive,hcatalog,mapreduce,hcat,input,format,get,table,schema,this,configuration,configuration,set,mapreduce,lib,hcat,output,schema,hcat,util,serialize,output,schema,this,result,type,new,writable,type,info,default,hcat,record,class
HCatInputFormatBase -> public HCatInputFormatBase(String database, String table, Configuration config) throws IOException;1501258801;Creates a HCatInputFormat for the given database, table, and_{@link org.apache.hadoop.conf.Configuration}._By default, the InputFormat returns {@link org.apache.hive.hcatalog.data.HCatRecord}._The return type of the InputFormat can be changed to Flink-native tuples by calling_{@link HCatInputFormatBase#asFlinkTuples()}.__@param database The name of the database to read from._@param table The name of the table to read._@param config The Configuration for the InputFormat._@throws java.io.IOException;public HCatInputFormatBase(String database, String table, Configuration config) throws IOException {_		super()__		this.configuration = config__		HadoopUtils.mergeHadoopConf(this.configuration)___		this.hCatInputFormat = org.apache.hive.hcatalog.mapreduce.HCatInputFormat.setInput(this.configuration, database, table)__		this.outputSchema = org.apache.hive.hcatalog.mapreduce.HCatInputFormat.getTableSchema(this.configuration)___		_		configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema))__		_		this.resultType = new WritableTypeInfo(DefaultHCatRecord.class)__	};creates,a,hcat,input,format,for,the,given,database,table,and,link,org,apache,hadoop,conf,configuration,by,default,the,input,format,returns,link,org,apache,hive,hcatalog,data,hcat,record,the,return,type,of,the,input,format,can,be,changed,to,flink,native,tuples,by,calling,link,hcat,input,format,base,as,flink,tuples,param,database,the,name,of,the,database,to,read,from,param,table,the,name,of,the,table,to,read,param,config,the,configuration,for,the,input,format,throws,java,io,ioexception;public,hcat,input,format,base,string,database,string,table,configuration,config,throws,ioexception,super,this,configuration,config,hadoop,utils,merge,hadoop,conf,this,configuration,this,h,cat,input,format,org,apache,hive,hcatalog,mapreduce,hcat,input,format,set,input,this,configuration,database,table,this,output,schema,org,apache,hive,hcatalog,mapreduce,hcat,input,format,get,table,schema,this,configuration,configuration,set,mapreduce,lib,hcat,output,schema,hcat,util,serialize,output,schema,this,result,type,new,writable,type,info,default,hcat,record,class
HCatInputFormatBase -> public HCatInputFormatBase(String database, String table) throws IOException;1480685315;Creates a HCatInputFormat for the given database and table._By default, the InputFormat returns {@link org.apache.hive.hcatalog.data.HCatRecord}._The return type of the InputFormat can be changed to Flink-native tuples by calling_{@link HCatInputFormatBase#asFlinkTuples()}.__@param database The name of the database to read from._@param table The name of the table to read._@throws java.io.IOException;public HCatInputFormatBase(String database, String table) throws IOException {_		this(database, table, new Configuration())__	};creates,a,hcat,input,format,for,the,given,database,and,table,by,default,the,input,format,returns,link,org,apache,hive,hcatalog,data,hcat,record,the,return,type,of,the,input,format,can,be,changed,to,flink,native,tuples,by,calling,link,hcat,input,format,base,as,flink,tuples,param,database,the,name,of,the,database,to,read,from,param,table,the,name,of,the,table,to,read,throws,java,io,ioexception;public,hcat,input,format,base,string,database,string,table,throws,ioexception,this,database,table,new,configuration
HCatInputFormatBase -> public HCatInputFormatBase(String database, String table) throws IOException;1495923070;Creates a HCatInputFormat for the given database and table._By default, the InputFormat returns {@link org.apache.hive.hcatalog.data.HCatRecord}._The return type of the InputFormat can be changed to Flink-native tuples by calling_{@link HCatInputFormatBase#asFlinkTuples()}.__@param database The name of the database to read from._@param table The name of the table to read._@throws java.io.IOException;public HCatInputFormatBase(String database, String table) throws IOException {_		this(database, table, new Configuration())__	};creates,a,hcat,input,format,for,the,given,database,and,table,by,default,the,input,format,returns,link,org,apache,hive,hcatalog,data,hcat,record,the,return,type,of,the,input,format,can,be,changed,to,flink,native,tuples,by,calling,link,hcat,input,format,base,as,flink,tuples,param,database,the,name,of,the,database,to,read,from,param,table,the,name,of,the,table,to,read,throws,java,io,ioexception;public,hcat,input,format,base,string,database,string,table,throws,ioexception,this,database,table,new,configuration
HCatInputFormatBase -> public HCatInputFormatBase(String database, String table) throws IOException;1501258801;Creates a HCatInputFormat for the given database and table._By default, the InputFormat returns {@link org.apache.hive.hcatalog.data.HCatRecord}._The return type of the InputFormat can be changed to Flink-native tuples by calling_{@link HCatInputFormatBase#asFlinkTuples()}.__@param database The name of the database to read from._@param table The name of the table to read._@throws java.io.IOException;public HCatInputFormatBase(String database, String table) throws IOException {_		this(database, table, new Configuration())__	};creates,a,hcat,input,format,for,the,given,database,and,table,by,default,the,input,format,returns,link,org,apache,hive,hcatalog,data,hcat,record,the,return,type,of,the,input,format,can,be,changed,to,flink,native,tuples,by,calling,link,hcat,input,format,base,as,flink,tuples,param,database,the,name,of,the,database,to,read,from,param,table,the,name,of,the,table,to,read,throws,java,io,ioexception;public,hcat,input,format,base,string,database,string,table,throws,ioexception,this,database,table,new,configuration
HCatInputFormatBase -> public HCatInputFormatBase<T> getFields(String... fields) throws IOException;1480685315;Specifies the fields which are returned by the InputFormat and their order.__@param fields The fields and their order which are returned by the InputFormat._@return This InputFormat with specified return fields._@throws java.io.IOException;public HCatInputFormatBase<T> getFields(String... fields) throws IOException {__		_		ArrayList<HCatFieldSchema> fieldSchemas = new ArrayList<HCatFieldSchema>(fields.length)__		for(String field : fields) {_			fieldSchemas.add(this.outputSchema.get(field))__		}_		this.outputSchema = new HCatSchema(fieldSchemas)___		_		configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema))___		return this__	};specifies,the,fields,which,are,returned,by,the,input,format,and,their,order,param,fields,the,fields,and,their,order,which,are,returned,by,the,input,format,return,this,input,format,with,specified,return,fields,throws,java,io,ioexception;public,hcat,input,format,base,t,get,fields,string,fields,throws,ioexception,array,list,hcat,field,schema,field,schemas,new,array,list,hcat,field,schema,fields,length,for,string,field,fields,field,schemas,add,this,output,schema,get,field,this,output,schema,new,hcat,schema,field,schemas,configuration,set,mapreduce,lib,hcat,output,schema,hcat,util,serialize,output,schema,return,this
HCatInputFormatBase -> public HCatInputFormatBase<T> getFields(String... fields) throws IOException;1495923070;Specifies the fields which are returned by the InputFormat and their order.__@param fields The fields and their order which are returned by the InputFormat._@return This InputFormat with specified return fields._@throws java.io.IOException;public HCatInputFormatBase<T> getFields(String... fields) throws IOException {__		_		ArrayList<HCatFieldSchema> fieldSchemas = new ArrayList<HCatFieldSchema>(fields.length)__		for (String field : fields) {_			fieldSchemas.add(this.outputSchema.get(field))__		}_		this.outputSchema = new HCatSchema(fieldSchemas)___		_		configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema))___		return this__	};specifies,the,fields,which,are,returned,by,the,input,format,and,their,order,param,fields,the,fields,and,their,order,which,are,returned,by,the,input,format,return,this,input,format,with,specified,return,fields,throws,java,io,ioexception;public,hcat,input,format,base,t,get,fields,string,fields,throws,ioexception,array,list,hcat,field,schema,field,schemas,new,array,list,hcat,field,schema,fields,length,for,string,field,fields,field,schemas,add,this,output,schema,get,field,this,output,schema,new,hcat,schema,field,schemas,configuration,set,mapreduce,lib,hcat,output,schema,hcat,util,serialize,output,schema,return,this
HCatInputFormatBase -> public HCatInputFormatBase<T> getFields(String... fields) throws IOException;1501258801;Specifies the fields which are returned by the InputFormat and their order.__@param fields The fields and their order which are returned by the InputFormat._@return This InputFormat with specified return fields._@throws java.io.IOException;public HCatInputFormatBase<T> getFields(String... fields) throws IOException {__		_		ArrayList<HCatFieldSchema> fieldSchemas = new ArrayList<HCatFieldSchema>(fields.length)__		for (String field : fields) {_			fieldSchemas.add(this.outputSchema.get(field))__		}_		this.outputSchema = new HCatSchema(fieldSchemas)___		_		configuration.set("mapreduce.lib.hcat.output.schema", HCatUtil.serialize(outputSchema))___		return this__	};specifies,the,fields,which,are,returned,by,the,input,format,and,their,order,param,fields,the,fields,and,their,order,which,are,returned,by,the,input,format,return,this,input,format,with,specified,return,fields,throws,java,io,ioexception;public,hcat,input,format,base,t,get,fields,string,fields,throws,ioexception,array,list,hcat,field,schema,field,schemas,new,array,list,hcat,field,schema,fields,length,for,string,field,fields,field,schemas,add,this,output,schema,get,field,this,output,schema,new,hcat,schema,field,schemas,configuration,set,mapreduce,lib,hcat,output,schema,hcat,util,serialize,output,schema,return,this
