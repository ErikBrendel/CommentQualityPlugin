commented;modifiers;parameterAmount;loc;comment;code
true;public;0;8;/**  * Returns a copy of this configuration  */ ;/**  * Returns a copy of this configuration  */ public KafkaConfiguration copy() {     try {         KafkaConfiguration copy = (KafkaConfiguration) clone().         return copy.     } catch (CloneNotSupportedException e) {         throw new RuntimeCamelException(e).     } }
false;public;0;60;;public Properties createProducerProperties() {     Properties props = new Properties().     addPropertyIfNotNull(props, ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, getKeySerializerClass()).     addPropertyIfNotNull(props, ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, getSerializerClass()).     addPropertyIfNotNull(props, ProducerConfig.ACKS_CONFIG, getRequestRequiredAcks()).     addPropertyIfNotNull(props, ProducerConfig.BUFFER_MEMORY_CONFIG, getBufferMemorySize()).     addPropertyIfNotNull(props, ProducerConfig.COMPRESSION_TYPE_CONFIG, getCompressionCodec()).     addPropertyIfNotNull(props, ProducerConfig.RETRIES_CONFIG, getRetries()).     addPropertyIfNotNull(props, ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, getInterceptorClasses()).     addPropertyIfNotNull(props, ProducerConfig.SEND_BUFFER_CONFIG, getRetries()).     addPropertyIfNotNull(props, ProducerConfig.BATCH_SIZE_CONFIG, getProducerBatchSize()).     addPropertyIfNotNull(props, ProducerConfig.CLIENT_ID_CONFIG, getClientId()).     addPropertyIfNotNull(props, ProducerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG, getConnectionMaxIdleMs()).     addPropertyIfNotNull(props, ProducerConfig.LINGER_MS_CONFIG, getLingerMs()).     addPropertyIfNotNull(props, ProducerConfig.MAX_BLOCK_MS_CONFIG, getMaxBlockMs()).     addPropertyIfNotNull(props, ProducerConfig.MAX_REQUEST_SIZE_CONFIG, getMaxRequestSize()).     addPropertyIfNotNull(props, ProducerConfig.PARTITIONER_CLASS_CONFIG, getPartitioner()).     addPropertyIfNotNull(props, ProducerConfig.RECEIVE_BUFFER_CONFIG, getReceiveBufferBytes()).     addPropertyIfNotNull(props, ProducerConfig.REQUEST_TIMEOUT_MS_CONFIG, getRequestTimeoutMs()).     addPropertyIfNotNull(props, ProducerConfig.SEND_BUFFER_CONFIG, getSendBufferBytes()).     addPropertyIfNotNull(props, ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, getMaxInFlightRequest()).     addPropertyIfNotNull(props, ProducerConfig.METADATA_MAX_AGE_CONFIG, getMetadataMaxAgeMs()).     addPropertyIfNotNull(props, ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG, getMetricReporters()).     addPropertyIfNotNull(props, ProducerConfig.METRICS_NUM_SAMPLES_CONFIG, getNoOfMetricsSample()).     addPropertyIfNotNull(props, ProducerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG, getMetricsSampleWindowMs()).     addPropertyIfNotNull(props, ProducerConfig.RECONNECT_BACKOFF_MS_CONFIG, getReconnectBackoffMs()).     addPropertyIfNotNull(props, ProducerConfig.RETRY_BACKOFF_MS_CONFIG, getRetryBackoffMs()).     addPropertyIfNotNull(props, ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, isEnableIdempotence()).     addPropertyIfNotNull(props, ProducerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG, getReconnectBackoffMaxMs()).     addPropertyIfNotNull(props, "schema.registry.url", getSchemaRegistryURL()).     // SSL     applySslConfiguration(props, getSslContextParameters()).     addPropertyIfNotNull(props, CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, getSecurityProtocol()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEY_PASSWORD_CONFIG, getSslKeyPassword()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, getSslKeystoreLocation()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, getSslKeystorePassword()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, getSslTruststoreLocation()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, getSslTruststorePassword()).     addPropertyIfNotNull(props, SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, getSslEnabledProtocols()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, getSslKeystoreType()).     addPropertyIfNotNull(props, SslConfigs.SSL_PROTOCOL_CONFIG, getSslProtocol()).     addPropertyIfNotNull(props, SslConfigs.SSL_PROVIDER_CONFIG, getSslProvider()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, getSslTruststoreType()).     addPropertyIfNotNull(props, SslConfigs.SSL_CIPHER_SUITES_CONFIG, getSslCipherSuites()).     addPropertyIfNotNull(props, SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, getSslEndpointAlgorithm()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG, getSslKeymanagerAlgorithm()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG, getSslTrustmanagerAlgorithm()).     // SASL     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_SERVICE_NAME, getSaslKerberosServiceName()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_KINIT_CMD, getKerberosInitCmd()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN, getKerberosBeforeReloginMinTime()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER, getKerberosRenewJitter()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR, getKerberosRenewWindowFactor()).     addListPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_PRINCIPAL_TO_LOCAL_RULES, getKerberosPrincipalToLocalRules()).     addPropertyIfNotNull(props, SaslConfigs.SASL_MECHANISM, getSaslMechanism()).     addPropertyIfNotNull(props, SaslConfigs.SASL_JAAS_CONFIG, getSaslJaasConfig()).     return props. }
false;public;0;61;;public Properties createConsumerProperties() {     Properties props = new Properties().     addPropertyIfNotNull(props, ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, getKeyDeserializer()).     addPropertyIfNotNull(props, ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, getValueDeserializer()).     addPropertyIfNotNull(props, ConsumerConfig.FETCH_MIN_BYTES_CONFIG, getFetchMinBytes()).     addPropertyIfNotNull(props, ConsumerConfig.FETCH_MAX_BYTES_CONFIG, getFetchMaxBytes()).     addPropertyIfNotNull(props, ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, getHeartbeatIntervalMs()).     addPropertyIfNotNull(props, ConsumerConfig.MAX_PARTITION_FETCH_BYTES_CONFIG, getMaxPartitionFetchBytes()).     addPropertyIfNotNull(props, ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, getSessionTimeoutMs()).     addPropertyIfNotNull(props, ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, getMaxPollIntervalMs()).     addPropertyIfNotNull(props, ConsumerConfig.MAX_POLL_RECORDS_CONFIG, getMaxPollRecords()).     addPropertyIfNotNull(props, ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, getInterceptorClasses()).     addPropertyIfNotNull(props, ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, getAutoOffsetReset()).     addPropertyIfNotNull(props, ConsumerConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG, getConnectionMaxIdleMs()).     addPropertyIfNotNull(props, ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, isAutoCommitEnable()).     addPropertyIfNotNull(props, ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, getPartitionAssignor()).     addPropertyIfNotNull(props, ConsumerConfig.RECEIVE_BUFFER_CONFIG, getReceiveBufferBytes()).     addPropertyIfNotNull(props, ConsumerConfig.REQUEST_TIMEOUT_MS_CONFIG, getConsumerRequestTimeoutMs()).     addPropertyIfNotNull(props, ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, getAutoCommitIntervalMs()).     addPropertyIfNotNull(props, ConsumerConfig.CHECK_CRCS_CONFIG, getCheckCrcs()).     addPropertyIfNotNull(props, ConsumerConfig.CLIENT_ID_CONFIG, getClientId()).     addPropertyIfNotNull(props, ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, getFetchWaitMaxMs()).     addPropertyIfNotNull(props, ConsumerConfig.METADATA_MAX_AGE_CONFIG, getMetadataMaxAgeMs()).     addPropertyIfNotNull(props, ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, getMetricReporters()).     addPropertyIfNotNull(props, ConsumerConfig.METRICS_NUM_SAMPLES_CONFIG, getNoOfMetricsSample()).     addPropertyIfNotNull(props, ConsumerConfig.METRICS_SAMPLE_WINDOW_MS_CONFIG, getMetricsSampleWindowMs()).     addPropertyIfNotNull(props, ConsumerConfig.RECONNECT_BACKOFF_MS_CONFIG, getReconnectBackoffMs()).     addPropertyIfNotNull(props, ConsumerConfig.RETRY_BACKOFF_MS_CONFIG, getRetryBackoffMs()).     addPropertyIfNotNull(props, ConsumerConfig.RECONNECT_BACKOFF_MAX_MS_CONFIG, getReconnectBackoffMaxMs()).     addPropertyIfNotNull(props, "schema.registry.url", getSchemaRegistryURL()).     // SSL     applySslConfiguration(props, getSslContextParameters()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEY_PASSWORD_CONFIG, getSslKeyPassword()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, getSslKeystoreLocation()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, getSslKeystorePassword()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, getSslTruststoreLocation()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_PASSWORD_CONFIG, getSslTruststorePassword()).     addPropertyIfNotNull(props, SslConfigs.SSL_CIPHER_SUITES_CONFIG, getSslCipherSuites()).     addPropertyIfNotNull(props, SslConfigs.SSL_ENDPOINT_IDENTIFICATION_ALGORITHM_CONFIG, getSslEndpointAlgorithm()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG, getSslKeymanagerAlgorithm()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG, getSslTrustmanagerAlgorithm()).     addPropertyIfNotNull(props, SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, getSslEnabledProtocols()).     addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, getSslKeystoreType()).     addPropertyIfNotNull(props, SslConfigs.SSL_PROTOCOL_CONFIG, getSslProtocol()).     addPropertyIfNotNull(props, SslConfigs.SSL_PROVIDER_CONFIG, getSslProvider()).     addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, getSslTruststoreType()).     // Security protocol     addPropertyIfNotNull(props, CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, getSecurityProtocol()).     addPropertyIfNotNull(props, ProducerConfig.SEND_BUFFER_CONFIG, getSendBufferBytes()).     // SASL     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_SERVICE_NAME, getSaslKerberosServiceName()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_KINIT_CMD, getKerberosInitCmd()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_MIN_TIME_BEFORE_RELOGIN, getKerberosBeforeReloginMinTime()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_JITTER, getKerberosRenewJitter()).     addPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_TICKET_RENEW_WINDOW_FACTOR, getKerberosRenewWindowFactor()).     addListPropertyIfNotNull(props, SaslConfigs.SASL_KERBEROS_PRINCIPAL_TO_LOCAL_RULES, getKerberosPrincipalToLocalRules()).     addPropertyIfNotNull(props, SaslConfigs.SASL_MECHANISM, getSaslMechanism()).     addPropertyIfNotNull(props, SaslConfigs.SASL_JAAS_CONFIG, getSaslJaasConfig()).     return props. }
true;private;2;42;/**  * Uses the standard camel {@link SSLContextParameters} object to fill the  * Kafka SSL properties  *  * @param props Kafka properties  * @param sslContextParameters SSL configuration  */ ;/**  * Uses the standard camel {@link SSLContextParameters} object to fill the  * Kafka SSL properties  *  * @param props Kafka properties  * @param sslContextParameters SSL configuration  */ private void applySslConfiguration(Properties props, SSLContextParameters sslContextParameters) {     if (sslContextParameters != null) {         addPropertyIfNotNull(props, SslConfigs.SSL_PROTOCOL_CONFIG, sslContextParameters.getSecureSocketProtocol()).         addPropertyIfNotNull(props, SslConfigs.SSL_PROVIDER_CONFIG, sslContextParameters.getProvider()).         CipherSuitesParameters cipherSuites = sslContextParameters.getCipherSuites().         if (cipherSuites != null) {             addCommaSeparatedList(props, SslConfigs.SSL_CIPHER_SUITES_CONFIG, cipherSuites.getCipherSuite()).         }         SecureSocketProtocolsParameters secureSocketProtocols = sslContextParameters.getSecureSocketProtocols().         if (secureSocketProtocols != null) {             addCommaSeparatedList(props, SslConfigs.SSL_ENABLED_PROTOCOLS_CONFIG, secureSocketProtocols.getSecureSocketProtocol()).         }         KeyManagersParameters keyManagers = sslContextParameters.getKeyManagers().         if (keyManagers != null) {             addPropertyIfNotNull(props, SslConfigs.SSL_KEYMANAGER_ALGORITHM_CONFIG, keyManagers.getAlgorithm()).             addPropertyIfNotNull(props, SslConfigs.SSL_KEY_PASSWORD_CONFIG, keyManagers.getKeyPassword()).             KeyStoreParameters keyStore = keyManagers.getKeyStore().             if (keyStore != null) {                 addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_TYPE_CONFIG, keyStore.getType()).                 addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_LOCATION_CONFIG, keyStore.getResource()).                 addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, keyStore.getPassword()).             }         }         TrustManagersParameters trustManagers = sslContextParameters.getTrustManagers().         if (trustManagers != null) {             addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTMANAGER_ALGORITHM_CONFIG, trustManagers.getAlgorithm()).             KeyStoreParameters keyStore = trustManagers.getKeyStore().             if (keyStore != null) {                 addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_TYPE_CONFIG, keyStore.getType()).                 addPropertyIfNotNull(props, SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG, keyStore.getResource()).                 addPropertyIfNotNull(props, SslConfigs.SSL_KEYSTORE_PASSWORD_CONFIG, keyStore.getPassword()).             }         }     } }
false;private,static;3;6;;private static <T> void addPropertyIfNotNull(Properties props, String key, T value) {     if (value != null) {         // Kafka expects all properties as String         props.put(key, value.toString()).     } }
false;private,static;3;8;;private static <T> void addListPropertyIfNotNull(Properties props, String key, T value) {     if (value != null) {         // Kafka expects all properties as String         String[] values = value.toString().split(",").         List<String> list = Arrays.asList(values).         props.put(key, list).     } }
false;private,static;3;5;;private static void addCommaSeparatedList(Properties props, String key, List<String> values) {     if (values != null && !values.isEmpty()) {         props.put(key, values.stream().collect(Collectors.joining(","))).     } }
false;public;0;3;;public boolean isTopicIsPattern() {     return topicIsPattern. }
true;public;1;3;/**  * Whether the topic is a pattern (regular expression). This can be used to  * subscribe to dynamic number of topics matching the pattern.  */ ;/**  * Whether the topic is a pattern (regular expression). This can be used to  * subscribe to dynamic number of topics matching the pattern.  */ public void setTopicIsPattern(boolean topicIsPattern) {     this.topicIsPattern = topicIsPattern. }
false;public;0;3;;public String getGroupId() {     return groupId. }
true;public;1;3;/**  * A string that uniquely identifies the group of consumer processes to  * which this consumer belongs. By setting the same group id multiple  * processes indicate that they are all part of the same consumer group.  * This option is required for consumers.  */ ;/**  * A string that uniquely identifies the group of consumer processes to  * which this consumer belongs. By setting the same group id multiple  * processes indicate that they are all part of the same consumer group.  * This option is required for consumers.  */ public void setGroupId(String groupId) {     this.groupId = groupId. }
false;public;0;3;;public boolean isBridgeEndpoint() {     return bridgeEndpoint. }
true;public;1;3;/**  * If the option is true, then KafkaProducer will ignore the  * KafkaConstants.TOPIC header setting of the inbound message.  */ ;/**  * If the option is true, then KafkaProducer will ignore the  * KafkaConstants.TOPIC header setting of the inbound message.  */ public void setBridgeEndpoint(boolean bridgeEndpoint) {     this.bridgeEndpoint = bridgeEndpoint. }
false;public;0;3;;public boolean isCircularTopicDetection() {     return circularTopicDetection. }
true;public;1;3;/**  * If the option is true, then KafkaProducer will detect if the message is  * attempted to be sent back to the same topic it may come from, if the  * message was original from a kafka consumer. If the KafkaConstants.TOPIC  * header is the same as the original kafka consumer topic, then the header  * setting is ignored, and the topic of the producer endpoint is used. In  * other words this avoids sending the same message back to where it came  * from. This option is not in use if the option bridgeEndpoint is set to  * true.  */ ;/**  * If the option is true, then KafkaProducer will detect if the message is  * attempted to be sent back to the same topic it may come from, if the  * message was original from a kafka consumer. If the KafkaConstants.TOPIC  * header is the same as the original kafka consumer topic, then the header  * setting is ignored, and the topic of the producer endpoint is used. In  * other words this avoids sending the same message back to where it came  * from. This option is not in use if the option bridgeEndpoint is set to  * true.  */ public void setCircularTopicDetection(boolean circularTopicDetection) {     this.circularTopicDetection = circularTopicDetection. }
false;public;0;3;;public String getPartitioner() {     return partitioner. }
true;public;1;3;/**  * The partitioner class for partitioning messages amongst sub-topics. The  * default partitioner is based on the hash of the key.  */ ;/**  * The partitioner class for partitioning messages amongst sub-topics. The  * default partitioner is based on the hash of the key.  */ public void setPartitioner(String partitioner) {     this.partitioner = partitioner. }
false;public;0;3;;public String getTopic() {     return topic. }
true;public;1;3;/**  * Name of the topic to use. On the consumer you can use comma to separate  * multiple topics. A producer can only send a message to a single topic.  */ ;/**  * Name of the topic to use. On the consumer you can use comma to separate  * multiple topics. A producer can only send a message to a single topic.  */ public void setTopic(String topic) {     this.topic = topic. }
false;public;0;3;;public int getConsumerStreams() {     return consumerStreams. }
true;public;1;3;/**  * Number of concurrent consumers on the consumer  */ ;/**  * Number of concurrent consumers on the consumer  */ public void setConsumerStreams(int consumerStreams) {     this.consumerStreams = consumerStreams. }
false;public;0;3;;public int getConsumersCount() {     return consumersCount. }
true;public;1;3;/**  * The number of consumers that connect to kafka server  */ ;/**  * The number of consumers that connect to kafka server  */ public void setConsumersCount(int consumersCount) {     this.consumersCount = consumersCount. }
false;public;0;3;;public String getClientId() {     return clientId. }
true;public;1;3;/**  * The client id is a user-specified string sent in each request to help  * trace calls. It should logically identify the application making the  * request.  */ ;/**  * The client id is a user-specified string sent in each request to help  * trace calls. It should logically identify the application making the  * request.  */ public void setClientId(String clientId) {     this.clientId = clientId. }
false;public;0;3;;public Boolean isAutoCommitEnable() {     return offsetRepository == null ? autoCommitEnable : false. }
true;public;1;3;/**  * If true, periodically commit to ZooKeeper the offset of messages already  * fetched by the consumer. This committed offset will be used when the  * process fails as the position from which the new consumer will begin.  */ ;/**  * If true, periodically commit to ZooKeeper the offset of messages already  * fetched by the consumer. This committed offset will be used when the  * process fails as the position from which the new consumer will begin.  */ public void setAutoCommitEnable(Boolean autoCommitEnable) {     this.autoCommitEnable = autoCommitEnable. }
false;public;0;3;;public boolean isAllowManualCommit() {     return allowManualCommit. }
true;public;1;3;/**  * Whether to allow doing manual commits via {@link KafkaManualCommit}.  * <p/>  * If this option is enabled then an instance of {@link KafkaManualCommit}  * is stored on the {@link Exchange} message header, which allows end users  * to access this API and perform manual offset commits via the Kafka  * consumer.  */ ;/**  * Whether to allow doing manual commits via {@link KafkaManualCommit}.  * <p/>  * If this option is enabled then an instance of {@link KafkaManualCommit}  * is stored on the {@link Exchange} message header, which allows end users  * to access this API and perform manual offset commits via the Kafka  * consumer.  */ public void setAllowManualCommit(boolean allowManualCommit) {     this.allowManualCommit = allowManualCommit. }
false;public;0;3;;public StateRepository<String, String> getOffsetRepository() {     return offsetRepository. }
true;public;1;3;/**  * The offset repository to use in order to locally store the offset of each  * partition of the topic. Defining one will disable the autocommit.  */ ;/**  * The offset repository to use in order to locally store the offset of each  * partition of the topic. Defining one will disable the autocommit.  */ public void setOffsetRepository(StateRepository<String, String> offsetRepository) {     this.offsetRepository = offsetRepository. }
false;public;0;3;;public Integer getAutoCommitIntervalMs() {     return autoCommitIntervalMs. }
true;public;1;3;/**  * The frequency in ms that the consumer offsets are committed to zookeeper.  */ ;/**  * The frequency in ms that the consumer offsets are committed to zookeeper.  */ public void setAutoCommitIntervalMs(Integer autoCommitIntervalMs) {     this.autoCommitIntervalMs = autoCommitIntervalMs. }
false;public;0;3;;public Integer getFetchMinBytes() {     return fetchMinBytes. }
true;public;1;3;/**  * The minimum amount of data the server should return for a fetch request.  * If insufficient data is available the request will wait for that much  * data to accumulate before answering the request.  */ ;/**  * The minimum amount of data the server should return for a fetch request.  * If insufficient data is available the request will wait for that much  * data to accumulate before answering the request.  */ public void setFetchMinBytes(Integer fetchMinBytes) {     this.fetchMinBytes = fetchMinBytes. }
true;public;0;3;/**  * The maximum amount of data the server should return for a fetch request  * This is not an absolute maximum, if the first message in the first  * non-empty partition of the fetch is larger than this value, the message  * will still be returned to ensure that the consumer can make progress. The  * maximum message size accepted by the broker is defined via  * message.max.bytes (broker config) or max.message.bytes (topic config).  * Note that the consumer performs multiple fetches in parallel.  */ ;/**  * The maximum amount of data the server should return for a fetch request  * This is not an absolute maximum, if the first message in the first  * non-empty partition of the fetch is larger than this value, the message  * will still be returned to ensure that the consumer can make progress. The  * maximum message size accepted by the broker is defined via  * message.max.bytes (broker config) or max.message.bytes (topic config).  * Note that the consumer performs multiple fetches in parallel.  */ public Integer getFetchMaxBytes() {     return fetchMaxBytes. }
false;public;1;3;;public void setFetchMaxBytes(Integer fetchMaxBytes) {     this.fetchMaxBytes = fetchMaxBytes. }
false;public;0;3;;public Integer getFetchWaitMaxMs() {     return fetchWaitMaxMs. }
true;public;1;3;/**  * The maximum amount of time the server will block before answering the  * fetch request if there isn't sufficient data to immediately satisfy  * fetch.min.bytes  */ ;/**  * The maximum amount of time the server will block before answering the  * fetch request if there isn't sufficient data to immediately satisfy  * fetch.min.bytes  */ public void setFetchWaitMaxMs(Integer fetchWaitMaxMs) {     this.fetchWaitMaxMs = fetchWaitMaxMs. }
false;public;0;3;;public String getAutoOffsetReset() {     return autoOffsetReset. }
true;public;1;3;/**  * What to do when there is no initial offset in ZooKeeper or if an offset  * is out of range: earliest : automatically reset the offset to the  * earliest offset latest : automatically reset the offset to the latest  * offset fail: throw exception to the consumer  */ ;/**  * What to do when there is no initial offset in ZooKeeper or if an offset  * is out of range: earliest : automatically reset the offset to the  * earliest offset latest : automatically reset the offset to the latest  * offset fail: throw exception to the consumer  */ public void setAutoOffsetReset(String autoOffsetReset) {     this.autoOffsetReset = autoOffsetReset. }
false;public;0;3;;public String getAutoCommitOnStop() {     return autoCommitOnStop. }
true;public;1;3;/**  * Whether to perform an explicit auto commit when the consumer stops to  * ensure the broker has a commit from the last consumed message. This  * requires the option autoCommitEnable is turned on. The possible values  * are: sync, async, or none. And sync is the default value.  */ ;/**  * Whether to perform an explicit auto commit when the consumer stops to  * ensure the broker has a commit from the last consumed message. This  * requires the option autoCommitEnable is turned on. The possible values  * are: sync, async, or none. And sync is the default value.  */ public void setAutoCommitOnStop(String autoCommitOnStop) {     this.autoCommitOnStop = autoCommitOnStop. }
false;public;0;3;;public boolean isBreakOnFirstError() {     return breakOnFirstError. }
true;public;1;3;/**  * This options controls what happens when a consumer is processing an  * exchange and it fails. If the option is <tt>false</tt> then the consumer  * continues to the next message and processes it. If the option is  * <tt>true</tt> then the consumer breaks out, and will seek back to offset  * of the message that caused a failure, and then re-attempt to process this  * message. However this can lead to endless processing of the same message  * if its bound to fail every time, eg a poison message. Therefore its  * recommended to deal with that for example by using Camel's error handler.  */ ;/**  * This options controls what happens when a consumer is processing an  * exchange and it fails. If the option is <tt>false</tt> then the consumer  * continues to the next message and processes it. If the option is  * <tt>true</tt> then the consumer breaks out, and will seek back to offset  * of the message that caused a failure, and then re-attempt to process this  * message. However this can lead to endless processing of the same message  * if its bound to fail every time, eg a poison message. Therefore its  * recommended to deal with that for example by using Camel's error handler.  */ public void setBreakOnFirstError(boolean breakOnFirstError) {     this.breakOnFirstError = breakOnFirstError. }
false;public;0;3;;public String getBrokers() {     return brokers. }
true;public;1;3;/**  * URL of the Kafka brokers to use. The format is host1:port1,host2:port2,  * and the list can be a subset of brokers or a VIP pointing to a subset of  * brokers.  * <p/>  * This option is known as <tt>bootstrap.servers</tt> in the Kafka  * documentation.  */ ;/**  * URL of the Kafka brokers to use. The format is host1:port1,host2:port2,  * and the list can be a subset of brokers or a VIP pointing to a subset of  * brokers.  * <p/>  * This option is known as <tt>bootstrap.servers</tt> in the Kafka  * documentation.  */ public void setBrokers(String brokers) {     this.brokers = brokers. }
false;public;0;3;;public String getSchemaRegistryURL() {     return schemaRegistryURL. }
true;public;1;3;/**  * URL of the Confluent schema registry servers to use.  * The format is host1:port1,host2:port2.  * This is known as schema.registry.url in the Confluent documentation.  * <p/>  * This option is only available in the Confluent Kafka product (not standard Apache Kafka)  */ ;/**  * URL of the Confluent schema registry servers to use.  * The format is host1:port1,host2:port2.  * This is known as schema.registry.url in the Confluent documentation.  * <p/>  * This option is only available in the Confluent Kafka product (not standard Apache Kafka)  */ public void setSchemaRegistryURL(String schemaRegistryURL) {     this.schemaRegistryURL = schemaRegistryURL. }
false;public;0;3;;public String getCompressionCodec() {     return compressionCodec. }
true;public;1;3;/**  * This parameter allows you to specify the compression codec for all data  * generated by this producer. Valid values are "none", "gzip" and "snappy".  */ ;/**  * This parameter allows you to specify the compression codec for all data  * generated by this producer. Valid values are "none", "gzip" and "snappy".  */ public void setCompressionCodec(String compressionCodec) {     this.compressionCodec = compressionCodec. }
false;public;0;3;;public Integer getRetryBackoffMs() {     return retryBackoffMs. }
true;public;1;3;/**  * Before each retry, the producer refreshes the metadata of relevant topics  * to see if a new leader has been elected. Since leader election takes a  * bit of time, this property specifies the amount of time that the producer  * waits before refreshing the metadata.  */ ;/**  * Before each retry, the producer refreshes the metadata of relevant topics  * to see if a new leader has been elected. Since leader election takes a  * bit of time, this property specifies the amount of time that the producer  * waits before refreshing the metadata.  */ public void setRetryBackoffMs(Integer retryBackoffMs) {     this.retryBackoffMs = retryBackoffMs. }
false;public;0;3;;public Integer getSendBufferBytes() {     return sendBufferBytes. }
true;public;1;3;/**  * Socket write buffer size  */ ;/**  * Socket write buffer size  */ public void setSendBufferBytes(Integer sendBufferBytes) {     this.sendBufferBytes = sendBufferBytes. }
false;public;0;3;;public Integer getRequestTimeoutMs() {     return requestTimeoutMs. }
true;public;1;3;/**  * The amount of time the broker will wait trying to meet the  * request.required.acks requirement before sending back an error to the  * client.  */ ;/**  * The amount of time the broker will wait trying to meet the  * request.required.acks requirement before sending back an error to the  * client.  */ public void setRequestTimeoutMs(Integer requestTimeoutMs) {     this.requestTimeoutMs = requestTimeoutMs. }
false;public;0;3;;public Integer getQueueBufferingMaxMessages() {     return queueBufferingMaxMessages. }
true;public;1;3;/**  * The maximum number of unsent messages that can be queued up the producer  * when using async mode before either the producer must be blocked or data  * must be dropped.  */ ;/**  * The maximum number of unsent messages that can be queued up the producer  * when using async mode before either the producer must be blocked or data  * must be dropped.  */ public void setQueueBufferingMaxMessages(Integer queueBufferingMaxMessages) {     this.queueBufferingMaxMessages = queueBufferingMaxMessages. }
false;public;0;3;;public String getSerializerClass() {     return serializerClass. }
true;public;1;3;/**  * The serializer class for messages.  */ ;/**  * The serializer class for messages.  */ public void setSerializerClass(String serializerClass) {     this.serializerClass = serializerClass. }
false;public;0;3;;public String getKeySerializerClass() {     return keySerializerClass. }
true;public;1;3;/**  * The serializer class for keys (defaults to the same as for messages if  * nothing is given).  */ ;/**  * The serializer class for keys (defaults to the same as for messages if  * nothing is given).  */ public void setKeySerializerClass(String keySerializerClass) {     this.keySerializerClass = keySerializerClass. }
false;public;0;3;;public String getKerberosInitCmd() {     return kerberosInitCmd. }
true;public;1;3;/**  * Kerberos kinit command path. Default is /usr/bin/kinit  */ ;/**  * Kerberos kinit command path. Default is /usr/bin/kinit  */ public void setKerberosInitCmd(String kerberosInitCmd) {     this.kerberosInitCmd = kerberosInitCmd. }
false;public;0;3;;public Integer getKerberosBeforeReloginMinTime() {     return kerberosBeforeReloginMinTime. }
true;public;1;3;/**  * Login thread sleep time between refresh attempts.  */ ;/**  * Login thread sleep time between refresh attempts.  */ public void setKerberosBeforeReloginMinTime(Integer kerberosBeforeReloginMinTime) {     this.kerberosBeforeReloginMinTime = kerberosBeforeReloginMinTime. }
false;public;0;3;;public Double getKerberosRenewJitter() {     return kerberosRenewJitter. }
true;public;1;3;/**  * Percentage of random jitter added to the renewal time.  */ ;/**  * Percentage of random jitter added to the renewal time.  */ public void setKerberosRenewJitter(Double kerberosRenewJitter) {     this.kerberosRenewJitter = kerberosRenewJitter. }
false;public;0;3;;public Double getKerberosRenewWindowFactor() {     return kerberosRenewWindowFactor. }
true;public;1;3;/**  * Login thread will sleep until the specified window factor of time from  * last refresh to ticket's expiry has been reached, at which time it will  * try to renew the ticket.  */ ;/**  * Login thread will sleep until the specified window factor of time from  * last refresh to ticket's expiry has been reached, at which time it will  * try to renew the ticket.  */ public void setKerberosRenewWindowFactor(Double kerberosRenewWindowFactor) {     this.kerberosRenewWindowFactor = kerberosRenewWindowFactor. }
false;public;0;3;;public String getKerberosPrincipalToLocalRules() {     return kerberosPrincipalToLocalRules. }
true;public;1;3;/**  * A list of rules for mapping from principal names to short names  * (typically operating system usernames). The rules are evaluated in order  * and the first rule that matches a principal name is used to map it to a  * short name. Any later rules in the list are ignored. By default,  * principal names of the form {username}/{hostname}@{REALM} are mapped to  * {username}. For more details on the format please see the security authorization and acls documentation..  * <p/>  * Multiple values can be separated by comma  */ ;/**  * A list of rules for mapping from principal names to short names  * (typically operating system usernames). The rules are evaluated in order  * and the first rule that matches a principal name is used to map it to a  * short name. Any later rules in the list are ignored. By default,  * principal names of the form {username}/{hostname}@{REALM} are mapped to  * {username}. For more details on the format please see the security authorization and acls documentation..  * <p/>  * Multiple values can be separated by comma  */ public void setKerberosPrincipalToLocalRules(String kerberosPrincipalToLocalRules) {     this.kerberosPrincipalToLocalRules = kerberosPrincipalToLocalRules. }
false;public;0;3;;public String getSslCipherSuites() {     return sslCipherSuites. }
true;public;1;3;/**  * A list of cipher suites. This is a named combination of authentication,  * encryption, MAC and key exchange algorithm used to negotiate the security  * settings for a network connection using TLS or SSL network protocol.By  * default all the available cipher suites are supported.  */ ;/**  * A list of cipher suites. This is a named combination of authentication,  * encryption, MAC and key exchange algorithm used to negotiate the security  * settings for a network connection using TLS or SSL network protocol.By  * default all the available cipher suites are supported.  */ public void setSslCipherSuites(String sslCipherSuites) {     this.sslCipherSuites = sslCipherSuites. }
false;public;0;3;;public String getSslEndpointAlgorithm() {     return sslEndpointAlgorithm. }
true;public;1;3;/**  * The endpoint identification algorithm to validate server hostname using  * server certificate.  */ ;/**  * The endpoint identification algorithm to validate server hostname using  * server certificate.  */ public void setSslEndpointAlgorithm(String sslEndpointAlgorithm) {     this.sslEndpointAlgorithm = sslEndpointAlgorithm. }
false;public;0;3;;public String getSslKeymanagerAlgorithm() {     return sslKeymanagerAlgorithm. }
true;public;1;3;/**  * The algorithm used by key manager factory for SSL connections. Default  * value is the key manager factory algorithm configured for the Java  * Virtual Machine.  */ ;/**  * The algorithm used by key manager factory for SSL connections. Default  * value is the key manager factory algorithm configured for the Java  * Virtual Machine.  */ public void setSslKeymanagerAlgorithm(String sslKeymanagerAlgorithm) {     this.sslKeymanagerAlgorithm = sslKeymanagerAlgorithm. }
false;public;0;3;;public String getSslTrustmanagerAlgorithm() {     return sslTrustmanagerAlgorithm. }
true;public;1;3;/**  * The algorithm used by trust manager factory for SSL connections. Default  * value is the trust manager factory algorithm configured for the Java  * Virtual Machine.  */ ;/**  * The algorithm used by trust manager factory for SSL connections. Default  * value is the trust manager factory algorithm configured for the Java  * Virtual Machine.  */ public void setSslTrustmanagerAlgorithm(String sslTrustmanagerAlgorithm) {     this.sslTrustmanagerAlgorithm = sslTrustmanagerAlgorithm. }
false;public;0;3;;public String getSslEnabledProtocols() {     return sslEnabledProtocols. }
true;public;1;3;/**  * The list of protocols enabled for SSL connections. TLSv1.2, TLSv1.1 and  * TLSv1 are enabled by default.  */ ;/**  * The list of protocols enabled for SSL connections. TLSv1.2, TLSv1.1 and  * TLSv1 are enabled by default.  */ public void setSslEnabledProtocols(String sslEnabledProtocols) {     this.sslEnabledProtocols = sslEnabledProtocols. }
false;public;0;3;;public String getSslKeystoreType() {     return sslKeystoreType. }
true;public;1;3;/**  * The file format of the key store file. This is optional for client.  * Default value is JKS  */ ;/**  * The file format of the key store file. This is optional for client.  * Default value is JKS  */ public void setSslKeystoreType(String sslKeystoreType) {     this.sslKeystoreType = sslKeystoreType. }
false;public;0;3;;public String getSslProtocol() {     return sslProtocol. }
true;public;1;3;/**  * The SSL protocol used to generate the SSLContext. Default setting is TLS,  * which is fine for most cases. Allowed values in recent JVMs are TLS,  * TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in older JVMs,  * but their usage is discouraged due to known security vulnerabilities.  */ ;/**  * The SSL protocol used to generate the SSLContext. Default setting is TLS,  * which is fine for most cases. Allowed values in recent JVMs are TLS,  * TLSv1.1 and TLSv1.2. SSL, SSLv2 and SSLv3 may be supported in older JVMs,  * but their usage is discouraged due to known security vulnerabilities.  */ public void setSslProtocol(String sslProtocol) {     this.sslProtocol = sslProtocol. }
false;public;0;3;;public String getSslProvider() {     return sslProvider. }
true;public;1;3;/**  * The name of the security provider used for SSL connections. Default value  * is the default security provider of the JVM.  */ ;/**  * The name of the security provider used for SSL connections. Default value  * is the default security provider of the JVM.  */ public void setSslProvider(String sslProvider) {     this.sslProvider = sslProvider. }
false;public;0;3;;public String getSslTruststoreType() {     return sslTruststoreType. }
true;public;1;3;/**  * The file format of the trust store file. Default value is JKS.  */ ;/**  * The file format of the trust store file. Default value is JKS.  */ public void setSslTruststoreType(String sslTruststoreType) {     this.sslTruststoreType = sslTruststoreType. }
false;public;0;3;;public String getSaslKerberosServiceName() {     return saslKerberosServiceName. }
true;public;1;3;/**  * The Kerberos principal name that Kafka runs as. This can be defined  * either in Kafka's JAAS config or in Kafka's config.  */ ;/**  * The Kerberos principal name that Kafka runs as. This can be defined  * either in Kafka's JAAS config or in Kafka's config.  */ public void setSaslKerberosServiceName(String saslKerberosServiceName) {     this.saslKerberosServiceName = saslKerberosServiceName. }
false;public;0;3;;public String getSaslMechanism() {     return saslMechanism. }
true;public;1;3;/**  * The Simple Authentication and Security Layer (SASL) Mechanism used. For  * the valid values see <a href=  * "http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml">http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml</a>  */ ;/**  * The Simple Authentication and Security Layer (SASL) Mechanism used. For  * the valid values see <a href=  * "http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml">http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml</a>  */ public void setSaslMechanism(String saslMechanism) {     this.saslMechanism = saslMechanism. }
false;public;0;3;;public String getSaslJaasConfig() {     return saslJaasConfig. }
true;public;1;3;/**  * Expose the kafka sasl.jaas.config parameter Example:  * org.apache.kafka.common.security.plain.PlainLoginModule required  * username="USERNAME" password="PASSWORD".  */ ;/**  * Expose the kafka sasl.jaas.config parameter Example:  * org.apache.kafka.common.security.plain.PlainLoginModule required  * username="USERNAME" password="PASSWORD".  */ public void setSaslJaasConfig(String saslMechanism) {     this.saslJaasConfig = saslMechanism. }
false;public;0;3;;public String getSecurityProtocol() {     return securityProtocol. }
true;public;1;3;/**  * Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT and  * SSL are supported  */ ;/**  * Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT and  * SSL are supported  */ public void setSecurityProtocol(String securityProtocol) {     this.securityProtocol = securityProtocol. }
false;public;0;3;;public SSLContextParameters getSslContextParameters() {     return sslContextParameters. }
true;public;1;3;/**  * SSL configuration using a Camel {@link SSLContextParameters} object. If  * configured it's applied before the other SSL endpoint parameters.  */ ;/**  * SSL configuration using a Camel {@link SSLContextParameters} object. If  * configured it's applied before the other SSL endpoint parameters.  */ public void setSslContextParameters(SSLContextParameters sslContextParameters) {     this.sslContextParameters = sslContextParameters. }
false;public;0;3;;public String getSslKeyPassword() {     return sslKeyPassword. }
true;public;1;3;/**  * The password of the private key in the key store file. This is optional  * for client.  */ ;/**  * The password of the private key in the key store file. This is optional  * for client.  */ public void setSslKeyPassword(String sslKeyPassword) {     this.sslKeyPassword = sslKeyPassword. }
false;public;0;3;;public String getSslKeystoreLocation() {     return sslKeystoreLocation. }
true;public;1;3;/**  * The location of the key store file. This is optional for client and can  * be used for two-way authentication for client.  */ ;/**  * The location of the key store file. This is optional for client and can  * be used for two-way authentication for client.  */ public void setSslKeystoreLocation(String sslKeystoreLocation) {     this.sslKeystoreLocation = sslKeystoreLocation. }
false;public;0;3;;public String getSslKeystorePassword() {     return sslKeystorePassword. }
true;public;1;3;/**  * The store password for the key store file.This is optional for client and  * only needed if ssl.keystore.location is configured.  */ ;/**  * The store password for the key store file.This is optional for client and  * only needed if ssl.keystore.location is configured.  */ public void setSslKeystorePassword(String sslKeystorePassword) {     this.sslKeystorePassword = sslKeystorePassword. }
false;public;0;3;;public String getSslTruststoreLocation() {     return sslTruststoreLocation. }
true;public;1;3;/**  * The location of the trust store file.  */ ;/**  * The location of the trust store file.  */ public void setSslTruststoreLocation(String sslTruststoreLocation) {     this.sslTruststoreLocation = sslTruststoreLocation. }
false;public;0;3;;public String getSslTruststorePassword() {     return sslTruststorePassword. }
true;public;1;3;/**  * The password for the trust store file.  */ ;/**  * The password for the trust store file.  */ public void setSslTruststorePassword(String sslTruststorePassword) {     this.sslTruststorePassword = sslTruststorePassword. }
false;public;0;3;;public Integer getBufferMemorySize() {     return bufferMemorySize. }
true;public;1;3;/**  * The total bytes of memory the producer can use to buffer records waiting  * to be sent to the server. If records are sent faster than they can be  * delivered to the server the producer will either block or throw an  * exception based on the preference specified by block.on.buffer.full.This  * setting should correspond roughly to the total memory the producer will  * use, but is not a hard bound since not all memory the producer uses is  * used for buffering. Some additional memory will be used for compression  * (if compression is enabled) as well as for maintaining in-flight  * requests.  */ ;/**  * The total bytes of memory the producer can use to buffer records waiting  * to be sent to the server. If records are sent faster than they can be  * delivered to the server the producer will either block or throw an  * exception based on the preference specified by block.on.buffer.full.This  * setting should correspond roughly to the total memory the producer will  * use, but is not a hard bound since not all memory the producer uses is  * used for buffering. Some additional memory will be used for compression  * (if compression is enabled) as well as for maintaining in-flight  * requests.  */ public void setBufferMemorySize(Integer bufferMemorySize) {     this.bufferMemorySize = bufferMemorySize. }
false;public;0;3;;public String getKey() {     return key. }
true;public;1;3;/**  * The record key (or null if no key is specified). If this option has been  * configured then it take precedence over header {@link KafkaConstants#KEY}  */ ;/**  * The record key (or null if no key is specified). If this option has been  * configured then it take precedence over header {@link KafkaConstants#KEY}  */ public void setKey(String key) {     this.key = key. }
false;public;0;3;;public Integer getPartitionKey() {     return partitionKey. }
true;public;1;3;/**  * The partition to which the record will be sent (or null if no partition  * was specified). If this option has been configured then it take  * precedence over header {@link KafkaConstants#PARTITION_KEY}  */ ;/**  * The partition to which the record will be sent (or null if no partition  * was specified). If this option has been configured then it take  * precedence over header {@link KafkaConstants#PARTITION_KEY}  */ public void setPartitionKey(Integer partitionKey) {     this.partitionKey = partitionKey. }
false;public;0;3;;public String getRequestRequiredAcks() {     return requestRequiredAcks. }
true;public;1;3;/**  * The number of acknowledgments the producer requires the leader to have  * received before considering a request complete. This controls the  * durability of records that are sent. The following settings are common:  * acks=0 If set to zero then the producer will not wait for any  * acknowledgment from the server at all. The record will be immediately  * added to the socket buffer and considered sent. No guarantee can be made  * that the server has received the record in this case, and the retries  * configuration will not take effect (as the client won't generally know of  * any failures). The offset given back for each record will always be set  * to -1. acks=1 This will mean the leader will write the record to its  * local log but will respond without awaiting full acknowledgement from all  * followers. In this case should the leader fail immediately after  * acknowledging the record but before the followers have replicated it then  * the record will be lost. acks=all This means the leader will wait for the  * full set of in-sync replicas to acknowledge the record. This guarantees  * that the record will not be lost as long as at least one in-sync replica  * remains alive. This is the strongest available guarantee.  */ ;/**  * The number of acknowledgments the producer requires the leader to have  * received before considering a request complete. This controls the  * durability of records that are sent. The following settings are common:  * acks=0 If set to zero then the producer will not wait for any  * acknowledgment from the server at all. The record will be immediately  * added to the socket buffer and considered sent. No guarantee can be made  * that the server has received the record in this case, and the retries  * configuration will not take effect (as the client won't generally know of  * any failures). The offset given back for each record will always be set  * to -1. acks=1 This will mean the leader will write the record to its  * local log but will respond without awaiting full acknowledgement from all  * followers. In this case should the leader fail immediately after  * acknowledging the record but before the followers have replicated it then  * the record will be lost. acks=all This means the leader will wait for the  * full set of in-sync replicas to acknowledge the record. This guarantees  * that the record will not be lost as long as at least one in-sync replica  * remains alive. This is the strongest available guarantee.  */ public void setRequestRequiredAcks(String requestRequiredAcks) {     this.requestRequiredAcks = requestRequiredAcks. }
false;public;0;3;;public Integer getRetries() {     return retries. }
true;public;1;3;/**  * Setting a value greater than zero will cause the client to resend any  * record whose send fails with a potentially transient error. Note that  * this retry is no different than if the client resent the record upon  * receiving the error. Allowing retries will potentially change the  * ordering of records because if two records are sent to a single  * partition, and the first fails and is retried but the second succeeds,  * then the second record may appear first.  */ ;/**  * Setting a value greater than zero will cause the client to resend any  * record whose send fails with a potentially transient error. Note that  * this retry is no different than if the client resent the record upon  * receiving the error. Allowing retries will potentially change the  * ordering of records because if two records are sent to a single  * partition, and the first fails and is retried but the second succeeds,  * then the second record may appear first.  */ public void setRetries(Integer retries) {     this.retries = retries. }
false;public;0;3;;public Integer getProducerBatchSize() {     return producerBatchSize. }
true;public;1;3;/**  * The producer will attempt to batch records together into fewer requests  * whenever multiple records are being sent to the same partition. This  * helps performance on both the client and the server. This configuration  * controls the default batch size in bytes. No attempt will be made to  * batch records larger than this size.Requests sent to brokers will contain  * multiple batches, one for each partition with data available to be sent.A  * small batch size will make batching less common and may reduce throughput  * (a batch size of zero will disable batching entirely). A very large batch  * size may use memory a bit more wastefully as we will always allocate a  * buffer of the specified batch size in anticipation of additional records.  */ ;/**  * The producer will attempt to batch records together into fewer requests  * whenever multiple records are being sent to the same partition. This  * helps performance on both the client and the server. This configuration  * controls the default batch size in bytes. No attempt will be made to  * batch records larger than this size.Requests sent to brokers will contain  * multiple batches, one for each partition with data available to be sent.A  * small batch size will make batching less common and may reduce throughput  * (a batch size of zero will disable batching entirely). A very large batch  * size may use memory a bit more wastefully as we will always allocate a  * buffer of the specified batch size in anticipation of additional records.  */ public void setProducerBatchSize(Integer producerBatchSize) {     this.producerBatchSize = producerBatchSize. }
false;public;0;3;;public Integer getConnectionMaxIdleMs() {     return connectionMaxIdleMs. }
true;public;1;3;/**  * Close idle connections after the number of milliseconds specified by this  * config.  */ ;/**  * Close idle connections after the number of milliseconds specified by this  * config.  */ public void setConnectionMaxIdleMs(Integer connectionMaxIdleMs) {     this.connectionMaxIdleMs = connectionMaxIdleMs. }
false;public;0;3;;public Integer getLingerMs() {     return lingerMs. }
true;public;1;3;/**  * The producer groups together any records that arrive in between request  * transmissions into a single batched request. Normally this occurs only  * under load when records arrive faster than they can be sent out. However  * in some circumstances the client may want to reduce the number of  * requests even under moderate load. This setting accomplishes this by  * adding a small amount of artificial delaythat is, rather than  * immediately sending out a record the producer will wait for up to the  * given delay to allow other records to be sent so that the sends can be  * batched together. This can be thought of as analogous to Nagle's  * algorithm in TCP. This setting gives the upper bound on the delay for  * batching: once we get batch.size worth of records for a partition it will  * be sent immediately regardless of this setting, however if we have fewer  * than this many bytes accumulated for this partition we will 'linger' for  * the specified time waiting for more records to show up. This setting  * defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would  * have the effect of reducing the number of requests sent but would add up  * to 5ms of latency to records sent in the absense of load.  */ ;/**  * The producer groups together any records that arrive in between request  * transmissions into a single batched request. Normally this occurs only  * under load when records arrive faster than they can be sent out. However  * in some circumstances the client may want to reduce the number of  * requests even under moderate load. This setting accomplishes this by  * adding a small amount of artificial delaythat is, rather than  * immediately sending out a record the producer will wait for up to the  * given delay to allow other records to be sent so that the sends can be  * batched together. This can be thought of as analogous to Nagle's  * algorithm in TCP. This setting gives the upper bound on the delay for  * batching: once we get batch.size worth of records for a partition it will  * be sent immediately regardless of this setting, however if we have fewer  * than this many bytes accumulated for this partition we will 'linger' for  * the specified time waiting for more records to show up. This setting  * defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would  * have the effect of reducing the number of requests sent but would add up  * to 5ms of latency to records sent in the absense of load.  */ public void setLingerMs(Integer lingerMs) {     this.lingerMs = lingerMs. }
false;public;0;3;;public Integer getMaxBlockMs() {     return maxBlockMs. }
true;public;1;3;/**  * The configuration controls how long sending to kafka will block. These  * methods can be blocked for multiple reasons. For e.g: buffer full,  * metadata unavailable.This configuration imposes maximum limit on the  * total time spent in fetching metadata, serialization of key and value,  * partitioning and allocation of buffer memory when doing a send(). In case  * of partitionsFor(), this configuration imposes a maximum time threshold  * on waiting for metadata  */ ;/**  * The configuration controls how long sending to kafka will block. These  * methods can be blocked for multiple reasons. For e.g: buffer full,  * metadata unavailable.This configuration imposes maximum limit on the  * total time spent in fetching metadata, serialization of key and value,  * partitioning and allocation of buffer memory when doing a send(). In case  * of partitionsFor(), this configuration imposes a maximum time threshold  * on waiting for metadata  */ public void setMaxBlockMs(Integer maxBlockMs) {     this.maxBlockMs = maxBlockMs. }
false;public;0;3;;public Integer getMaxRequestSize() {     return maxRequestSize. }
true;public;1;3;/**  * The maximum size of a request. This is also effectively a cap on the  * maximum record size. Note that the server has its own cap on record size  * which may be different from this. This setting will limit the number of  * record batches the producer will send in a single request to avoid  * sending huge requests.  */ ;/**  * The maximum size of a request. This is also effectively a cap on the  * maximum record size. Note that the server has its own cap on record size  * which may be different from this. This setting will limit the number of  * record batches the producer will send in a single request to avoid  * sending huge requests.  */ public void setMaxRequestSize(Integer maxRequestSize) {     this.maxRequestSize = maxRequestSize. }
false;public;0;3;;public Integer getReceiveBufferBytes() {     return receiveBufferBytes. }
true;public;1;3;/**  * The size of the TCP receive buffer (SO_RCVBUF) to use when reading data.  */ ;/**  * The size of the TCP receive buffer (SO_RCVBUF) to use when reading data.  */ public void setReceiveBufferBytes(Integer receiveBufferBytes) {     this.receiveBufferBytes = receiveBufferBytes. }
false;public;0;3;;public Integer getMaxInFlightRequest() {     return maxInFlightRequest. }
true;public;1;3;/**  * The maximum number of unacknowledged requests the client will send on a  * single connection before blocking. Note that if this setting is set to be  * greater than 1 and there are failed sends, there is a risk of message  * re-ordering due to retries (i.e., if retries are enabled).  */ ;/**  * The maximum number of unacknowledged requests the client will send on a  * single connection before blocking. Note that if this setting is set to be  * greater than 1 and there are failed sends, there is a risk of message  * re-ordering due to retries (i.e., if retries are enabled).  */ public void setMaxInFlightRequest(Integer maxInFlightRequest) {     this.maxInFlightRequest = maxInFlightRequest. }
false;public;0;3;;public Integer getMetadataMaxAgeMs() {     return metadataMaxAgeMs. }
true;public;1;3;/**  * The period of time in milliseconds after which we force a refresh of  * metadata even if we haven't seen any partition leadership changes to  * proactively discover any new brokers or partitions.  */ ;/**  * The period of time in milliseconds after which we force a refresh of  * metadata even if we haven't seen any partition leadership changes to  * proactively discover any new brokers or partitions.  */ public void setMetadataMaxAgeMs(Integer metadataMaxAgeMs) {     this.metadataMaxAgeMs = metadataMaxAgeMs. }
false;public;0;3;;public String getMetricReporters() {     return metricReporters. }
true;public;1;3;/**  * A list of classes to use as metrics reporters. Implementing the  * MetricReporter interface allows plugging in classes that will be notified  * of new metric creation. The JmxReporter is always included to register  * JMX statistics.  */ ;/**  * A list of classes to use as metrics reporters. Implementing the  * MetricReporter interface allows plugging in classes that will be notified  * of new metric creation. The JmxReporter is always included to register  * JMX statistics.  */ public void setMetricReporters(String metricReporters) {     this.metricReporters = metricReporters. }
false;public;0;3;;public Integer getNoOfMetricsSample() {     return noOfMetricsSample. }
true;public;1;3;/**  * The number of samples maintained to compute metrics.  */ ;/**  * The number of samples maintained to compute metrics.  */ public void setNoOfMetricsSample(Integer noOfMetricsSample) {     this.noOfMetricsSample = noOfMetricsSample. }
false;public;0;3;;public Integer getMetricsSampleWindowMs() {     return metricsSampleWindowMs. }
true;public;1;3;/**  * The number of samples maintained to compute metrics.  */ ;/**  * The number of samples maintained to compute metrics.  */ public void setMetricsSampleWindowMs(Integer metricsSampleWindowMs) {     this.metricsSampleWindowMs = metricsSampleWindowMs. }
false;public;0;3;;public Integer getReconnectBackoffMs() {     return reconnectBackoffMs. }
true;public;1;3;/**  * The amount of time to wait before attempting to reconnect to a given  * host. This avoids repeatedly connecting to a host in a tight loop. This  * backoff applies to all requests sent by the consumer to the broker.  */ ;/**  * The amount of time to wait before attempting to reconnect to a given  * host. This avoids repeatedly connecting to a host in a tight loop. This  * backoff applies to all requests sent by the consumer to the broker.  */ public void setReconnectBackoffMs(Integer reconnectBackoffMs) {     this.reconnectBackoffMs = reconnectBackoffMs. }
false;public;0;3;;public Integer getHeartbeatIntervalMs() {     return heartbeatIntervalMs. }
true;public;1;3;/**  * The expected time between heartbeats to the consumer coordinator when  * using Kafka's group management facilities. Heartbeats are used to ensure  * that the consumer's session stays active and to facilitate rebalancing  * when new consumers join or leave the group. The value must be set lower  * than session.timeout.ms, but typically should be set no higher than 1/3  * of that value. It can be adjusted even lower to control the expected time  * for normal rebalances.  */ ;/**  * The expected time between heartbeats to the consumer coordinator when  * using Kafka's group management facilities. Heartbeats are used to ensure  * that the consumer's session stays active and to facilitate rebalancing  * when new consumers join or leave the group. The value must be set lower  * than session.timeout.ms, but typically should be set no higher than 1/3  * of that value. It can be adjusted even lower to control the expected time  * for normal rebalances.  */ public void setHeartbeatIntervalMs(Integer heartbeatIntervalMs) {     this.heartbeatIntervalMs = heartbeatIntervalMs. }
false;public;0;3;;public Integer getMaxPartitionFetchBytes() {     return maxPartitionFetchBytes. }
true;public;1;3;/**  * The maximum amount of data per-partition the server will return. The  * maximum total memory used for a request will be #partitions *  * max.partition.fetch.bytes. This size must be at least as large as the  * maximum message size the server allows or else it is possible for the  * producer to send messages larger than the consumer can fetch. If that  * happens, the consumer can get stuck trying to fetch a large message on a  * certain partition.  */ ;/**  * The maximum amount of data per-partition the server will return. The  * maximum total memory used for a request will be #partitions *  * max.partition.fetch.bytes. This size must be at least as large as the  * maximum message size the server allows or else it is possible for the  * producer to send messages larger than the consumer can fetch. If that  * happens, the consumer can get stuck trying to fetch a large message on a  * certain partition.  */ public void setMaxPartitionFetchBytes(Integer maxPartitionFetchBytes) {     this.maxPartitionFetchBytes = maxPartitionFetchBytes. }
false;public;0;3;;public Integer getSessionTimeoutMs() {     return sessionTimeoutMs. }
true;public;1;3;/**  * The timeout used to detect failures when using Kafka's group management  * facilities.  */ ;/**  * The timeout used to detect failures when using Kafka's group management  * facilities.  */ public void setSessionTimeoutMs(Integer sessionTimeoutMs) {     this.sessionTimeoutMs = sessionTimeoutMs. }
false;public;0;3;;public Integer getMaxPollRecords() {     return maxPollRecords. }
true;public;1;3;/**  * The maximum number of records returned in a single call to poll()  */ ;/**  * The maximum number of records returned in a single call to poll()  */ public void setMaxPollRecords(Integer maxPollRecords) {     this.maxPollRecords = maxPollRecords. }
false;public;0;3;;public Long getPollTimeoutMs() {     return pollTimeoutMs. }
true;public;1;3;/**  * The timeout used when polling the KafkaConsumer.  */ ;/**  * The timeout used when polling the KafkaConsumer.  */ public void setPollTimeoutMs(Long pollTimeoutMs) {     this.pollTimeoutMs = pollTimeoutMs. }
false;public;0;3;;public Long getMaxPollIntervalMs() {     return maxPollIntervalMs. }
true;public;1;3;/**  * The maximum delay between invocations of poll() when using consumer group  * management. This places an upper bound on the amount of time that the  * consumer can be idle before fetching more records. If poll() is not  * called before expiration of this timeout, then the consumer is considered  * failed and the group will rebalance in order to reassign the partitions  * to another member.  */ ;/**  * The maximum delay between invocations of poll() when using consumer group  * management. This places an upper bound on the amount of time that the  * consumer can be idle before fetching more records. If poll() is not  * called before expiration of this timeout, then the consumer is considered  * failed and the group will rebalance in order to reassign the partitions  * to another member.  */ public void setMaxPollIntervalMs(Long maxPollIntervalMs) {     this.maxPollIntervalMs = maxPollIntervalMs. }
false;public;0;3;;public String getPartitionAssignor() {     return partitionAssignor. }
true;public;1;3;/**  * The class name of the partition assignment strategy that the client will  * use to distribute partition ownership amongst consumer instances when  * group management is used  */ ;/**  * The class name of the partition assignment strategy that the client will  * use to distribute partition ownership amongst consumer instances when  * group management is used  */ public void setPartitionAssignor(String partitionAssignor) {     this.partitionAssignor = partitionAssignor. }
false;public;0;3;;public Integer getConsumerRequestTimeoutMs() {     return consumerRequestTimeoutMs. }
true;public;1;3;/**  * The configuration controls the maximum amount of time the client will  * wait for the response of a request. If the response is not received  * before the timeout elapses the client will resend the request if  * necessary or fail the request if retries are exhausted.  */ ;/**  * The configuration controls the maximum amount of time the client will  * wait for the response of a request. If the response is not received  * before the timeout elapses the client will resend the request if  * necessary or fail the request if retries are exhausted.  */ public void setConsumerRequestTimeoutMs(Integer consumerRequestTimeoutMs) {     this.consumerRequestTimeoutMs = consumerRequestTimeoutMs. }
false;public;0;3;;public Boolean getCheckCrcs() {     return checkCrcs. }
true;public;1;3;/**  * Automatically check the CRC32 of the records consumed. This ensures no  * on-the-wire or on-disk corruption to the messages occurred. This check  * adds some overhead, so it may be disabled in cases seeking extreme  * performance.  */ ;/**  * Automatically check the CRC32 of the records consumed. This ensures no  * on-the-wire or on-disk corruption to the messages occurred. This check  * adds some overhead, so it may be disabled in cases seeking extreme  * performance.  */ public void setCheckCrcs(Boolean checkCrcs) {     this.checkCrcs = checkCrcs. }
false;public;0;3;;public String getKeyDeserializer() {     return keyDeserializer. }
true;public;1;3;/**  * Deserializer class for key that implements the Deserializer interface.  */ ;/**  * Deserializer class for key that implements the Deserializer interface.  */ public void setKeyDeserializer(String keyDeserializer) {     this.keyDeserializer = keyDeserializer. }
false;public;0;3;;public String getValueDeserializer() {     return valueDeserializer. }
true;public;1;3;/**  * Deserializer class for value that implements the Deserializer interface.  */ ;/**  * Deserializer class for value that implements the Deserializer interface.  */ public void setValueDeserializer(String valueDeserializer) {     this.valueDeserializer = valueDeserializer. }
false;public;0;3;;public String getSeekTo() {     return seekTo. }
true;public;1;3;/**  * Set if KafkaConsumer will read from beginning or end on startup:  * beginning : read from beginning end : read from end This is replacing the  * earlier property seekToBeginning  */ ;/**  * Set if KafkaConsumer will read from beginning or end on startup:  * beginning : read from beginning end : read from end This is replacing the  * earlier property seekToBeginning  */ public void setSeekTo(String seekTo) {     this.seekTo = seekTo. }
false;public;0;3;;public ExecutorService getWorkerPool() {     return workerPool. }
true;public;1;3;/**  * To use a custom worker pool for continue routing {@link Exchange} after  * kafka server has acknowledge the message that was sent to it from  * {@link KafkaProducer} using asynchronous non-blocking processing.  */ ;/**  * To use a custom worker pool for continue routing {@link Exchange} after  * kafka server has acknowledge the message that was sent to it from  * {@link KafkaProducer} using asynchronous non-blocking processing.  */ public void setWorkerPool(ExecutorService workerPool) {     this.workerPool = workerPool. }
false;public;0;3;;public Integer getWorkerPoolCoreSize() {     return workerPoolCoreSize. }
true;public;1;3;/**  * Number of core threads for the worker pool for continue routing  * {@link Exchange} after kafka server has acknowledge the message that was  * sent to it from {@link KafkaProducer} using asynchronous non-blocking  * processing.  */ ;/**  * Number of core threads for the worker pool for continue routing  * {@link Exchange} after kafka server has acknowledge the message that was  * sent to it from {@link KafkaProducer} using asynchronous non-blocking  * processing.  */ public void setWorkerPoolCoreSize(Integer workerPoolCoreSize) {     this.workerPoolCoreSize = workerPoolCoreSize. }
false;public;0;3;;public Integer getWorkerPoolMaxSize() {     return workerPoolMaxSize. }
true;public;1;3;/**  * Maximum number of threads for the worker pool for continue routing  * {@link Exchange} after kafka server has acknowledge the message that was  * sent to it from {@link KafkaProducer} using asynchronous non-blocking  * processing.  */ ;/**  * Maximum number of threads for the worker pool for continue routing  * {@link Exchange} after kafka server has acknowledge the message that was  * sent to it from {@link KafkaProducer} using asynchronous non-blocking  * processing.  */ public void setWorkerPoolMaxSize(Integer workerPoolMaxSize) {     this.workerPoolMaxSize = workerPoolMaxSize. }
false;public;0;3;;public boolean isRecordMetadata() {     return recordMetadata. }
true;public;1;3;/**  * Whether the producer should store the {@link RecordMetadata} results from  * sending to Kafka. The results are stored in a {@link List} containing the  * {@link RecordMetadata} metadata's. The list is stored on a header with  * the key {@link KafkaConstants#KAFKA_RECORDMETA}  */ ;/**  * Whether the producer should store the {@link RecordMetadata} results from  * sending to Kafka. The results are stored in a {@link List} containing the  * {@link RecordMetadata} metadata's. The list is stored on a header with  * the key {@link KafkaConstants#KAFKA_RECORDMETA}  */ public void setRecordMetadata(boolean recordMetadata) {     this.recordMetadata = recordMetadata. }
false;public;0;3;;public String getInterceptorClasses() {     return interceptorClasses. }
true;public;1;3;/**  * Sets interceptors for producer or consumers. Producer interceptors have  * to be classes implementing  * {@link org.apache.kafka.clients.producer.ProducerInterceptor} Consumer  * interceptors have to be classes implementing  * {@link org.apache.kafka.clients.consumer.ConsumerInterceptor} Note that  * if you use Producer interceptor on a consumer it will throw a class cast  * exception in runtime  */ ;/**  * Sets interceptors for producer or consumers. Producer interceptors have  * to be classes implementing  * {@link org.apache.kafka.clients.producer.ProducerInterceptor} Consumer  * interceptors have to be classes implementing  * {@link org.apache.kafka.clients.consumer.ConsumerInterceptor} Note that  * if you use Producer interceptor on a consumer it will throw a class cast  * exception in runtime  */ public void setInterceptorClasses(String interceptorClasses) {     this.interceptorClasses = interceptorClasses. }
false;public;0;3;;public boolean isEnableIdempotence() {     return enableIdempotence. }
true;public;1;3;/**  * If set to 'true' the producer will ensure that exactly one copy of each  * message is written in the stream. If 'false', producer retries may write  * duplicates of the retried message in the stream. If set to true this  * option will require max.in.flight.requests.per.connection to be set to 1  * and retries cannot be zero and additionally acks must be set to 'all'.  */ ;/**  * If set to 'true' the producer will ensure that exactly one copy of each  * message is written in the stream. If 'false', producer retries may write  * duplicates of the retried message in the stream. If set to true this  * option will require max.in.flight.requests.per.connection to be set to 1  * and retries cannot be zero and additionally acks must be set to 'all'.  */ public void setEnableIdempotence(boolean enableIdempotence) {     this.enableIdempotence = enableIdempotence. }
false;public;0;3;;public Integer getReconnectBackoffMaxMs() {     return reconnectBackoffMaxMs. }
true;public;1;3;/**  * The maximum amount of time in milliseconds to wait when reconnecting to a  * broker that has repeatedly failed to connect. If provided, the backoff  * per host will increase exponentially for each consecutive connection  * failure, up to this maximum. After calculating the backoff increase, 20%  * random jitter is added to avoid connection storms.  */ ;/**  * The maximum amount of time in milliseconds to wait when reconnecting to a  * broker that has repeatedly failed to connect. If provided, the backoff  * per host will increase exponentially for each consecutive connection  * failure, up to this maximum. After calculating the backoff increase, 20%  * random jitter is added to avoid connection storms.  */ public void setReconnectBackoffMaxMs(Integer reconnectBackoffMaxMs) {     this.reconnectBackoffMaxMs = reconnectBackoffMaxMs. }
false;public;0;3;;public HeaderFilterStrategy getHeaderFilterStrategy() {     return headerFilterStrategy. }
true;public;1;3;/**  * To use a custom HeaderFilterStrategy to filter header to and from Camel  * message.  */ ;/**  * To use a custom HeaderFilterStrategy to filter header to and from Camel  * message.  */ public void setHeaderFilterStrategy(HeaderFilterStrategy headerFilterStrategy) {     this.headerFilterStrategy = headerFilterStrategy. }
false;public;0;3;;public KafkaHeaderDeserializer getKafkaHeaderDeserializer() {     return kafkaHeaderDeserializer. }
true;public;1;3;/**  * Sets custom KafkaHeaderDeserializer for deserialization kafka headers  * values to camel headers values.  *  * @param kafkaHeaderDeserializer custom kafka header deserializer to be  *            used  */ ;/**  * Sets custom KafkaHeaderDeserializer for deserialization kafka headers  * values to camel headers values.  *  * @param kafkaHeaderDeserializer custom kafka header deserializer to be  *            used  */ public void setKafkaHeaderDeserializer(final KafkaHeaderDeserializer kafkaHeaderDeserializer) {     this.kafkaHeaderDeserializer = kafkaHeaderDeserializer. }
false;public;0;3;;public KafkaHeaderSerializer getKafkaHeaderSerializer() {     return kafkaHeaderSerializer. }
true;public;1;3;/**  * Sets custom KafkaHeaderDeserializer for serialization camel headers  * values to kafka headers values.  *  * @param kafkaHeaderSerializer custom kafka header serializer to be used  */ ;/**  * Sets custom KafkaHeaderDeserializer for serialization camel headers  * values to kafka headers values.  *  * @param kafkaHeaderSerializer custom kafka header serializer to be used  */ public void setKafkaHeaderSerializer(final KafkaHeaderSerializer kafkaHeaderSerializer) {     this.kafkaHeaderSerializer = kafkaHeaderSerializer. }
