commented;modifiers;parameterAmount;loc;comment;code
false;;0;16;;Properties getProps() {     Properties props = endpoint.getConfiguration().createProducerProperties().     endpoint.updateClassProperties(props).     // brokers can be configured on endpoint or component level     String brokers = endpoint.getConfiguration().getBrokers().     if (brokers == null) {         brokers = endpoint.getComponent().getBrokers().     }     if (brokers == null) {         throw new IllegalArgumentException("URL to the Kafka brokers must be configured with the brokers option on either the component or endpoint.").     }     props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers).     return props. }
false;public;0;4;;@SuppressWarnings("rawtypes") public org.apache.kafka.clients.producer.KafkaProducer getKafkaProducer() {     return kafkaProducer. }
true;public;1;4;/**  * To use a custom {@link org.apache.kafka.clients.producer.KafkaProducer} instance.  */ ;/**  * To use a custom {@link org.apache.kafka.clients.producer.KafkaProducer} instance.  */ @SuppressWarnings("rawtypes") public void setKafkaProducer(org.apache.kafka.clients.producer.KafkaProducer kafkaProducer) {     this.kafkaProducer = kafkaProducer. }
false;public;0;3;;public ExecutorService getWorkerPool() {     return workerPool. }
false;public;1;3;;public void setWorkerPool(ExecutorService workerPool) {     this.workerPool = workerPool. }
false;protected;0;22;;@Override @SuppressWarnings("rawtypes") protected void doStart() throws Exception {     Properties props = getProps().     if (kafkaProducer == null) {         ClassLoader threadClassLoader = Thread.currentThread().getContextClassLoader().         try {             // Kafka uses reflection for loading authentication settings, use its classloader             Thread.currentThread().setContextClassLoader(org.apache.kafka.clients.producer.KafkaProducer.class.getClassLoader()).             kafkaProducer = new org.apache.kafka.clients.producer.KafkaProducer(props).         } finally {             Thread.currentThread().setContextClassLoader(threadClassLoader).         }     }     // if we are in asynchronous mode we need a worker pool     if (!endpoint.isSynchronous() && workerPool == null) {         workerPool = endpoint.createProducerExecutor().         // we create a thread pool so we should also shut it down         shutdownWorkerPool = true.     } }
false;protected;0;11;;@Override protected void doStop() throws Exception {     if (kafkaProducer != null) {         kafkaProducer.close().     }     if (shutdownWorkerPool && workerPool != null) {         endpoint.getCamelContext().getExecutorServiceManager().shutdown(workerPool).         workerPool = null.     } }
false;public;0;4;;@Override public boolean hasNext() {     return msgList.hasNext(). }
false;public;0;14;;@Override public ProducerRecord next() {     // must convert each entry of the iterator into the value according to the serializer     Object next = msgList.next().     Object value = tryConvertToSerializedType(exchange, next, endpoint.getConfiguration().getSerializerClass()).     if (hasPartitionKey && hasMessageKey) {         return new ProducerRecord(msgTopic, partitionKey, null, key, value, propagatedHeaders).     } else if (hasMessageKey) {         return new ProducerRecord(msgTopic, null, null, key, value, propagatedHeaders).     } else {         return new ProducerRecord(msgTopic, null, null, null, value, propagatedHeaders).     } }
false;public;0;4;;@Override public void remove() {     msgList.remove(). }
false;protected;1;100;;@SuppressWarnings({ "unchecked", "rawtypes" }) protected Iterator<ProducerRecord> createRecorder(Exchange exchange) throws Exception {     String topic = endpoint.getConfiguration().getTopic().     if (!endpoint.getConfiguration().isBridgeEndpoint()) {         String headerTopic = exchange.getIn().getHeader(KafkaConstants.TOPIC, String.class).         boolean allowHeader = true.         // which we most likely do not want to do         if (headerTopic != null && endpoint.getConfiguration().isCircularTopicDetection()) {             Endpoint from = exchange.getFromEndpoint().             if (from instanceof KafkaEndpoint) {                 String fromTopic = ((KafkaEndpoint) from).getConfiguration().getTopic().                 allowHeader = !headerTopic.equals(fromTopic).                 if (!allowHeader) {                     log.debug("Circular topic detected from message header." + " Cannot send to same topic as the message comes from: {}" + ". Will use endpoint configured topic: {}", from, topic).                 }             }         }         if (allowHeader && headerTopic != null) {             topic = headerTopic.         }     }     if (topic == null) {         // if topic property was not received from configuration or header parameters take it from the remaining URI         topic = URISupport.extractRemainderPath(new URI(endpoint.getEndpointUri()), true).     }     // endpoint take precedence over header configuration     final Integer partitionKey = endpoint.getConfiguration().getPartitionKey() != null ? endpoint.getConfiguration().getPartitionKey() : exchange.getIn().getHeader(KafkaConstants.PARTITION_KEY, Integer.class).     final boolean hasPartitionKey = partitionKey != null.     // endpoint take precedence over header configuration     Object key = endpoint.getConfiguration().getKey() != null ? endpoint.getConfiguration().getKey() : exchange.getIn().getHeader(KafkaConstants.KEY).     final Object messageKey = key != null ? tryConvertToSerializedType(exchange, key, endpoint.getConfiguration().getKeySerializerClass()) : null.     final boolean hasMessageKey = messageKey != null.     // extracting headers which need to be propagated     List<Header> propagatedHeaders = getPropagatedHeaders(exchange, endpoint.getConfiguration()).     Object msg = exchange.getIn().getBody().     // is the message body a list or something that contains multiple values     Iterator<Object> iterator = null.     if (msg instanceof Iterable) {         iterator = ((Iterable<Object>) msg).iterator().     } else if (msg instanceof Iterator) {         iterator = (Iterator<Object>) msg.     }     if (iterator != null) {         final Iterator<Object> msgList = iterator.         final String msgTopic = topic.         return new Iterator<ProducerRecord>() {              @Override             public boolean hasNext() {                 return msgList.hasNext().             }              @Override             public ProducerRecord next() {                 // must convert each entry of the iterator into the value according to the serializer                 Object next = msgList.next().                 Object value = tryConvertToSerializedType(exchange, next, endpoint.getConfiguration().getSerializerClass()).                 if (hasPartitionKey && hasMessageKey) {                     return new ProducerRecord(msgTopic, partitionKey, null, key, value, propagatedHeaders).                 } else if (hasMessageKey) {                     return new ProducerRecord(msgTopic, null, null, key, value, propagatedHeaders).                 } else {                     return new ProducerRecord(msgTopic, null, null, null, value, propagatedHeaders).                 }             }              @Override             public void remove() {                 msgList.remove().             }         }.     }     // must convert each entry of the iterator into the value according to the serializer     Object value = tryConvertToSerializedType(exchange, msg, endpoint.getConfiguration().getSerializerClass()).     ProducerRecord record.     if (hasPartitionKey && hasMessageKey) {         record = new ProducerRecord(topic, partitionKey, null, key, value, propagatedHeaders).     } else if (hasMessageKey) {         record = new ProducerRecord(topic, null, null, key, value, propagatedHeaders).     } else {         record = new ProducerRecord(topic, null, null, null, value, propagatedHeaders).     }     return Collections.singletonList(record).iterator(). }
false;private;2;9;;private List<Header> getPropagatedHeaders(Exchange exchange, KafkaConfiguration getConfiguration) {     HeaderFilterStrategy headerFilterStrategy = getConfiguration.getHeaderFilterStrategy().     KafkaHeaderSerializer headerSerializer = getConfiguration.getKafkaHeaderSerializer().     return exchange.getIn().getHeaders().entrySet().stream().filter(entry -> shouldBeFiltered(entry, exchange, headerFilterStrategy)).map(entry -> getRecordHeader(entry, headerSerializer)).filter(Objects::nonNull).collect(Collectors.toList()). }
false;private;3;3;;private boolean shouldBeFiltered(Map.Entry<String, Object> entry, Exchange exchange, HeaderFilterStrategy headerFilterStrategy) {     return !headerFilterStrategy.applyFilterToExternalHeaders(entry.getKey(), entry.getValue(), exchange). }
false;private;2;7;;private RecordHeader getRecordHeader(Map.Entry<String, Object> entry, KafkaHeaderSerializer headerSerializer) {     byte[] headerValue = headerSerializer.serialize(entry.getKey(), entry.getValue()).     if (headerValue == null) {         return null.     }     return new RecordHeader(entry.getKey(), headerValue). }
false;public;1;28;;@Override @SuppressWarnings({ "unchecked", "rawtypes" }) public // Camel calls this method if the endpoint isSynchronous(), as the KafkaEndpoint creates a SynchronousDelegateProducer for it void process(Exchange exchange) throws Exception {     Iterator<ProducerRecord> c = createRecorder(exchange).     List<Future<RecordMetadata>> futures = new LinkedList<>().     List<RecordMetadata> recordMetadatas = new ArrayList<>().     if (endpoint.getConfiguration().isRecordMetadata()) {         if (exchange.hasOut()) {             exchange.getOut().setHeader(KafkaConstants.KAFKA_RECORDMETA, recordMetadatas).         } else {             exchange.getIn().setHeader(KafkaConstants.KAFKA_RECORDMETA, recordMetadatas).         }     }     while (c.hasNext()) {         ProducerRecord rec = c.next().         if (log.isDebugEnabled()) {             log.debug("Sending message to topic: {}, partition: {}, key: {}", rec.topic(), rec.partition(), rec.key()).         }         futures.add(kafkaProducer.send(rec)).     }     for (Future<RecordMetadata> f : futures) {         // wait for them all to be sent         recordMetadatas.add(f.get()).     } }
false;public;2;21;;@Override @SuppressWarnings({ "unchecked", "rawtypes" }) public boolean process(Exchange exchange, AsyncCallback callback) {     try {         Iterator<ProducerRecord> c = createRecorder(exchange).         KafkaProducerCallBack cb = new KafkaProducerCallBack(exchange, callback).         while (c.hasNext()) {             cb.increment().             ProducerRecord rec = c.next().             if (log.isDebugEnabled()) {                 log.debug("Sending message to topic: {}, partition: {}, key: {}", rec.topic(), rec.partition(), rec.key()).             }             kafkaProducer.send(rec, cb).         }         return cb.allSent().     } catch (Exception ex) {         exchange.setException(ex).     }     callback.done(true).     return true. }
true;protected;3;19;/**  * Attempts to convert the object to the same type as the serialized class specified  */ ;/**  * Attempts to convert the object to the same type as the serialized class specified  */ protected Object tryConvertToSerializedType(Exchange exchange, Object object, String serializerClass) {     Object answer = null.     if (KafkaConstants.KAFKA_DEFAULT_SERIALIZER.equals(serializerClass)) {         answer = exchange.getContext().getTypeConverter().tryConvertTo(String.class, exchange, object).     } else if ("org.apache.kafka.common.serialization.ByteArraySerializer".equals(serializerClass)) {         answer = exchange.getContext().getTypeConverter().tryConvertTo(byte[].class, exchange, object).     } else if ("org.apache.kafka.common.serialization.ByteBufferSerializer".equals(serializerClass)) {         answer = exchange.getContext().getTypeConverter().tryConvertTo(ByteBuffer.class, exchange, object).     } else if ("org.apache.kafka.common.serialization.BytesSerializer".equals(serializerClass)) {         // we need to convert to byte array first         byte[] array = exchange.getContext().getTypeConverter().tryConvertTo(byte[].class, exchange, object).         if (array != null) {             answer = new Bytes(array).         }     }     return answer != null ? answer : object. }
false;;0;3;;void increment() {     count.incrementAndGet(). }
false;;0;9;;boolean allSent() {     if (count.decrementAndGet() == 0) {         log.trace("All messages sent, continue routing.").         // was able to get all the work done while queuing the requests         callback.done(true).         return true.     }     return false. }
false;public;0;5;;@Override public void run() {     log.trace("All messages sent, continue routing.").     callback.done(false). }
false;public;2;20;;@Override public void onCompletion(RecordMetadata recordMetadata, Exception e) {     if (e != null) {         exchange.setException(e).     }     recordMetadatas.add(recordMetadata).     if (count.decrementAndGet() == 0) {         // use worker pool to continue routing the exchange         // as this thread is from Kafka Callback and should not be used by Camel routing         workerPool.submit(new Runnable() {              @Override             public void run() {                 log.trace("All messages sent, continue routing.").                 callback.done(false).             }         }).     } }
