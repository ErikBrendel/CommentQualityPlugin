commented;modifiers;parameterAmount;loc;comment;code
false;public;4;12;;@Override public long append(HdfsOutputStream hdfsostr, Object key, Object value, TypeConverter typeConverter) {     InputStream is = null.     try {         is = typeConverter.convertTo(InputStream.class, value).         return copyBytes(is, (FSDataOutputStream) hdfsostr.getOut(), HdfsConstants.DEFAULT_BUFFERSIZE, false).     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } finally {         IOHelper.close(is).     } }
false;public;3;21;;@Override public long next(HdfsInputStream hdfsistr, Holder<Object> key, Holder<Object> value) {     try {         ByteArrayOutputStream bos = new ByteArrayOutputStream(hdfsistr.getChunkSize()).         byte[] buf = new byte[hdfsistr.getChunkSize()].         int bytesRead = ((InputStream) hdfsistr.getIn()).read(buf).         if (bytesRead >= 0) {             bos.write(buf, 0, bytesRead).             key.value = null.             value.value = bos.             return bytesRead.         } else {             key.value = null.             // indication that we may have read from empty file             value.value = bos.             return 0.         }     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;0;3;;@Override public void progress() { }
false;public;0;3;;@Override public void progress() { }
false;public;2;24;;@Override public Closeable createOutputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rout.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         if (!configuration.isAppend()) {             rout = hdfsInfo.getFileSystem().create(hdfsInfo.getPath(), configuration.isOverwrite(), configuration.getBufferSize(), configuration.getReplication(), configuration.getBlockSize(), new Progressable() {                  @Override                 public void progress() {                 }             }).         } else {             rout = hdfsInfo.getFileSystem().append(hdfsInfo.getPath(), configuration.getBufferSize(), new Progressable() {                  @Override                 public void progress() {                 }             }).         }         return rout.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;2;15;;@Override public Closeable createInputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rin.         if (configuration.getFileSystemType().equals(HdfsFileSystemType.LOCAL)) {             HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).             rin = hdfsInfo.getFileSystem().open(hdfsInfo.getPath()).         } else {             rin = new FileInputStream(getHfdsFileToTmpFile(hdfsPath, configuration)).         }         return rin.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;private;2;28;;private File getHfdsFileToTmpFile(String hdfsPath, HdfsConfiguration configuration) {     try {         String fname = hdfsPath.substring(hdfsPath.lastIndexOf('/')).         File outputDest = File.createTempFile(fname, ".hdfs").         if (outputDest.exists()) {             outputDest.delete().         }         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         FileSystem fileSystem = hdfsInfo.getFileSystem().         FileUtil.copy(fileSystem, new Path(hdfsPath), outputDest, false, fileSystem.getConf()).         try {             FileUtil.copyMerge(// src             fileSystem, new Path(hdfsPath), // dest             FileSystem.getLocal(new Configuration()), new Path(outputDest.toURI()), false, fileSystem.getConf(), null).         } catch (IOException e) {             return outputDest.         }         return new File(outputDest, fname).     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;4;15;;@Override public long append(HdfsOutputStream hdfsostr, Object key, Object value, TypeConverter typeConverter) {     try {         Holder<Integer> keySize = new Holder<>().         Writable keyWritable = getWritable(key, typeConverter, keySize).         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = getWritable(value, typeConverter, valueSize).         Writer writer = (SequenceFile.Writer) hdfsostr.getOut().         writer.append(keyWritable, valueWritable).         writer.sync().         return keySize.value + valueSize.value.     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;3;19;;@Override public long next(HdfsInputStream hdfsistr, Holder<Object> key, Holder<Object> value) {     try {         SequenceFile.Reader reader = (SequenceFile.Reader) hdfsistr.getIn().         Holder<Integer> keySize = new Holder<>().         Writable keyWritable = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), new Configuration()).         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), new Configuration()).         if (reader.next(keyWritable, valueWritable)) {             key.value = getObject(keyWritable, keySize).             value.value = getObject(valueWritable, valueSize).             return keySize.value + valueSize.value.         } else {             return 0.         }     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;0;3;;@Override public void progress() { }
false;public;2;21;;@Override public Closeable createOutputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rout.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         Class<?> keyWritableClass = configuration.getKeyType().getWritableClass().         Class<?> valueWritableClass = configuration.getValueType().getWritableClass().         rout = SequenceFile.createWriter(hdfsInfo.getConf(), Writer.file(hdfsInfo.getPath()), Writer.keyClass(keyWritableClass), Writer.valueClass(valueWritableClass), Writer.bufferSize(configuration.getBufferSize()), Writer.replication(configuration.getReplication()), Writer.blockSize(configuration.getBlockSize()), Writer.compression(configuration.getCompressionType(), configuration.getCompressionCodec().getCodec()), Writer.progressable(new Progressable() {              @Override             public void progress() {             }         }), Writer.metadata(new SequenceFile.Metadata())).         return rout.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;2;11;;@Override public Closeable createInputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rin.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         rin = new SequenceFile.Reader(hdfsInfo.getConf(), Reader.file(hdfsInfo.getPath())).         return rin.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;4;13;;@Override public long append(HdfsOutputStream hdfsostr, Object key, Object value, TypeConverter typeConverter) {     try {         Holder<Integer> keySize = new Holder<>().         Writable keyWritable = getWritable(key, typeConverter, keySize).         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = getWritable(value, typeConverter, valueSize).         ((MapFile.Writer) hdfsostr.getOut()).append((WritableComparable<?>) keyWritable, valueWritable).         return keySize.value + valueSize.value.     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;3;19;;@Override public long next(HdfsInputStream hdfsistr, Holder<Object> key, Holder<Object> value) {     try {         MapFile.Reader reader = (MapFile.Reader) hdfsistr.getIn().         Holder<Integer> keySize = new Holder<>().         WritableComparable<?> keyWritable = (WritableComparable<?>) ReflectionUtils.newInstance(reader.getKeyClass(), new Configuration()).         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), new Configuration()).         if (reader.next(keyWritable, valueWritable)) {             key.value = getObject(keyWritable, keySize).             value.value = getObject(valueWritable, valueSize).             return keySize.value + valueSize.value.         } else {             return 0.         }     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;0;3;;@Override public void progress() { }
false;public;2;20;;@Override @SuppressWarnings("rawtypes") public Closeable createOutputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rout.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         Class<? extends WritableComparable> keyWritableClass = configuration.getKeyType().getWritableClass().         Class<? extends WritableComparable> valueWritableClass = configuration.getValueType().getWritableClass().         rout = new MapFile.Writer(hdfsInfo.getConf(), new Path(hdfsPath), MapFile.Writer.keyClass(keyWritableClass), MapFile.Writer.valueClass(valueWritableClass), MapFile.Writer.compression(configuration.getCompressionType(), configuration.getCompressionCodec().getCodec()), MapFile.Writer.progressable(new Progressable() {              @Override             public void progress() {             }         })).         return rout.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;2;11;;@Override public Closeable createInputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rin.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         rin = new MapFile.Reader(new Path(hdfsPath), hdfsInfo.getConf()).         return rin.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;4;13;;@Override public long append(HdfsOutputStream hdfsostr, Object key, Object value, TypeConverter typeConverter) {     try {         Holder<Integer> keySize = new Holder<>().         Writable keyWritable = getWritable(key, typeConverter, keySize).         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = getWritable(value, typeConverter, valueSize).         ((BloomMapFile.Writer) hdfsostr.getOut()).append((WritableComparable<?>) keyWritable, valueWritable).         return keySize.value + valueSize.value.     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;3;19;;@Override public long next(HdfsInputStream hdfsistr, Holder<Object> key, Holder<Object> value) {     try {         MapFile.Reader reader = (BloomMapFile.Reader) hdfsistr.getIn().         Holder<Integer> keySize = new Holder<>().         WritableComparable<?> keyWritable = (WritableComparable<?>) ReflectionUtils.newInstance(reader.getKeyClass(), new Configuration()).         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), new Configuration()).         if (reader.next(keyWritable, valueWritable)) {             key.value = getObject(keyWritable, keySize).             value.value = getObject(valueWritable, valueSize).             return keySize.value + valueSize.value.         } else {             return 0.         }     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;0;3;;@Override public void progress() { }
false;public;2;21;;@SuppressWarnings("rawtypes") @Override public Closeable createOutputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rout.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         Class<? extends WritableComparable> keyWritableClass = configuration.getKeyType().getWritableClass().         Class<? extends WritableComparable> valueWritableClass = configuration.getValueType().getWritableClass().         rout = new BloomMapFile.Writer(hdfsInfo.getConf(), new Path(hdfsPath), org.apache.hadoop.io.MapFile.Writer.keyClass(keyWritableClass), org.apache.hadoop.io.MapFile.Writer.valueClass(valueWritableClass), org.apache.hadoop.io.MapFile.Writer.compression(configuration.getCompressionType(), configuration.getCompressionCodec().getCodec()), org.apache.hadoop.io.MapFile.Writer.progressable(new Progressable() {              @Override             public void progress() {             }         })).         return rout.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;2;11;;@Override public Closeable createInputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rin.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         rin = new BloomMapFile.Reader(new Path(hdfsPath), hdfsInfo.getConf()).         return rin.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;4;11;;@Override public long append(HdfsOutputStream hdfsostr, Object key, Object value, TypeConverter typeConverter) {     try {         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = getWritable(value, typeConverter, valueSize).         ((ArrayFile.Writer) hdfsostr.getOut()).append(valueWritable).         return valueSize.value.     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;3;16;;@Override public long next(HdfsInputStream hdfsistr, Holder<Object> key, Holder<Object> value) {     try {         ArrayFile.Reader reader = (ArrayFile.Reader) hdfsistr.getIn().         Holder<Integer> valueSize = new Holder<>().         Writable valueWritable = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), new Configuration()).         if (reader.next(valueWritable) != null) {             value.value = getObject(valueWritable, valueSize).             return valueSize.value.         } else {             return 0.         }     } catch (Exception ex) {         throw new RuntimeCamelException(ex).     } }
false;public;0;3;;@Override public void progress() { }
false;public;2;18;;@SuppressWarnings("rawtypes") @Override public Closeable createOutputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rout.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         Class<? extends WritableComparable> valueWritableClass = configuration.getValueType().getWritableClass().         rout = new ArrayFile.Writer(hdfsInfo.getConf(), hdfsInfo.getFileSystem(), hdfsPath, valueWritableClass, configuration.getCompressionType(), new Progressable() {              @Override             public void progress() {             }         }).         return rout.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;public;2;11;;@Override public Closeable createInputStream(String hdfsPath, HdfsConfiguration configuration) {     try {         Closeable rin.         HdfsInfo hdfsInfo = HdfsInfoFactory.newHdfsInfo(hdfsPath).         rin = new ArrayFile.Reader(hdfsInfo.getFileSystem(), hdfsPath, hdfsInfo.getConf()).         return rin.     } catch (IOException ex) {         throw new RuntimeCamelException(ex).     } }
false;private,static;3;8;;private static Writable getWritable(Object obj, TypeConverter typeConverter, Holder<Integer> size) {     Class<?> objCls = obj == null ? null : obj.getClass().     HdfsWritableFactories.HdfsWritableFactory objWritableFactory = WritableCache.writables.get(objCls).     if (objWritableFactory == null) {         objWritableFactory = new HdfsWritableFactories.HdfsObjectWritableFactory().     }     return objWritableFactory.create(obj, typeConverter, size). }
false;private,static;2;8;;private static Object getObject(Writable writable, Holder<Integer> size) {     Class<?> writableClass = NullWritable.class.     if (writable != null) {         writableClass = writable.getClass().     }     HdfsWritableFactories.HdfsWritableFactory writableObjectFactory = WritableCache.readables.get(writableClass).     return writableObjectFactory.read(writable, size). }
false;public,abstract;4;1;;public abstract long append(HdfsOutputStream hdfsostr, Object key, Object value, TypeConverter typeConverter).
false;public,abstract;3;1;;public abstract long next(HdfsInputStream hdfsistr, Holder<Object> key, Holder<Object> value).
false;public,abstract;2;1;;public abstract Closeable createOutputStream(String hdfsPath, HdfsConfiguration configuration).
false;public,abstract;2;1;;public abstract Closeable createInputStream(String hdfsPath, HdfsConfiguration configuration).
false;public,static;4;21;;public static long copyBytes(InputStream in, OutputStream out, int buffSize, boolean close) throws IOException {     long numBytes = 0.     PrintStream ps = out instanceof PrintStream ? (PrintStream) out : null.     byte[] buf = new byte[buffSize].     try {         int bytesRead = in.read(buf).         while (bytesRead >= 0) {             out.write(buf, 0, bytesRead).             numBytes += bytesRead.             if ((ps != null) && ps.checkError()) {                 throw new IOException("Unable to write to output stream.").             }             bytesRead = in.read(buf).         }     } finally {         if (close) {             IOHelper.close(out, in).         }     }     return numBytes. }
