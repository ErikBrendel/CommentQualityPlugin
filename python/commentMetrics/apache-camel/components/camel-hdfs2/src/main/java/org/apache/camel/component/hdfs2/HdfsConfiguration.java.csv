commented;modifiers;parameterAmount;loc;comment;code
false;private;3;7;;private Boolean getBoolean(Map<String, Object> hdfsSettings, String param, Boolean dflt) {     if (hdfsSettings.containsKey(param)) {         return Boolean.valueOf((String) hdfsSettings.get(param)).     } else {         return dflt.     } }
false;private;3;7;;private Integer getInteger(Map<String, Object> hdfsSettings, String param, Integer dflt) {     if (hdfsSettings.containsKey(param)) {         return Integer.valueOf((String) hdfsSettings.get(param)).     } else {         return dflt.     } }
false;private;3;7;;private Short getShort(Map<String, Object> hdfsSettings, String param, Short dflt) {     if (hdfsSettings.containsKey(param)) {         return Short.valueOf((String) hdfsSettings.get(param)).     } else {         return dflt.     } }
false;private;3;7;;private Long getLong(Map<String, Object> hdfsSettings, String param, Long dflt) {     if (hdfsSettings.containsKey(param)) {         return Long.valueOf((String) hdfsSettings.get(param)).     } else {         return dflt.     } }
false;private;3;8;;private HdfsFileType getFileType(Map<String, Object> hdfsSettings, String param, HdfsFileType dflt) {     String eit = (String) hdfsSettings.get(param).     if (eit != null) {         return HdfsFileType.valueOf(eit).     } else {         return dflt.     } }
false;private;3;8;;private HdfsFileSystemType getFileSystemType(Map<String, Object> hdfsSettings, String param, HdfsFileSystemType dflt) {     String eit = (String) hdfsSettings.get(param).     if (eit != null) {         return HdfsFileSystemType.valueOf(eit).     } else {         return dflt.     } }
false;private;3;8;;private WritableType getWritableType(Map<String, Object> hdfsSettings, String param, WritableType dflt) {     String eit = (String) hdfsSettings.get(param).     if (eit != null) {         return WritableType.valueOf(eit).     } else {         return dflt.     } }
false;private;3;8;;private SequenceFile.CompressionType getCompressionType(Map<String, Object> hdfsSettings, String param, SequenceFile.CompressionType ct) {     String eit = (String) hdfsSettings.get(param).     if (eit != null) {         return SequenceFile.CompressionType.valueOf(eit).     } else {         return ct.     } }
false;private;3;8;;private HdfsCompressionCodec getCompressionCodec(Map<String, Object> hdfsSettings, String param, HdfsCompressionCodec cd) {     String eit = (String) hdfsSettings.get(param).     if (eit != null) {         return HdfsCompressionCodec.valueOf(eit).     } else {         return cd.     } }
false;private;3;7;;private String getString(Map<String, Object> hdfsSettings, String param, String dflt) {     if (hdfsSettings.containsKey(param)) {         return (String) hdfsSettings.get(param).     } else {         return dflt.     } }
false;private;1;22;;private List<HdfsProducer.SplitStrategy> getSplitStrategies(Map<String, Object> hdfsSettings) {     List<HdfsProducer.SplitStrategy> strategies = new ArrayList<>().     for (Object obj : hdfsSettings.keySet()) {         String key = (String) obj.         if ("splitStrategy".equals(key)) {             String eit = (String) hdfsSettings.get(key).             if (eit != null) {                 String[] strstrategies = eit.split(",").                 for (String strstrategy : strstrategies) {                     String[] tokens = strstrategy.split(":").                     if (tokens.length != 2) {                         throw new IllegalArgumentException("Wrong Split Strategy " + key + "=" + eit).                     }                     HdfsProducer.SplitStrategyType sst = HdfsProducer.SplitStrategyType.valueOf(tokens[0]).                     long ssv = Long.valueOf(tokens[1]).                     strategies.add(new HdfsProducer.SplitStrategy(sst, ssv)).                 }             }         }     }     return strategies. }
false;public;0;2;;public void checkConsumerOptions() { }
false;public;0;10;;public void checkProducerOptions() {     if (isAppend()) {         if (getSplitStrategies().size() != 0) {             throw new IllegalArgumentException("Split Strategies incompatible with append=true").         }         if (getFileType() != HdfsFileType.NORMAL_FILE) {             throw new IllegalArgumentException("append=true works only with NORMAL_FILEs").         }     } }
false;public;1;31;;public void parseURI(URI uri) throws URISyntaxException {     String protocol = uri.getScheme().     if (!protocol.equalsIgnoreCase("hdfs2")) {         throw new IllegalArgumentException("Unrecognized protocol: " + protocol + " for uri: " + uri).     }     hostName = uri.getHost().     if (hostName == null) {         hostName = "localhost".     }     port = uri.getPort() == -1 ? HdfsConstants.DEFAULT_PORT : uri.getPort().     path = uri.getPath().     Map<String, Object> hdfsSettings = URISupport.parseParameters(uri).     overwrite = getBoolean(hdfsSettings, "overwrite", overwrite).     append = getBoolean(hdfsSettings, "append", append).     wantAppend = append.     bufferSize = getInteger(hdfsSettings, "bufferSize", bufferSize).     replication = getShort(hdfsSettings, "replication", replication).     blockSize = getLong(hdfsSettings, "blockSize", blockSize).     compressionType = getCompressionType(hdfsSettings, "compressionType", compressionType).     compressionCodec = getCompressionCodec(hdfsSettings, "compressionCodec", compressionCodec).     fileType = getFileType(hdfsSettings, "fileType", fileType).     fileSystemType = getFileSystemType(hdfsSettings, "fileSystemType", fileSystemType).     keyType = getWritableType(hdfsSettings, "keyType", keyType).     valueType = getWritableType(hdfsSettings, "valueType", valueType).     openedSuffix = getString(hdfsSettings, "openedSuffix", openedSuffix).     readSuffix = getString(hdfsSettings, "readSuffix", readSuffix).     pattern = getString(hdfsSettings, "pattern", pattern).     chunkSize = getInteger(hdfsSettings, "chunkSize", chunkSize).     splitStrategies = getSplitStrategies(hdfsSettings). }
false;public;0;3;;public URI getUri() {     return uri. }
false;public;1;3;;public void setUri(URI uri) {     this.uri = uri. }
false;public;0;3;;public String getHostName() {     return hostName. }
true;public;1;3;/**  * HDFS host to use  */ ;/**  * HDFS host to use  */ public void setHostName(String hostName) {     this.hostName = hostName. }
false;public;0;3;;public int getPort() {     return port. }
true;public;1;3;/**  * HDFS port to use  */ ;/**  * HDFS port to use  */ public void setPort(int port) {     this.port = port. }
false;public;0;3;;public String getPath() {     return path. }
true;public;1;3;/**  * The directory path to use  */ ;/**  * The directory path to use  */ public void setPath(String path) {     this.path = path. }
false;public;0;3;;public boolean isOverwrite() {     return overwrite. }
true;public;1;3;/**  * Whether to overwrite existing files with the same name  */ ;/**  * Whether to overwrite existing files with the same name  */ public void setOverwrite(boolean overwrite) {     this.overwrite = overwrite. }
false;public;0;3;;public boolean isAppend() {     return append. }
false;public;0;3;;public boolean isWantAppend() {     return wantAppend. }
true;public;1;3;/**  * Append to existing file. Notice that not all HDFS file systems support the append option.  */ ;/**  * Append to existing file. Notice that not all HDFS file systems support the append option.  */ public void setAppend(boolean append) {     this.append = append. }
false;public;0;3;;public int getBufferSize() {     return bufferSize. }
true;public;1;3;/**  * The buffer size used by HDFS  */ ;/**  * The buffer size used by HDFS  */ public void setBufferSize(int bufferSize) {     this.bufferSize = bufferSize. }
false;public;0;3;;public short getReplication() {     return replication. }
true;public;1;3;/**  * The HDFS replication factor  */ ;/**  * The HDFS replication factor  */ public void setReplication(short replication) {     this.replication = replication. }
false;public;0;3;;public long getBlockSize() {     return blockSize. }
true;public;1;3;/**  * The size of the HDFS blocks  */ ;/**  * The size of the HDFS blocks  */ public void setBlockSize(long blockSize) {     this.blockSize = blockSize. }
false;public;0;3;;public HdfsFileType getFileType() {     return fileType. }
true;public;1;3;/**  * The file type to use. For more details see Hadoop HDFS documentation about the various files types.  */ ;/**  * The file type to use. For more details see Hadoop HDFS documentation about the various files types.  */ public void setFileType(HdfsFileType fileType) {     this.fileType = fileType. }
false;public;0;3;;public SequenceFile.CompressionType getCompressionType() {     return compressionType. }
true;public;1;3;/**  * The compression type to use (is default not in use)  */ ;/**  * The compression type to use (is default not in use)  */ public void setCompressionType(SequenceFile.CompressionType compressionType) {     this.compressionType = compressionType. }
false;public;0;3;;public HdfsCompressionCodec getCompressionCodec() {     return compressionCodec. }
true;public;1;3;/**  * The compression codec to use  */ ;/**  * The compression codec to use  */ public void setCompressionCodec(HdfsCompressionCodec compressionCodec) {     this.compressionCodec = compressionCodec. }
true;public;1;3;/**  * Set to LOCAL to not use HDFS but local java.io.File instead.  */ ;/**  * Set to LOCAL to not use HDFS but local java.io.File instead.  */ public void setFileSystemType(HdfsFileSystemType fileSystemType) {     this.fileSystemType = fileSystemType. }
false;public;0;3;;public HdfsFileSystemType getFileSystemType() {     return fileSystemType. }
false;public;0;3;;public WritableType getKeyType() {     return keyType. }
true;public;1;3;/**  * The type for the key in case of sequence or map files.  */ ;/**  * The type for the key in case of sequence or map files.  */ public void setKeyType(WritableType keyType) {     this.keyType = keyType. }
false;public;0;3;;public WritableType getValueType() {     return valueType. }
true;public;1;3;/**  * The type for the key in case of sequence or map files  */ ;/**  * The type for the key in case of sequence or map files  */ public void setValueType(WritableType valueType) {     this.valueType = valueType. }
true;public;1;3;/**  * When a file is opened for reading/writing the file is renamed with this suffix to avoid to read it during the writing phase.  */ ;/**  * When a file is opened for reading/writing the file is renamed with this suffix to avoid to read it during the writing phase.  */ public void setOpenedSuffix(String openedSuffix) {     this.openedSuffix = openedSuffix. }
false;public;0;3;;public String getOpenedSuffix() {     return openedSuffix. }
true;public;1;3;/**  * Once the file has been read is renamed with this suffix to avoid to read it again.  */ ;/**  * Once the file has been read is renamed with this suffix to avoid to read it again.  */ public void setReadSuffix(String readSuffix) {     this.readSuffix = readSuffix. }
false;public;0;3;;public String getReadSuffix() {     return readSuffix. }
true;public;1;3;/**  * The pattern used for scanning the directory  */ ;/**  * The pattern used for scanning the directory  */ public void setPattern(String pattern) {     this.pattern = pattern. }
false;public;0;3;;public String getPattern() {     return pattern. }
true;public;1;3;/**  * When reading a normal file, this is split into chunks producing a message per chunk.  */ ;/**  * When reading a normal file, this is split into chunks producing a message per chunk.  */ public void setChunkSize(int chunkSize) {     this.chunkSize = chunkSize. }
false;public;0;3;;public int getChunkSize() {     return chunkSize. }
true;public;1;3;/**  * How often (time in millis) in to run the idle checker background task. This option is only in use if the splitter strategy is IDLE.  */ ;/**  * How often (time in millis) in to run the idle checker background task. This option is only in use if the splitter strategy is IDLE.  */ public void setCheckIdleInterval(int checkIdleInterval) {     this.checkIdleInterval = checkIdleInterval. }
false;public;0;3;;public int getCheckIdleInterval() {     return checkIdleInterval. }
false;public;0;3;;public List<HdfsProducer.SplitStrategy> getSplitStrategies() {     return splitStrategies. }
false;public;0;3;;public String getSplitStrategy() {     return splitStrategy. }
true;public;1;3;/**  * In the current version of Hadoop opening a file in append mode is disabled since it's not very reliable. So, for the moment,  * it's only possible to create new files. The Camel HDFS endpoint tries to solve this problem in this way:  * <ul>  * <li>If the split strategy option has been defined, the hdfs path will be used as a directory and files will be created using the configured UuidGenerator.</li>  * <li>Every time a splitting condition is met, a new file is created.</li>  * </ul>  * The splitStrategy option is defined as a string with the following syntax:  * <br/><tt>splitStrategy=ST:value,ST:value,...</tt>  * <br/>where ST can be:  * <ul>  * <li>BYTES a new file is created, and the old is closed when the number of written bytes is more than value</li>  * <li>MESSAGES a new file is created, and the old is closed when the number of written messages is more than value</li>  * <li>IDLE a new file is created, and the old is closed when no writing happened in the last value milliseconds</li>  * </ul>  */ ;/**  * In the current version of Hadoop opening a file in append mode is disabled since it's not very reliable. So, for the moment,  * it's only possible to create new files. The Camel HDFS endpoint tries to solve this problem in this way:  * <ul>  * <li>If the split strategy option has been defined, the hdfs path will be used as a directory and files will be created using the configured UuidGenerator.</li>  * <li>Every time a splitting condition is met, a new file is created.</li>  * </ul>  * The splitStrategy option is defined as a string with the following syntax:  * <br/><tt>splitStrategy=ST:value,ST:value,...</tt>  * <br/>where ST can be:  * <ul>  * <li>BYTES a new file is created, and the old is closed when the number of written bytes is more than value</li>  * <li>MESSAGES a new file is created, and the old is closed when the number of written messages is more than value</li>  * <li>IDLE a new file is created, and the old is closed when no writing happened in the last value milliseconds</li>  * </ul>  */ public void setSplitStrategy(String splitStrategy) {     this.splitStrategy = splitStrategy. }
false;public;0;3;;public boolean isConnectOnStartup() {     return connectOnStartup. }
true;public;1;3;/**  * Whether to connect to the HDFS file system on starting the producer/consumer.  * If false then the connection is created on-demand. Notice that HDFS may take up till 15 minutes to establish  * a connection, as it has hardcoded 45 x 20 sec redelivery. By setting this option to false allows your  * application to startup, and not block for up till 15 minutes.  */ ;/**  * Whether to connect to the HDFS file system on starting the producer/consumer.  * If false then the connection is created on-demand. Notice that HDFS may take up till 15 minutes to establish  * a connection, as it has hardcoded 45 x 20 sec redelivery. By setting this option to false allows your  * application to startup, and not block for up till 15 minutes.  */ public void setConnectOnStartup(boolean connectOnStartup) {     this.connectOnStartup = connectOnStartup. }
false;public;0;3;;public String getOwner() {     return owner. }
true;public;1;3;/**  * The file owner must match this owner for the consumer to pickup the file. Otherwise the file is skipped.  */ ;/**  * The file owner must match this owner for the consumer to pickup the file. Otherwise the file is skipped.  */ public void setOwner(String owner) {     this.owner = owner. }
